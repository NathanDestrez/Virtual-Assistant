{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8692ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:45.846879500Z",
     "start_time": "2024-01-15T15:52:40.520380100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Data Connector\n",
    "from llama_index import SimpleDirectoryReader\n",
    "# Index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Llama Index LLM\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "# Other LLM\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Retriever\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Display\n",
    "from llama_index.response.notebook_utils import display_source_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a81bb2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:45.848881500Z",
     "start_time": "2024-01-15T15:52:45.846368900Z"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a573802",
   "metadata": {},
   "source": [
    "nest_asyncio.apply() patches the existing event loop in a Jupyter Notebook environment to allow nested usage of asyncio.\n",
    "\n",
    "It is utilized later in the notebook to ensure that the asyncio event loop functions correctly within a Jupyter Notebook environment, enabling the concurrent execution of multiple asynchronous retrieval tasks without encountering event loop compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad164e3",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a3c564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:46.155373600Z",
     "start_time": "2024-01-15T15:52:45.848881500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformerEmbeddings with the loaded model\n",
    "local_embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefcccc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:47.754627Z",
     "start_time": "2024-01-15T15:52:46.156322200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Mistral model from LM studio server\n",
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "# Initialize service context : LLM and Embeddings model for the vector store\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92595283",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6199eada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:49.911758800Z",
     "start_time": "2024-01-15T15:52:47.756618900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138 docs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data connector/ reader. \n",
    "# SimpleDirectoryReader adapt to the document format.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"thesis.pdf\"]\n",
    ")\n",
    "\n",
    "documents  = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6877c",
   "metadata": {},
   "source": [
    "# Load in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bca48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.229509200Z",
     "start_time": "2024-01-15T15:52:49.910762600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a simple vector store index \n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Smart app"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da8636a8ce10e9cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#query_str = \"explain me how we used langchain in the methodology?\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.233569600Z",
     "start_time": "2024-01-15T15:52:54.229509200Z"
    }
   },
   "id": "14cc65e6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76320f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.245068600Z",
     "start_time": "2024-01-15T15:52:54.232566300Z"
    }
   },
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b875daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.250088700Z",
     "start_time": "2024-01-15T15:52:54.247054600Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str # remove the original query\n",
    "    )\n",
    "    \n",
    "    response = llm.generate([fmt_prompt])\n",
    "    \n",
    "    # Assuming there's only one generation in the response\n",
    "    if response.generations and len(response.generations[0]) > 0:\n",
    "        generation_text = response.generations[0][0].text\n",
    "        queries = generation_text.split(\"\\n\")\n",
    "        return queries\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecec81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.264863500Z",
     "start_time": "2024-01-15T15:52:54.250088700Z"
    }
   },
   "outputs": [],
   "source": [
    "#queries = generate_queries(llm, query_str, num_queries=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abec5",
   "metadata": {},
   "source": [
    "### More examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a4672f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.264863500Z",
     "start_time": "2024-01-15T15:52:54.257279500Z"
    }
   },
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"\n",
    "    Run queries against retrievers asynchronously.\n",
    "\n",
    "    :param queries: A list of queries to be processed.\n",
    "    :param retrievers: A list of retriever objects that will process the queries.\n",
    "    :return: A dictionary mapping each query and its index to its corresponding result.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        # For each query, iterate over each retriever.\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            # For each retriever, create an asynchronous task to retrieve the query\n",
    "            # and add it to the tasks list.\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "     # Iterate over each pair of query and its result.\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        # Map each query and its index to its result in the dictionary.\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124ed9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.265861200Z",
     "start_time": "2024-01-15T15:52:54.260973Z"
    }
   },
   "outputs": [],
   "source": [
    "# vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69ba8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.593334800Z",
     "start_time": "2024-01-15T15:52:54.265861200Z"
    }
   },
   "outputs": [],
   "source": [
    "# bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f623bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.600238900Z",
     "start_time": "2024-01-15T15:52:54.596130300Z"
    }
   },
   "outputs": [],
   "source": [
    "#results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e480d10f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.618419200Z",
     "start_time": "2024-01-15T15:52:54.603642100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llama index function \n",
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59614a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.619416Z",
     "start_time": "2024-01-15T15:52:54.614591300Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46e6b",
   "metadata": {},
   "source": [
    "# Plug into RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f09855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.669763300Z",
     "start_time": "2024-01-15T15:52:54.619416Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 91.20it/s]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"explain me how we used langchain in the methodology?\"\n",
    "queries = generate_queries(llm, query_str, num_queries=4)\n",
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:56.900828900Z",
     "start_time": "2024-01-15T15:52:54.671756500Z"
    }
   },
   "id": "fa7e6f7366744347",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87edde62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:53:32.529381100Z",
     "start_time": "2024-01-15T15:53:32.509413600Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'await' outside async function (2561157583.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[24], line 20\u001B[1;36m\u001B[0m\n\u001B[1;33m    result =  await run_queries(queries, [vector_retriever, bm25_retriever])\u001B[0m\n\u001B[1;37m              ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m 'await' outside async function\n"
     ]
    }
   ],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.llm = llm  # Store the llm instance\n",
    "        #self.query_str = \" \"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(self.llm, query_str, num_queries=4)  # Use the llm instance\n",
    "        result =  run_queries(queries, [vector_retriever, bm25_retriever]) \n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.246160200Z",
     "start_time": "2024-01-15T15:52:56.906809Z"
    }
   },
   "id": "035b1147",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf189ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.253099800Z",
     "start_time": "2024-01-15T15:52:57.248120400Z"
    }
   },
   "outputs": [],
   "source": [
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "response_synthesizer= get_response_synthesizer(service_context,streaming=True) # streaming False for classic answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7d7b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.265389900Z",
     "start_time": "2024-01-15T15:52:57.253099800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context, \n",
    "    streaming=True # streaming False for classic answer generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1af30452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.610799200Z",
     "start_time": "2024-01-15T15:52:57.266390500Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'coroutine' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m streaming_response \u001B[38;5;241m=\u001B[39m query_engine\u001B[38;5;241m.\u001B[39mquery(\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTell me about the document\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      3\u001B[0m )\n\u001B[0;32m      5\u001B[0m streaming_response\u001B[38;5;241m.\u001B[39mprint_response_stream()\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\llama_index\\core\\base_query_engine.py:40\u001B[0m, in \u001B[0;36mBaseQueryEngine.query\u001B[1;34m(self, str_or_query_bundle)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     39\u001B[0m     str_or_query_bundle \u001B[38;5;241m=\u001B[39m QueryBundle(str_or_query_bundle)\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_query(str_or_query_bundle)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:172\u001B[0m, in \u001B[0;36mRetrieverQueryEngine._query\u001B[1;34m(self, query_bundle)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[0;32m    169\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mQUERY, payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mQUERY_STR: query_bundle\u001B[38;5;241m.\u001B[39mquery_str}\n\u001B[0;32m    170\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[0;32m    171\u001B[0m     nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretrieve(query_bundle)\n\u001B[1;32m--> 172\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_synthesizer\u001B[38;5;241m.\u001B[39msynthesize(\n\u001B[0;32m    173\u001B[0m         query\u001B[38;5;241m=\u001B[39mquery_bundle,\n\u001B[0;32m    174\u001B[0m         nodes\u001B[38;5;241m=\u001B[39mnodes,\n\u001B[0;32m    175\u001B[0m     )\n\u001B[0;32m    177\u001B[0m     query_event\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: response})\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\llama_index\\response_synthesizers\\base.py:159\u001B[0m, in \u001B[0;36mBaseSynthesizer.synthesize\u001B[1;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msynthesize\u001B[39m(\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    154\u001B[0m     query: QueryTextType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kwargs: Any,\n\u001B[0;32m    158\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RESPONSE_TYPE:\n\u001B[1;32m--> 159\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(nodes) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    160\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Response(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmpty Response\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(query, \u001B[38;5;28mstr\u001B[39m):\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'coroutine' has no len()"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Tell me about the document\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669862b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T15:52:57.609803Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What are the conclusions for the future of the tool in the company\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T15:52:57.610799200Z"
    }
   },
   "id": "acc07ac439acf612",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
