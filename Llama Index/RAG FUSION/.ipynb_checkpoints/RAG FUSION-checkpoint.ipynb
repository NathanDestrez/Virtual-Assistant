{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8692ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Data Connector\n",
    "from llama_index import SimpleDirectoryReader\n",
    "# Index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Llama Index LLM\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "# Other LLM\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Retriever\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Display\n",
    "from llama_index.response.notebook_utils import display_source_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a81bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a573802",
   "metadata": {},
   "source": [
    "nest_asyncio.apply() patches the existing event loop in a Jupyter Notebook environment to allow nested usage of asyncio.\n",
    "\n",
    "It is utilized later in the notebook to ensure that the asyncio event loop functions correctly within a Jupyter Notebook environment, enabling the concurrent execution of multiple asynchronous retrieval tasks without encountering event loop compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad164e3",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a3c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformerEmbeddings with the loaded model\n",
    "local_embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefcccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Mistral model from LM studio server\n",
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://localhost:1234/v1\")\n",
    "# Initialize service context : LLM and Embeddings model for the vector store\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92595283",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6199eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138 docs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data connector/ reader. \n",
    "# SimpleDirectoryReader adapt to the doucment format.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"thesis.pdf\"]\n",
    ")\n",
    "\n",
    "documents  = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6877c",
   "metadata": {},
   "source": [
    "# Load in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a simple vector store index \n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca889",
   "metadata": {},
   "source": [
    "# Define Advanced Retriever\n",
    "\n",
    "1. Query generation/rewriting: generate multiple queries given the original user query\n",
    "\n",
    "2. Perform retrieval for each query over an ensemble of retrievers.\n",
    "\n",
    "3. Reranking/fusion: fuse results from all queries, and apply a reranking step to “fuse” the top relevant results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c3b0a",
   "metadata": {},
   "source": [
    "## Query Generation/Rewriting\n",
    "In this step we're creating the function to generate k different queries from the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14cc65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"explain me how we used langchain in the methodology?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76320f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b875daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str # remove the original query\n",
    "    )\n",
    "    \n",
    "    response = llm.generate([fmt_prompt])\n",
    "    \n",
    "    # Assuming there's only one generation in the response\n",
    "    if response.generations and len(response.generations[0]) > 0:\n",
    "        generation_text = response.generations[0][0].text\n",
    "        queries = generation_text.split(\"\\n\")\n",
    "        return queries\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aecec81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generate_queries(llm, query_str, num_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2106f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the role of Langchain in our methodology and how was it implemented?',\n",
       " '2. Can you provide an example of how Langchain was utilized in our research process?',\n",
       " \"3. How did Langchain contribute to the success of our project's methodology?\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries # The 3 queries generated from explain me how we used langchain in the methodology?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abec5",
   "metadata": {},
   "source": [
    "### More exemples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a50ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate_queries(llm, \"who is the author of the paper\", num_queries=4)\n",
    "t2 = generate_queries(llm, \"What are the conclusion of the research document\", num_queries=4)\n",
    "t3 = generate_queries(llm, \"What is the self attention mechanism\", num_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af1b4a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Who wrote the specific paper titled [paper title]?',\n",
       " '2. Author name for the research article with this citation [citation information]?',\n",
       " '3. Can you identify the individual(s) that authored the publication [publication details]?']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30a43efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. \"Summary of findings in the research document\"',\n",
       " '2. \"Concluding remarks in the research document\"',\n",
       " '3. \"Key takeaways from the research document conclusions\"']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8d625c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. How does the self attention mechanism work in deep learning?',\n",
       " '2. Explanation of the self attention mechanism in natural language processing.',\n",
       " '3. Self attention mechanism vs traditional attention mechanisms in neural networks.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ce52d",
   "metadata": {},
   "source": [
    "## Perform Vector Search for Each Query\n",
    "\n",
    "This code defines an asynchronous function run_queries to execute search queries using multiple retrieval methods. For each query, it asynchronously sends requests to each retriever (like a vector retriever and a BM25 retriever). The results are compiled into a dictionary, mapping each query and its position to the corresponding result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a4672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"\n",
    "    Run queries against retrievers asynchronously.\n",
    "\n",
    "    :param queries: A list of queries to be processed.\n",
    "    :param retrievers: A list of retriever objects that will process the queries.\n",
    "    :return: A dictionary mapping each query and its index to its corresponding result.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        # For each query, iterate over each retriever.\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            # For each retriever, create an asynchronous task to retrieve the query\n",
    "            # and add it to the tasks list.\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "     # Iterate over each pair of query and its result.\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        # Map each query and its index to its result in the dictionary.\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "124ed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69ba8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0736097",
   "metadata": {},
   "source": [
    "BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\n",
    "\n",
    "A bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. [Wiki here](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,their%20proximity%20within%20the%20document.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f623bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 127.39it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c8595",
   "metadata": {},
   "source": [
    "The function is called with a set of queries and two specific retrievers, vector_retriever and bm25_retriever, to fetch and collate their results asynchronously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e47ee",
   "metadata": {},
   "source": [
    "### more exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c8d9c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 142.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 157.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 142.62it/s]\n"
     ]
    }
   ],
   "source": [
    "r1 = await run_queries(t1, [vector_retriever, bm25_retriever])\n",
    "r2 = await run_queries(t2, [vector_retriever, bm25_retriever])\n",
    "r3 = await run_queries(t3, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d7ddd",
   "metadata": {},
   "source": [
    "## Perform Fusion\n",
    "\n",
    "The next step here is to perform fusion: combining the results from several retrievers into one and re-ranking.\n",
    "\n",
    "Note that a given node might be retrieved multiple times from different retrievers, so there needs to be a way to de-dup and rerank the node given the multiple retrievals.\n",
    "\n",
    "This stage perform “reciprocal rank fusion”: for each node, add up its reciprocal rank in every list where it’s retrieved.\n",
    "\n",
    "Then reorder nodes by highest score to least.\n",
    "\n",
    "Full paper [here](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e480d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama index function \n",
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59614a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279b1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4c8a7b97-554b-4d24-b8f4-4c1624f372d6<br>**Similarity:** 0.03333333333333333<br>**Text:** 60 \n",
       "Nathan Destrez  straightforward tasks to complex operations.  In addition to its customizable nature, LangChain also \n",
       "provides pre -built chains. These are pre -assembled components designed for specific tasks, enabling \n",
       "developers to quickly start projects. For more intricate and unique applications, the framework's \n",
       "modular nature allows for the creation of customized chains, offering a balance between convenience \n",
       "and personalization. LangChain's design caters to a diverse range of use...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4c4ac70d-c5f8-4e1e-accd-3ba10e0ef501<br>**Similarity:** 0.03278688524590164<br>**Text:** 59 \n",
       "Nathan Destrez  1.10 Virtual Assistants in Industry and Academia  \n",
       "1.10.1  The Role of LangChain and Emerging Trends  \n",
       "Virtual assistants have emerged as a pivotal innovation, transforming interactions between \n",
       "humans and machines. This literature review delves into the multifaceted world of virtual assistants, \n",
       "examining their development and application across industry and  academia. By exploring the existing \n",
       "landscape of these digital aides, this section aims to shed light on the prog...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in final_results:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8f8fe",
   "metadata": {},
   "source": [
    "### More exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53fb7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr1 = fuse_results(r1)\n",
    "fr2 = fuse_results(r2)\n",
    "fr3 = fuse_results(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e433ec78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 5ffc1cea-595d-42da-abba-367572efd8d9<br>**Similarity:** 0.016666666666666666<br>**Text:** 8 \n",
       "Nathan Destrez<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 990a6448-f539-4bca-8ed6-9d987b53bbed<br>**Similarity:** 0.016666666666666666<br>**Text:** 89 \n",
       "Nathan Destrez  An initial study comparing the ratio of stop words to total words was conducted, but it did not yield \n",
       "significant patterns. Consequently, most short texts, predominantly composed of stop words, were \n",
       "excluded from further processing.  \n",
       "A key observation during our initial explorations with the embeddings base was the retriever's \n",
       "occasional struggle with implicit concepts or unique terminologies present in only a few documents.  \n",
       " \n",
       "Figure 6 Document retrieved from the Sky...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr1:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f660d3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e65448db-700c-4bee-92cd-bd5156f81859<br>**Similarity:** 0.03333333333333333<br>**Text:** 5 \n",
       "Nathan Destrez   \n",
       "Contents   \n",
       "Introduction  ................................ ................................ ................................ ................................  9 \n",
       "Literature Review  ................................ ................................ ................................ ......................  11 \n",
       "1.1 Historical evolution of chatbots and virtual assistants.  ................................ .................  11 \n",
       "1.2 AI in France and the Regulation in Europe  ......<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 907588f1-3b0e-4b97-921d-276ec90ea552<br>**Similarity:** 0.016666666666666666<br>**Text:** 49 \n",
       "Nathan Destrez  1.7.5 Transformers and their role in representing longer textual data.  \n",
       "BERT's ability to understand context has naturally extended the use of embeddings from \n",
       "individual words to entire sentences or even longer texts. Sentence Transformers, as discussed in the \n",
       "article \"Understanding BERT\" on Towards AI, take this concept furth er by providing mechanisms to \n",
       "derive meaningful sentence -level embeddings. These embeddings can then be used in various NLP \n",
       "tasks, such as sem...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr2:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "297c8f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 4169b175-6618-470d-9d1c-21ef6058c7bc<br>**Similarity:** 0.03333333333333333<br>**Text:** 42 \n",
       "Nathan Destrez  1.7.3 The Attention mechanism  \n",
       "The concept of Attention within the domain of neural networks has garnered significant interest \n",
       "due to its remarkable impact on enhancing state -of-the-art results across various research fields. This \n",
       "includes areas as diverse as image captioning, language translation, and interactive question \n",
       "answering. Attention has rapidly ascended to become an indispensable instrument in the researcher's \n",
       "toolkit. The assertion by some in the field th...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** acfce4b0-5315-4a6e-a515-3d52d41d2f79<br>**Similarity:** 0.03278688524590164<br>**Text:** 46 \n",
       "Nathan Destrez  model to 'focus' on the information that is most predictive of the desired outcome. This geometric \n",
       "reconfiguration is pivotal in enhancing the model's performance by ensuring that it attends to the \n",
       "most salient features within the data.  \n",
       "The burgeoning field of research has begun to refer to this mechanism as \"Memory,\" positing that this \n",
       "term more aptly describes its functionality. The Attention layer facilitates the model's ability to \"recall\" \n",
       "and focus on previously...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr3:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46e6b",
   "metadata": {},
   "source": [
    "# Plug into RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f09855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035b1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://localhost:1234/v1\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87edde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.llm = llm  # Store the llm instance\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(self.llm, query_str, num_queries=4)  # Use the llm instance\n",
    "        results = run_queries(queries, self._retrievers)\n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cf189ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "response_synthesizer= get_response_synthesizer(service_context,streaming=True) # streaming False for classic answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed7d7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context, \n",
    "    streaming=True # streaming False for classic answer generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1af30452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan_2\\anaconda3\\envs\\llamaIndex\\Lib\\site-packages\\llama_index\\core\\base_retriever.py:54: RuntimeWarning: coroutine 'run_queries' was never awaited\n",
      "  nodes = self._retrieve(query_bundle)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document is a literature review on virtual assistants in industry and academia with a focus on LangChain and emerging trends. It discusses the role of LangChain as a groundbreaking tool for simplifying the integration and application of Large Language Models (LLMs) in both commercial and academic settings. LangChain's impact includes streamlining the creation of context-aware applications, enhancing reasoning capabilities, and offering modular components for customization. The document also provides an overview of virtual assistants' development and capabilities across industry and academia."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Tell me about the document\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "669862b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan_2\\anaconda3\\envs\\llamaIndex\\Lib\\site-packages\\llama_index\\core\\base_retriever.py:54: RuntimeWarning: coroutine 'run_queries' was never awaited\n",
      "  nodes = self._retrieve(query_bundle)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided text, LangChain is a groundbreaking tool that simplifies the integration and application of Large Language Models (LLMs) in both commercial and academic settings. Its collaboration with Retrieval Augmented Generation (RAG) has garnered significant attention for its role in streamlining the creation of context-aware applications, enhancing reasoning capabilities, and making informed decisions. The modular architecture of LangChain offers versatility and customization options, allowing developers to tailor applications to meet various requirements. Overall, LangChain's impact on virtual assistant technology is profound and multifaceted, and its future potential in the company is significant."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What are the conclusions for the future of the tool in the company\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbd64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
