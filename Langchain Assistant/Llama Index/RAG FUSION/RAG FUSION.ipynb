{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8692ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:44.491196400Z",
     "start_time": "2024-01-15T15:07:38.975129600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Data Connector\n",
    "from llama_index import SimpleDirectoryReader\n",
    "# Index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Llama Index LLM\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "# Other LLM\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Retriever\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Display\n",
    "from llama_index.response.notebook_utils import display_source_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a81bb2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:44.500404Z",
     "start_time": "2024-01-15T15:07:44.492920500Z"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a573802",
   "metadata": {},
   "source": [
    "nest_asyncio.apply() patches the existing event loop in a Jupyter Notebook environment to allow nested usage of asyncio.\n",
    "\n",
    "It is utilized later in the notebook to ensure that the asyncio event loop functions correctly within a Jupyter Notebook environment, enabling the concurrent execution of multiple asynchronous retrieval tasks without encountering event loop compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad164e3",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a3c564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:44.816365100Z",
     "start_time": "2024-01-15T15:07:44.496418100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformerEmbeddings with the loaded model\n",
    "local_embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefcccc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:46.483946600Z",
     "start_time": "2024-01-15T15:07:44.816365100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Mistral model from LM studio server\n",
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "# Initialize service context : LLM and Embeddings model for the vector store\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92595283",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6199eada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:48.711116900Z",
     "start_time": "2024-01-15T15:07:46.483946600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138 docs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data connector/ reader. \n",
    "# SimpleDirectoryReader adapt to the document format.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"thesis.pdf\"]\n",
    ")\n",
    "\n",
    "documents  = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6877c",
   "metadata": {},
   "source": [
    "# Load in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bca48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:53.426951700Z",
     "start_time": "2024-01-15T15:07:48.708163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a simple vector store index \n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca889",
   "metadata": {},
   "source": [
    "# Define Advanced Retriever\n",
    "\n",
    "1. Query generation/rewriting: generate multiple queries given the original user query\n",
    "\n",
    "2. Perform retrieval for each query over an ensemble of retrievers.\n",
    "\n",
    "3. Reranking/fusion: fuse results from all queries, and apply a reranking step to “fuse” the top relevant results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c3b0a",
   "metadata": {},
   "source": [
    "## Query Generation/Rewriting\n",
    "In this step we're creating the function to generate k different queries from the original"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_str = \"explain me how we used langchain in the methodology?\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:53.432161400Z",
     "start_time": "2024-01-15T15:07:53.428570700Z"
    }
   },
   "id": "14cc65e6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76320f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:53.437973100Z",
     "start_time": "2024-01-15T15:07:53.431194100Z"
    }
   },
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b875daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:53.443464400Z",
     "start_time": "2024-01-15T15:07:53.437973100Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str # remove the original query\n",
    "    )\n",
    "    \n",
    "    response = llm.generate([fmt_prompt])\n",
    "    \n",
    "    # Assuming there's only one generation in the response\n",
    "    if response.generations and len(response.generations[0]) > 0:\n",
    "        generation_text = response.generations[0][0].text\n",
    "        queries = generation_text.split(\"\\n\")\n",
    "        return queries\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecec81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:55.597278500Z",
     "start_time": "2024-01-15T15:07:53.441463800Z"
    }
   },
   "outputs": [],
   "source": [
    "queries = generate_queries(llm, query_str, num_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2106f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:55.608256500Z",
     "start_time": "2024-01-15T15:07:55.597278500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['1. What is the role of Langchain in our methodology and how was it implemented?',\n '2. Can you provide an example of using Langchain in our research process?',\n '3. How does Langchain enhance collaboration and communication within our team during project execution?']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries # The 3 queries generated from:  \"explain to me how we used langchain in the methodology?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abec5",
   "metadata": {},
   "source": [
    "### More examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a50ff63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.908694200Z",
     "start_time": "2024-01-15T15:07:55.604774700Z"
    }
   },
   "outputs": [],
   "source": [
    "t1 = generate_queries(llm, \"who is the author of the paper\", num_queries=4)\n",
    "t2 = generate_queries(llm, \"What are the conclusion of the research document\", num_queries=4)\n",
    "t3 = generate_queries(llm, \"What is the self attention mechanism\", num_queries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1b4a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.916583500Z",
     "start_time": "2024-01-15T15:07:59.911727700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['1. Who wrote the specific paper with this title?',\n '2. Author name for the given paper publication.',\n '3. Identify the individual(s) that authored the mentioned paper.']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a43efc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.934579800Z",
     "start_time": "2024-01-15T15:07:59.918582600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['1. \"Summary of findings in the research document\"',\n '2. \"Conclusions drawn from the research study\"',\n '3. \"Key takeaways from the research paper\"']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d625c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.969767800Z",
     "start_time": "2024-01-15T15:07:59.932864200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['1. How does the self attention mechanism work in deep learning?',\n '2. What are the benefits of using self attention mechanism in neural networks?',\n '3. Can you explain the mathematical formula for calculating self attention scores?']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ce52d",
   "metadata": {},
   "source": [
    "## Perform Vector Search for Each Query\n",
    "\n",
    "This code defines an asynchronous function run_queries to execute search queries using multiple retrieval methods. For each query, it asynchronously sends requests to each retriever (like a vector retriever and a BM25 retriever). The results are compiled into a dictionary, mapping each query and its position to the corresponding result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7a4672f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.970764100Z",
     "start_time": "2024-01-15T15:07:59.939566200Z"
    }
   },
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"\n",
    "    Run queries against retrievers asynchronously.\n",
    "\n",
    "    :param queries: A list of queries to be processed.\n",
    "    :param retrievers: A list of retriever objects that will process the queries.\n",
    "    :return: A dictionary mapping each query and its index to its corresponding result.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        # For each query, iterate over each retriever.\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            # For each retriever, create an asynchronous task to retrieve the query\n",
    "            # and add it to the tasks list.\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "     # Iterate over each pair of query and its result.\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        # Map each query and its index to its result in the dictionary.\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124ed9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:07:59.971769Z",
     "start_time": "2024-01-15T15:07:59.943278800Z"
    }
   },
   "outputs": [],
   "source": [
    "# vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e69ba8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.276047600Z",
     "start_time": "2024-01-15T15:07:59.949833700Z"
    }
   },
   "outputs": [],
   "source": [
    "# bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0736097",
   "metadata": {},
   "source": [
    "BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.\n",
    "\n",
    "A bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. [Wiki here](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,their%20proximity%20within%20the%20document.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f623bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.334486Z",
     "start_time": "2024-01-15T15:08:00.278041100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 150.47it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c8595",
   "metadata": {},
   "source": [
    "The function is called with a set of queries and two specific retrievers, vector_retriever and bm25_retriever, to fetch and collate their results asynchronously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e47ee",
   "metadata": {},
   "source": [
    "### more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c8d9c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.427023400Z",
     "start_time": "2024-01-15T15:08:00.323522600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 177.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 200.67it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 207.50it/s]\n"
     ]
    }
   ],
   "source": [
    "r1 = await run_queries(t1, [vector_retriever, bm25_retriever])\n",
    "r2 = await run_queries(t2, [vector_retriever, bm25_retriever])\n",
    "r3 = await run_queries(t3, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d7ddd",
   "metadata": {},
   "source": [
    "## Perform Fusion\n",
    "\n",
    "The next step here is to perform fusion: combining the results from several retrievers into one and re-ranking.\n",
    "\n",
    "Note that a given node might be retrieved multiple times from different retrievers, so there needs to be a way to de-dup and rerank the node given the multiple retrievals.\n",
    "\n",
    "This stage perform “reciprocal rank fusion”: for each node, add up its reciprocal rank in every list where it’s retrieved.\n",
    "\n",
    "Then reorder nodes by highest score to least.\n",
    "\n",
    "Full paper [here](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e480d10f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.429596600Z",
     "start_time": "2024-01-15T15:08:00.424713500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llama index function \n",
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59614a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.437658100Z",
     "start_time": "2024-01-15T15:08:00.428020300Z"
    }
   },
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "279b1c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.438742800Z",
     "start_time": "2024-01-15T15:08:00.433125300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** 86988f55-dea7-490c-84a0-041ef2f2564f<br>**Similarity:** 0.03333333333333333<br>**Text:** 60 \nNathan Destrez  straightforward tasks to complex operations.  In addition to its customizable nature, LangChain also \nprovides pre -built chains. These are pre -assembled components designed for specific tasks, enabling \ndevelopers to quickly start projects. For more intricate and unique applications, the framework's \nmodular nature allows for the creation of customized chains, offering a balance between convenience \nand personalization. LangChain's design caters to a diverse range of use...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** c09a2f19-a6a4-43c8-9d28-2ac06484d376<br>**Similarity:** 0.03278688524590164<br>**Text:** 59 \nNathan Destrez  1.10 Virtual Assistants in Industry and Academia  \n1.10.1  The Role of LangChain and Emerging Trends  \nVirtual assistants have emerged as a pivotal innovation, transforming interactions between \nhumans and machines. This literature review delves into the multifaceted world of virtual assistants, \nexamining their development and application across industry and  academia. By exploring the existing \nlandscape of these digital aides, this section aims to shed light on the prog...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in final_results:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba8f8fe",
   "metadata": {},
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53fb7992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.441742800Z",
     "start_time": "2024-01-15T15:08:00.438240100Z"
    }
   },
   "outputs": [],
   "source": [
    "fr1 = fuse_results(r1)\n",
    "fr2 = fuse_results(r2)\n",
    "fr3 = fuse_results(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e433ec78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.449473300Z",
     "start_time": "2024-01-15T15:08:00.439741900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** fc6f78a0-7ee2-4394-b58d-f1fa4e51fa84<br>**Similarity:** 0.03306010928961749<br>**Text:** 4 \nNathan Destrez<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** 735f970d-ea5b-45c7-ad68-91c2f0161382<br>**Similarity:** 0.016666666666666666<br>**Text:** 89 \nNathan Destrez  An initial study comparing the ratio of stop words to total words was conducted, but it did not yield \nsignificant patterns. Consequently, most short texts, predominantly composed of stop words, were \nexcluded from further processing.  \nA key observation during our initial explorations with the embeddings base was the retriever's \noccasional struggle with implicit concepts or unique terminologies present in only a few documents.  \n \nFigure 6 Document retrieved from the Sky...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr1:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f660d3ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.481521700Z",
     "start_time": "2024-01-15T15:08:00.445370900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** 87c6da31-42aa-4a63-a0ae-741f8702d80f<br>**Similarity:** 0.03333333333333333<br>**Text:** 5 \nNathan Destrez   \nContents   \nIntroduction  ................................ ................................ ................................ ................................  9 \nLiterature Review  ................................ ................................ ................................ ......................  11 \n1.1 Historical evolution of chatbots and virtual assistants.  ................................ .................  11 \n1.2 AI in France and the Regulation in Europe  ......<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** 0e24cd10-6edf-43b9-a19a-110b50652db9<br>**Similarity:** 0.016666666666666666<br>**Text:** 49 \nNathan Destrez  1.7.5 Transformers and their role in representing longer textual data.  \nBERT's ability to understand context has naturally extended the use of embeddings from \nindividual words to entire sentences or even longer texts. Sentence Transformers, as discussed in the \narticle \"Understanding BERT\" on Towards AI, take this concept furth er by providing mechanisms to \nderive meaningful sentence -level embeddings. These embeddings can then be used in various NLP \ntasks, such as sem...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr2:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "297c8f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.482517Z",
     "start_time": "2024-01-15T15:08:00.450469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** d0f6f609-996f-4f96-a22b-57625e6bf31b<br>**Similarity:** 0.03333333333333333<br>**Text:** 42 \nNathan Destrez  1.7.3 The Attention mechanism  \nThe concept of Attention within the domain of neural networks has garnered significant interest \ndue to its remarkable impact on enhancing state -of-the-art results across various research fields. This \nincludes areas as diverse as image captioning, language translation, and interactive question \nanswering. Attention has rapidly ascended to become an indispensable instrument in the researcher's \ntoolkit. The assertion by some in the field th...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**Node ID:** 13d442ca-fcab-495a-8a42-ecb1ccb764af<br>**Similarity:** 0.03278688524590164<br>**Text:** 46 \nNathan Destrez  model to 'focus' on the information that is most predictive of the desired outcome. This geometric \nreconfiguration is pivotal in enhancing the model's performance by ensuring that it attends to the \nmost salient features within the data.  \nThe burgeoning field of research has begun to refer to this mechanism as \"Memory,\" positing that this \nterm more aptly describes its functionality. The Attention layer facilitates the model's ability to \"recall\" \nand focus on previously...<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n in fr3:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46e6b",
   "metadata": {},
   "source": [
    "# Plug into RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24f09855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.526262900Z",
     "start_time": "2024-01-15T15:08:00.455607100Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87edde62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.527260Z",
     "start_time": "2024-01-15T15:08:00.511590800Z"
    }
   },
   "outputs": [],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.llm = llm  # Store the llm instance\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(self.llm, query_str, num_queries=4)  # Use the llm instance\n",
    "        results = run_queries(queries, self._retrievers)\n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.839709700Z",
     "start_time": "2024-01-15T15:08:00.515293200Z"
    }
   },
   "id": "035b1147",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cf189ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.846363500Z",
     "start_time": "2024-01-15T15:08:00.841363Z"
    }
   },
   "outputs": [],
   "source": [
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "response_synthesizer= get_response_synthesizer(service_context,streaming=True) # streaming False for classic answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed7d7b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:00.856332400Z",
     "start_time": "2024-01-15T15:08:00.843862800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context, \n",
    "    streaming=True # streaming False for classic answer generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1af30452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:08.053924200Z",
     "start_time": "2024-01-15T15:08:00.856332400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\llama_index\\core\\base_retriever.py:61: RuntimeWarning: coroutine 'run_queries' was never awaited\n",
      "  nodes = self._retrieve(query_bundle)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document appears to be a literature review on virtual assistants, focusing on their development and application in industry and academia. It highlights LangChain as a groundbreaking tool for simplifying the integration and application of Large Language Models (LLMs) in both commercial and academic settings. LangChain's impact is discussed in terms of its ability to create context-aware applications, enhance reasoning capabilities, and offer modular components for customization. The document also mentions LangChain's collaboration with Retrieval Augmented Generation (RAG) and its role in streamlining the creation of advanced virtual assistant applications."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Tell me about the document\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669862b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:08:14.878509300Z",
     "start_time": "2024-01-15T15:08:08.054910600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\llama_index\\core\\base_retriever.py:61: RuntimeWarning: coroutine 'run_queries' was never awaited\n",
      "  nodes = self._retrieve(query_bundle)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The literature review highlights LangChain as a groundbreaking tool that significantly simplifies the integration and application of Large Language Models (LLMs) in both commercial and academic settings. Its impact is multifaceted and profound, enabling developers to create context-aware applications with enhanced reasoning capabilities. LangChain's modular architecture offers versatility and customization options, making it essential for tailoring applications to meet a range of requirements. The tool's ability to streamline the deployment and enhance the accessibility of advanced virtual assistants is crucial for accelerating their adoption in various industries."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What are the conclusions for the future of the tool in the company\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbd64b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T15:06:45.496142300Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
