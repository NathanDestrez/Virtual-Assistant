{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8692ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:45.846879500Z",
     "start_time": "2024-01-15T15:52:40.520380100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "# Data Connector\n",
    "from llama_index import SimpleDirectoryReader\n",
    "# Index\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Llama Index LLM\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "# Other LLM\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Retriever\n",
    "from llama_index.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Display\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "# Asynchrone\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a81bb2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:45.848881500Z",
     "start_time": "2024-01-15T15:52:45.846368900Z"
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a573802",
   "metadata": {},
   "source": [
    "nest_asyncio.apply() patches the existing event loop in a Jupyter Notebook environment to allow nested usage of asyncio.\n",
    "\n",
    "It is utilized later in the notebook to ensure that the asyncio event loop functions correctly within a Jupyter Notebook environment, enabling the concurrent execution of multiple asynchronous retrieval tasks without encountering event loop compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad164e3",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a3c564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:46.155373600Z",
     "start_time": "2024-01-15T15:52:45.848881500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformerEmbeddings with the loaded model\n",
    "local_embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefcccc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:47.754627Z",
     "start_time": "2024-01-15T15:52:46.156322200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Mistral model from LM studio server\n",
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "# Initialize service context : LLM and Embeddings model for the vector store\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92595283",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6199eada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:49.911758800Z",
     "start_time": "2024-01-15T15:52:47.756618900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 138 docs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data connector/ reader. \n",
    "# SimpleDirectoryReader adapt to the document format.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"thesis.pdf\"]\n",
    ")\n",
    "\n",
    "documents  = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6877c",
   "metadata": {},
   "source": [
    "# Load in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97bca48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.229509200Z",
     "start_time": "2024-01-15T15:52:49.910762600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a simple vector store index \n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8636a8ce10e9cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Smart app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14cc65e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.233569600Z",
     "start_time": "2024-01-15T15:52:54.229509200Z"
    }
   },
   "outputs": [],
   "source": [
    "#query_str = \"explain me how we used langchain in the methodology?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76320f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.245068600Z",
     "start_time": "2024-01-15T15:52:54.232566300Z"
    }
   },
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b875daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.250088700Z",
     "start_time": "2024-01-15T15:52:54.247054600Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str # remove the original query\n",
    "    )\n",
    "    \n",
    "    response = llm.generate([fmt_prompt])\n",
    "    \n",
    "    # Assuming there's only one generation in the response\n",
    "    if response.generations and len(response.generations[0]) > 0:\n",
    "        generation_text = response.generations[0][0].text\n",
    "        queries = generation_text.split(\"\\n\")\n",
    "        return queries\n",
    "    else:\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecec81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.264863500Z",
     "start_time": "2024-01-15T15:52:54.250088700Z"
    }
   },
   "outputs": [],
   "source": [
    "#queries = generate_queries(llm, query_str, num_queries=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9abec5",
   "metadata": {},
   "source": [
    "### More examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a4672f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.264863500Z",
     "start_time": "2024-01-15T15:52:54.257279500Z"
    }
   },
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"\n",
    "    Run queries against retrievers asynchronously.\n",
    "\n",
    "    :param queries: A list of queries to be processed.\n",
    "    :param retrievers: A list of retriever objects that will process the queries.\n",
    "    :return: A dictionary mapping each query and its index to its corresponding result.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        # For each query, iterate over each retriever.\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            # For each retriever, create an asynchronous task to retrieve the query\n",
    "            # and add it to the tasks list.\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "     # Iterate over each pair of query and its result.\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        # Map each query and its index to its result in the dictionary.\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124ed9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.265861200Z",
     "start_time": "2024-01-15T15:52:54.260973Z"
    }
   },
   "outputs": [],
   "source": [
    "# vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69ba8a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.593334800Z",
     "start_time": "2024-01-15T15:52:54.265861200Z"
    }
   },
   "outputs": [],
   "source": [
    "# bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f623bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.600238900Z",
     "start_time": "2024-01-15T15:52:54.596130300Z"
    }
   },
   "outputs": [],
   "source": [
    "#results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e480d10f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.618419200Z",
     "start_time": "2024-01-15T15:52:54.603642100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Llama index function \n",
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59614a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.619416Z",
     "start_time": "2024-01-15T15:52:54.614591300Z"
    }
   },
   "outputs": [],
   "source": [
    "#final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46e6b",
   "metadata": {},
   "source": [
    "# Plug into RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24f09855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:54.669763300Z",
     "start_time": "2024-01-15T15:52:54.619416Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7e6f7366744347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:56.900828900Z",
     "start_time": "2024-01-15T15:52:54.671756500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# query_str = \"explain me how we used langchain in the methodology?\"\n",
    "# queries = generate_queries(llm, query_str, num_queries=4)\n",
    "# results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87edde62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:53:32.529381100Z",
     "start_time": "2024-01-15T15:53:32.509413600Z"
    }
   },
   "outputs": [],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self.llm = llm  # Store the llm instance\n",
    "        self.query_str = \" \"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(self.llm, query_bundle, num_queries=4)  # Use the llm instance\n",
    "        result =  asyncio.run(run_queries(queries, [vector_retriever, bm25_retriever])) \n",
    "        final_results = fuse_results(\n",
    "            result, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "035b1147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.246160200Z",
     "start_time": "2024-01-15T15:52:56.906809Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=\"NULL\",temperature=0,openai_api_base=\"http://192.168.48.33:1234/v1\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf189ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.253099800Z",
     "start_time": "2024-01-15T15:52:57.248120400Z"
    }
   },
   "outputs": [],
   "source": [
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "response_synthesizer= get_response_synthesizer(service_context,streaming=True) # streaming False for classic answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed7d7b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.265389900Z",
     "start_time": "2024-01-15T15:52:57.253099800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context, \n",
    "    streaming=True # streaming False for classic answer generation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1af30452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T15:52:57.610799200Z",
     "start_time": "2024-01-15T15:52:57.266390500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 96.77it/s]\n",
      "C:\\Users\\Nathan.destrez\\AppData\\Local\\anaconda3\\envs\\VA\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document discusses the Natural Language Generation (NLG) process, which involves filtering and analyzing input data to determine main topics and relationships between them. This is followed by interpreting the data, creating a narrative structure, summarizing the topic accurately, ensuring grammatical correctness, and formatting the generated text according to a template or format. The NLG process relies on advanced machine learning models such as Markov chains, recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and transformer models like GPT, BERT, and XLNet. Markov chains generate text based on the previous words in a sequence, while the other models provide more nuanced understanding of data sets. The document also mentions an issue with a retriever's struggle to understand implicit concepts or unique terminologies present in only a few documents, which was mitigated by introducing additional contextual information into the embeddings. Additionally, a PDF-based method for documentation integration was developed to manage and integrate PDF formatted documents into the knowledge base. This method involves extracting crucial metadata from PDF documents and optimizing the extraction and transformation of documentation content into usable data for the virtual assistant."
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"Tell me about the document\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669862b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T15:52:57.609803Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_response = query_engine.query(\n",
    "    \"What are the conclusions for the future of the tool in the company\",\n",
    ")\n",
    "\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc07ac439acf612",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T15:52:57.610799200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70afff-99cb-4110-a72c-c411dabbca44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
