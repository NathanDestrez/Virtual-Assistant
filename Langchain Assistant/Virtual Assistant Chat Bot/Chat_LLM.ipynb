{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f12fbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Path/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Chroma\n",
    "import chromadb \n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "import time\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae239881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fc37e",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcb8837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chroma_client = client = chromadb.PersistentClient(path='Path')\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763b82e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 473 in the collection\n"
     ]
    }
   ],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=\"PRODUCTX\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dca6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_docs(question, similar_doc_count):\n",
    "    retrieved_docs = langchain_chroma.similarity_search(question, k=similar_doc_count)\n",
    "    return retrieved_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503d230",
   "metadata": {},
   "source": [
    "# Build QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cebc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "  Instruction:\n",
    "  You are an assistant to answer question about system management in Company .\n",
    "  Use only information in the following paragraphs to answer the question at the end.\n",
    "  Explain the answer with reference to these paragraphs.\n",
    "  If you don't have the information in paragraph then give response \"I dont't know\".\n",
    "\n",
    "  {context}\n",
    "\n",
    "  Question: {question}\n",
    "\n",
    "  Response:\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "292266ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qa_chain(model_name_key):\n",
    "    model_paths = {\n",
    "        \"Dolly\": \"/Path/models/Dolly\",\n",
    "        \"Dolly_7\": \"/Path/models/Dolly_7\",\n",
    "        \"Mistral\": \"/Path/models/Mistral\"\n",
    "    }\n",
    "\n",
    "    # Retrieve the model path using the provided key\n",
    "    model_path = model_paths.get(model_name_key)\n",
    "    if not model_path:\n",
    "        raise ValueError(f\"Model name key '{model_name_key}' is not valid. Choose from {list(model_paths.keys())}.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_4bit=True)#torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    instruct_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    num_beams=1,  # Explicitly setting to single-beam (greedy) decoding\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={'load_in_4bit': False}\n",
    ")\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "    hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
    "    return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True)# Note: if you use dolly 12B or smaller model but a GPU with less than 24GB RAM, use 8bit. This requires %pip install bitsandbytes\n",
    "  # instruct_pipeline = pipeline(model=model_name, trust_remote_code=True, device_map=\"auto\", model_kwargs={'load_in_8bit': True})\n",
    "  # For GPUs without bfloat16 support, like the T4 or V100, use torch_dtype=torch.float16 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ab7cc82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qa_chain = build_qa_chain(\"Dolly_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eef594",
   "metadata": {},
   "source": [
    "# Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc9d2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayHTML(html):\n",
    "    \"\"\"Display HTML in Jupyter notebook.\"\"\"\n",
    "    from IPython.display import HTML\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a1aefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    similar_docs = get_similar_docs(question, similar_doc_count=3)\n",
    "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
    "    result_html = f\"<p><blockquote style=\\\"font-size:24\\\">{question}</blockquote></p>\"\n",
    "    result_html += f\"<p><blockquote style=\\\"font-size:18px\\\">{result['output_text']}</blockquote></p>\"\n",
    "    result_html += \"<p><hr/></p>\"\n",
    "    for d in result[\"input_documents\"]:\n",
    "        source_id = d.metadata[\"source\"]\n",
    "        result_html += f\"<p><blockquote>{d.page_content}<br/>(Source: {source_id}\\\">{source_id}</a>)</blockquote></p>\"\n",
    "    displayHTML(result_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0b7d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_stream(question):\n",
    "    # Retrieve similar documents based on the question.\n",
    "    similar_docs = get_similar_docs(question, similar_doc_count=3)\n",
    "\n",
    "    # Process the question through your QA chain.\n",
    "    result = qa_chain({\"input_documents\": similar_docs, \"question\": question})\n",
    "\n",
    "    # Start building your HTML content (though this part won't be \"typed\" out word by word).\n",
    "    result_html = f\"<p><blockquote style=\\\"font-size:24\\\">{question}</blockquote></p>\"\n",
    "\n",
    "    # Here is where we will start the \"typing\" effect for the answer.\n",
    "    # First, we split the result into words.\n",
    "    words = result['output_text'].split()\n",
    "\n",
    "    # \"Type\" each word with a delay.\n",
    "    for word in words:\n",
    "        # Append the word to your HTML content.\n",
    "        result_html += word + ' '\n",
    "        \n",
    "        # This will create the dynamic \"typing\" display.\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(result_html))  # Display current state.\n",
    "        time.sleep(0.5)  # Delay between \"typing\" each word.\n",
    "\n",
    "    # After the answer, continue with your HTML content.\n",
    "    result_html += \"<p><hr/></p>\"\n",
    "    for d in result[\"input_documents\"]:\n",
    "        source_id = d.metadata[\"source\"]\n",
    "        result_html += f\"<p><blockquote>{d.page_content}<br/>(Source: <a href=\\\"URL{source_id}\\\">{source_id}</a>)</blockquote></p>\"\n",
    "\n",
    "    # Finally, display the full content.\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(result_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51a5b9",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65190972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_question_stream(\"What is The Warranty Inspection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d83b841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "  Instruction:\n",
      "  You are an assistant to answer question about system management in Company .\n",
      "  Use only information in the following paragraphs to answer the question at the end.\n",
      "  Explain the answer with reference to these paragraphs.\n",
      "  If you don't have the information in paragraph then give response \"I dont't know\".\n",
      "\n",
      "  Context : (Documentation = Administrastion Manual, Title = Data & filesystems, Chapter = Disks) SDD drives are highly recommended . No RAID is required , although a RAID-0 is useful to benefit from larger volume of storage . Tool is also designed to operate on HDD , in such cases fast HDD with RAID10 is recommended . Each Tool node is designed hold up an amount of about 4TiB of data and requires a margin for maintenance operations ( hence it is required to provision 50% of additional storage space ). It is recommended to have large storage for big data distributed database systems ( Cassandra data , OpenSearch ) - there is no need for disk redundancy ( RAID ) It is recommended to have large redundant storage for RocksDB if enabled ( alternative to Cassandra ) It is recommended to have redundant storage on a NFS server for Jupyter notebooks\n",
      "\n",
      "Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Create a simple data set) It works quite the same as with other BIRT data sources . Create a new report with File>New>Other>Business Intelligence and Reporting Tool>Report In the Data Explorer tab right click on Data Sources to create a Tool Data Source When your data source has been created , right click on Data Sets to create a new data set with a query to Tool Once you have chosen the name of your data set and clicked on Next , the Tool Web UI should appear . You can build your query and generate it by clicking on Graph or Generate query . You can either validate your dataset or use the other tab to parameterize your query . You can now use this data in all your BIRT tables and charts . A comprehensive BIRT tutorial can be found on the official webpage : http ://eclipse . org/birt/documentation/tutorial/index . php\n",
      "\n",
      "Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances) You may want to use a NFS Server to share notebooks and other resources between different nodes . It requires one NFS server which can be one of the nodes of the Tool cluster ( cf . Configure a NFS server example configuration below if needed ). A NFS server might not be a highly available device and creates a single point of failure , hence it is particularly important to have a proper backup policy of the data . However backups are required for all data . NFS protocol is not suitable for databases , but is useful to share common data like Jupyter notebooks , some configutation or customization data . Also , please do not forget to backup the NFS data often enough , in order to avoid data loss in case of failure of the NFS Server .\n",
      "\n",
      "  Question: What should i do before making a purchase\n",
      "\n",
      "  Response:\n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\">What should i do before making a purchase</blockquote></p><p><blockquote style=\"font-size:18px\">\n",
       "  Before making a purchase, it is recommended to have a large storage for big data distributed database systems such as Cassandra data and OpenSearch. It is also recommended to have redundant storage on a NFS server for Jupyter notebooks. Additionally, it is important to have a proper backup policy for all data.</blockquote></p><p><hr/></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Data & filesystems, Chapter = Disks) SDD drives are highly recommended . No RAID is required , although a RAID-0 is useful to benefit from larger volume of storage . Tool is also designed to operate on HDD , in such cases fast HDD with RAID10 is recommended . Each Tool node is designed hold up an amount of about 4TiB of data and requires a margin for maintenance operations ( hence it is required to provision 50% of additional storage space ). It is recommended to have large storage for big data distributed database systems ( Cassandra data , OpenSearch ) - there is no need for disk redundancy ( RAID ) It is recommended to have large redundant storage for RocksDB if enabled ( alternative to Cassandra ) It is recommended to have redundant storage on a NFS server for Jupyter notebooks<br/>(Source: /Data & filesystems/Disks\">/Data & filesystems/Disks</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Create a simple data set) It works quite the same as with other BIRT data sources . Create a new report with File>New>Other>Business Intelligence and Reporting Tool>Report In the Data Explorer tab right click on Data Sources to create a Tool Data Source When your data source has been created , right click on Data Sets to create a new data set with a query to Tool Once you have chosen the name of your data set and clicked on Next , the Tool Web UI should appear . You can build your query and generate it by clicking on Graph or Generate query . You can either validate your dataset or use the other tab to parameterize your query . You can now use this data in all your BIRT tables and charts . A comprehensive BIRT tutorial can be found on the official webpage : http ://eclipse . org/birt/documentation/tutorial/index . php<br/>(Source: /Reporting/Walkthrough/Create a simple data set\">/Reporting/Walkthrough/Create a simple data set</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances) You may want to use a NFS Server to share notebooks and other resources between different nodes . It requires one NFS server which can be one of the nodes of the Tool cluster ( cf . Configure a NFS server example configuration below if needed ). A NFS server might not be a highly available device and creates a single point of failure , hence it is particularly important to have a proper backup policy of the data . However backups are required for all data . NFS protocol is not suitable for databases , but is useful to share common data like Jupyter notebooks , some configutation or customization data . Also , please do not forget to backup the NFS data often enough , in order to avoid data loss in case of failure of the NFS Server .<br/>(Source: /Sharing Resources between different instances\">/Sharing Resources between different instances</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question(\"What should i do before making a purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08e8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is Tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb54464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\">How to push constellation Data</blockquote></p>To push constellation data to Tool, you can use the REST protocol and submit custom data via extensions. Please contact Company for more information. <p><hr/></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Special Types, Paragraph = constellation_diagram) The constellation_diagram is a custom type provided by Tool which provides support for RF signal constellation diagram . When the constellation_diagram type is used , the value field is no longer a number but an object with the fields : sweep_time_seconds ( float ) num_pts_constellation ( integer ) constellation_x ( array of integers ) constellation_y ( array of integers ) constellation_x and constellation_y data is normalized to a single byte integer value , between -128 to 127 . The contents of constellation x and y contains constellation diagram data in even indices , and eye pattern diagram data in odd indices . Example : [ { \"name\": \"constellation . diagram\", \"timestamp\": 1359786400000 , \"type\": \"constellation_diagram\", \"value\": { \"sweep_time_seconds\": 0 . 002 , \"num_pts_constellation\": 4 , \"constellation_x\": [ 24 , -4 , -53 , -64 ], \"constellation_y\": [ 12 , -5 , 14 , -6 ] }, \"tags\": { \"host\": \"test\" } }] 2014-2023 , Company Communications Tool Administration Manual - KC-153-MA-0019 v . 2023-dev-S17<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//Add Data Points/Special Types/constellation_diagram\">/Add Data Points/Special Types/constellation_diagram</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated constellation diagram graph) If the Constellation view type is selected , the resulting series are displayed in a constellation graph . Select metric / group / number of symbols for eye diagram Constellation diagram Eye diagrams Selection of charts to display Chart showing in phase vs quadrature through time Chart showing phase through time Chart showing amplitudes through time Viewport for selecting visible time in the charts within the current sample ( showing miniature of current charts of selected capture ) Sample player that allows to navigate through captures made over time and/or manage playback<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//Web interface/Generated constellation diagram graph\">/Web interface/Generated constellation diagram graph</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I push custom data to Tool?) You can submit custom data to Tool by using the REST protocol . See Add Data Points . Custom data types are supported via extensions . Please contact Company for more information . 2014-2023 , Company Communications Tool Administration Manual - KC-153-MA-0019 v . 2023-dev-S17<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//FAQ/Can I push custom data to Tool?\">/FAQ/Can I push custom data to Tool?</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question_stream('How to push constellation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e9c80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "  Instruction:\n",
      "  You are an assistant to answer question about system management .\n",
      "  Use only information in the following paragraphs to answer the question at the end.\n",
      "  Explain the answer with reference to these paragraphs.\n",
      "  If you don't have the information in paragraph then give response \"I dont't know\".\n",
      "\n",
      "  Context : (Documentation = Administrastion Manual, Title = Throwaway (no storage) datastore) This datastore is not saving any data . It can be used for testing performances or behaviour of Tool data points push interfaces and/or of the network under stress . It provides a single metric with a counter of datapoints received over the past hour .\n",
      "\n",
      "Context : (Documentation = Administrastion Manual, Title = Technology for the Satellite Ground Ecosystem, Chapter = Cutting Edge Technology) … including satellite ground apps that can be reconfigured as mission shifts and orchestrated services that keep the ground in sync with new software-defined satellites and soon , connect with 5G .\n",
      "\n",
      "Context : (Documentation = Administrastion Manual, Title = Tool Architecture, Chapter = Introduction) Tool is a data storage & analytics service . It relies on other server-side high-quality services : Grafana ( dashboard ) and usually Cassandra ( datastore ). Note that third-party software are optional and can be replaced by alternatives to match any need or requirement . cf . Integrated third-party software .\n",
      "\n",
      "  Question:  Explain me in detail how to push constellation Data\n",
      "\n",
      "  Response:\n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\"> Explain me in detail how to push constellation Data</blockquote></p><p><blockquote style=\"font-size:18px\">\n",
       "  To push constellation data to the Throwaway datastore, you can follow these steps:\n",
       "\n",
       "  1. Open the documentation and navigate to the Architecture chapter.\n",
       "  2. Under the Architecture section, you will find information about the  service, which relies on other server-side high-quality services such as Grafana (dashboard) and usually Cassandra (datastore).\n",
       "  </blockquote></p><p><hr/></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Throwaway (no storage) datastore) This datastore is not saving any data . It can be used for testing performances or behaviour of Tool data points push interfaces and/or of the network under stress . It provides a single metric with a counter of datapoints received over the past hour .<br/>(Source: /Throwaway (no storage) datastore\">/Throwaway (no storage) datastore</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Technology for the Satellite Ground Ecosystem, Chapter = Cutting Edge Technology) … including satellite ground apps that can be reconfigured as mission shifts and orchestrated services that keep the ground in sync with new software-defined satellites and soon , connect with 5G .<br/>(Source: /Technology for the Satellite Ground Ecosystem/Cutting Edge Technology\">/Technology for the Satellite Ground Ecosystem/Cutting Edge Technology</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Tool Architecture, Chapter = Introduction) Tool is a data storage & analytics service . It relies on other server-side high-quality services : Grafana ( dashboard ) and usually Cassandra ( datastore ). Note that third-party software are optional and can be replaced by alternatives to match any need or requirement . cf . Integrated third-party software .<br/>(Source: /Tool Architecture/Introduction\">/Tool Architecture/Introduction</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question(' Explain me in detail how to push constellation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58a693c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "  Instruction:\n",
      "  You are an assistant to answer question about system management .\n",
      "  Use only information in the following paragraphs to answer the question at the end.\n",
      "  Explain the answer with reference to these paragraphs.\n",
      "  If you don't have the information in paragraph then give response \"I dont't know\".\n",
      "\n",
      "  Context : (Documentation = Administrastion Manual, Title = Time Series Python Connector, Chapter = Send data points to server) Use DataPointBuilder and add_data_points to send data points to the Tool server . from Product1TS import Product1TSPythonConnector from Product1TS import DataPoint from datetime import datetime timestamp_Product2 = int ( datetime . now (). timestamp ()*1000 ) point_value = 12 api = Product1TSPythonConnector . API . init (\"http ://url-to-Tool/api/v1/\") dp = DataPoint . DataPointBuilder ()\\ . with_point ( timestamp_Product2 , point_value )\\ . with_name (\"test_metric_name\")\\ . with_tag (\"sensor\", \"bmp280\")\\ . with_tag (\"location\", \"Tool dev room\") api . add_data_points ( DataPoint . DataPointsPayloadBuilder (). add_builder ( dp ). build ())\n",
      "\n",
      "Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I push custom data to the tool?) You can submit custom data to the tool by using the REST protocol . See Add Data Points . Custom data types are supported via extensions . Please contact Company for more information . 2014-2023 , Company Communications Tool Administration Manual - KC-153-MA-0019 v . 2023-dev-S17\n",
      "\n",
      "Context : (Documentation = User Manual, Title = FAQ, Chapter = How much data can be displayed on Tool?) By default there are 3 safeguards for the size of data displayed on Tool : A limit of 100 series ( group by ) A limit of 10 000 000 points before aggregation A limit of 10 000 points after aggregation These can be overridden by clicking on the padlock icon on the right of the query interface , but be aware that plotting a large number of points is rarely informative and causes performance issues .\n",
      "\n",
      "  Question: What is the add data points in Tool ?\n",
      "\n",
      "  Response:\n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\">What is the add data points in Tool ?</blockquote></p><p><blockquote style=\"font-size:18px\">\n",
       "  The add data points in Tool can be done using the REST protocol. The custom data types are supported via extensions. Please contact Company for more information. The limit of 100 series (group by), 10 000 000 points before aggregation, and 10 000 points after aggregation can be overridden by clicking on the padlock icon on the right of the query interface, but be aware</blockquote></p><p><hr/></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Tool Time Series Python Connector, Chapter = Send data points to Tool server) Use DataPointBuilder and add_data_points to send data points to the Tool server . from Product1TS import Product1TSPythonConnector from Product1TS import DataPoint from datetime import datetime timestamp_Product2 = int ( datetime . now (). timestamp ()*1000 ) point_value = 12 api = Product1TSPythonConnector . API . init (\"http ://url-to-Tool/api/v1/\") dp = DataPoint . DataPointBuilder ()\\ . with_point ( timestamp_Product2 , point_value )\\ . with_name (\"test_metric_name\")\\ . with_tag (\"sensor\", \"bmp280\")\\ . with_tag (\"location\", \"Tool dev room\") api . add_data_points ( DataPoint . DataPointsPayloadBuilder (). add_builder ( dp ). build ())<br/>(Source: /Tool Time Series Python Connector/Send data points to Tool server\">/Tool Time Series Python Connector/Send data points to Tool server</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I push custom data to Tool?) You can submit custom data to Tool by using the REST protocol . See Add Data Points . Custom data types are supported via extensions . Please contact Company for more information . 2014-2023 , Company Communications Tool Administration Manual - KC-153-MA-0019 v . 2023-dev-S17<br/>(Source: /FAQ/Can I push custom data to Tool?\">/FAQ/Can I push custom data to Tool?</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = FAQ, Chapter = How much data can be displayed on Tool?) By default there are 3 safeguards for the size of data displayed on Tool : A limit of 100 series ( group by ) A limit of 10 000 000 points before aggregation A limit of 10 000 points after aggregation These can be overridden by clicking on the padlock icon on the right of the query interface , but be aware that plotting a large number of points is rarely informative and causes performance issues .<br/>(Source: /FAQ/How much data can be displayed on Tool?\">/FAQ/How much data can be displayed on Tool?</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question('What is the add data points in Tool ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ad9a1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "  Instruction:\n",
      "  You are an assistant to answer question about system management in Company .\n",
      "  Use only information in the following paragraphs to answer the question at the end.\n",
      "  Explain the answer with reference to these paragraphs.\n",
      "  If you don't have the information in paragraph then give response \"I dont't know\".\n",
      "\n",
      "  Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview) The correlation UI is a web client that allows you to build correlation queries , execute them , and display their result .\n",
      "\n",
      "Context : (Documentation = User Manual, Title = User Interface, Chapter = Analyzing results, Paragraph = Correlation search) When you execute your correlation search query , the results are returned in the form of a bar chart . The plot of the reference series is also displayed . The bar chart represents the correlation scores sorted by strength . The height of the bar is the result of the similarity measure . Its color represents the reliability of this measure . If it has been established on a large number of samples , and thus it is reliable , it will be green . If the number of samples is lower , it is represented in orange or red . To get more information on each correlations , you can pass your cursor on the bar chart to get a detailed tooltip . You can also click on a bar to query for the corresponding series . This allows you to visualize the raw data and better understand the correlation scores .\n",
      "\n",
      "Context : (Documentation = User Manual, Title = Correlations) User Interface Overview Building a correlation query Analyzing results Correlation API Correlation Search API Correlation Matrix API Details on similarity measures Discarding linear correlations DTW References 2014-2023 , Company Communications Tool User Manual - KC-153-UM-0020 v . 2023-dev-S17\n",
      "\n",
      "  Question: how do I correlate 2 datas?\n",
      "\n",
      "  Response:\n",
      "  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\">how do I correlate 2 datas?</blockquote></p><p><blockquote style=\"font-size:18px\">\n",
       "  To correlate 2 datas in Company, you can use the correlation search API. When you execute your correlation search query, the results are returned in the form of a bar chart. The plot of the reference series is also displayed. The bar chart represents the correlation scores sorted by strength. The height of the bar is the result of the similarity measure. Its color represents the reliability of this measure. If it has been established on a large number of samples, and</blockquote></p><p><hr/></p><p><blockquote>Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview) The correlation UI is a web client that allows you to build correlation queries , execute them , and display their result .<br/>(Source: /User Interface/Overview\">/User Interface/Overview</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = User Interface, Chapter = Analyzing results, Paragraph = Correlation search) When you execute your correlation search query , the results are returned in the form of a bar chart . The plot of the reference series is also displayed . The bar chart represents the correlation scores sorted by strength . The height of the bar is the result of the similarity measure . Its color represents the reliability of this measure . If it has been established on a large number of samples , and thus it is reliable , it will be green . If the number of samples is lower , it is represented in orange or red . To get more information on each correlations , you can pass your cursor on the bar chart to get a detailed tooltip . You can also click on a bar to query for the corresponding series . This allows you to visualize the raw data and better understand the correlation scores .<br/>(Source: /User Interface/Analyzing results/Correlation search\">/User Interface/Analyzing results/Correlation search</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = Correlations) User Interface Overview Building a correlation query Analyzing results Correlation API Correlation Search API Correlation Matrix API Details on similarity measures Discarding linear correlations DTW References 2014-2023 , Company Communications Tool User Manual - KC-153-UM-0020 v . 2023-dev-S17<br/>(Source: /Correlations\">/Correlations</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question('how do I correlate 2 datas?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c53bbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><blockquote style=\"font-size:24\">Give me a list of aggregators in syminer</blockquote></p>There are many different aggregators available in Tool. The default aggregator option selected in the UI is Average. <p><hr/></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Aggregators) Note The list of aggregators is incomplete and depends on enabled modules . This documentation will be generated automatically in the future using the features API .<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//Aggregators\">/Aggregators</a>)</blockquote></p><p><blockquote>Context : (Documentation = User Manual, Title = Aggregators, Chapter = Description) There are many different aggregators available in Tool . Note : The default aggregator option selected in the UI is Average<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//Aggregators/Description\">/Aggregators/Description</a>)</blockquote></p><p><blockquote>Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Metric Properties) nameThe name of the metric ( s ) to return data points for . The name is required . aggregatorsThis is an ordered array of aggregators . They are processed in the order specified . The output of an aggregator is passed to the input of the next until all have been processed . If no aggregator is specified , then all data points are returned . Most aggregators support downsampling . Downsampling allows you to reduce the sampling rate of the data points and aggregate these values over a longer period of time . For example , you could average all daily values over the last week . Rather than getting 7 values you would get one value which is the average for the week . Sampling is specified with a âvalueâ and a âunitâ. value - An integer value . unit - The time range . Possible unit values are âmillisecondsâ, âsecondsâ, âminutesâ, âhoursâ, âdaysâ, âweeksâ, âmonthsâ, and âyearsâ. align_sampling - An optional property . Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The effect of setting this to true is that your data will take the same shape when graphed as you refresh the data . This is false by default . Note that align_sampling and align_start_time are mutually exclusive . If more than one are set , unexpected results will occur . align_start_time - An optional property . When set to true the time for the aggregated data point for each range will fall on the start of the range instead of being the value for the first data point within that range . This is false by default . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur . align_end_time - An optional property . Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The difference between align_start_time and align_end_time is that align_end_time sets the timestamp for the datapoint to the beginning of the following period versus the beginning of the current period . As with align_start_time , setting this to true will cause your data to take the same shape when graphed as you refresh the data . Note that align_start_time and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur . start_time - An optional property . Used along with align_start_time . This is the alignment start time . This defaults to 0 . tagsTags narrow down the search . Only metrics that include the tag and matches one of the values are returned . Tags is optional . group_byThe resulting data points can be grouped by one or more tags , a time range , or by value , or by a combination of the three . The âgroup_byâ property in the query is an array of one or more groupers . Each grouper has a name and then additional properties specific to that grouper . See Grouping for information . Note that grouping by a time range , by value , or by bins can slow down the query . exclude_tags By default , the result of the query includes tags and tag values associated with the data points . If exclude_tags is set to true , the tags will be excluded from the response . limit Limits the number of data points returned from the data store . The limit is applied before any aggregator is executed . order Orders the returned data points . Values for order are âascâ for ascending or âdescâ for descending . Defaults to ascending . Thissorting is done before any aggregators are executed .<br/>(Source: <a href=\"https://gardening.stackexchange.com/a//Query Metrics/Metric Properties\">/Query Metrics/Metric Properties</a>)</blockquote></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_question_stream('Give me a list of aggregators in syminer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e659d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
