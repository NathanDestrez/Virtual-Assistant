{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0285476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan_2/anaconda3/envs/Mistral/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Chroma\n",
    "import chromadb \n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "import time\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d8031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "        \"Dolly\": \"/home/nathan_2/DL2_Kratos_data-Science/models/Dolly\",\n",
    "        \"Dolly_7\": \"/home/nathan_2/DL2_Kratos_data-Science/models/Dolly_7\",\n",
    "        \"Mistral\": \"/home/nathan_2/DL2_Kratos_data-Science/models/Mistral\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f7f402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:00<00:00, 60.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load your local model\n",
    "model_path = \"/home/nathan_2/DL2_Kratos_data-Science/models/Mistral\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", load_in_4bit=True)#torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87508c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check the device of the model\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c0b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your local tokenizer and set pad_token_id to eos_token_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028e0da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/nathan_2/anaconda3/envs/Mistral/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/nathan_2/anaconda3/envs/Mistral/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A list of colors: red, blue, green, yellow, orange, purple, pink,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b311d",
   "metadata": {},
   "source": [
    "## Greedy Decoding Version\n",
    "For greedy decoding, set do_sample to False, and ensure num_beams is set to 1 (or omitted, as 1 is the default value). In greedy decoding, top_p and top_k are not relevant.\n",
    "\n",
    "This version will generate text using the most probable next word at each step. It's faster and more deterministic but may lack diversity in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8644097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_greedy = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,  # Greedy decoding\n",
    "    num_beams=1,  # Explicitly setting to single-beam (greedy) decoding\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={'load_in_4bit': False}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f6d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain me in detail what is a neural network.\n",
      "A neural network is a type of machine learning algorithm that is designed to model complex relationships between inputs and outputs. It is composed of a series of interconnected nodes, called neurons, that are organized into layers. Each neuron receives input from other neurons in the previous layer, processes the input using a set of mathematical operations, and then sends output to neurons in the next layer.\n",
      "\n",
      "The input to the neural network is typically a set of features or characteristics of the data that\n",
      "Greedy Decoding Time: 1.8158848285675049 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the start time\n",
    "start_time_greedy = time.time()\n",
    "\n",
    "# Generate text using Greedy Decoding\n",
    "res_greedy = generate_text_greedy(\"Explain me in detail what is a neural network.\")\n",
    "print(res_greedy[0][\"generated_text\"])\n",
    "\n",
    "# Measure the end time and calculate the duration\n",
    "end_time_greedy = time.time()\n",
    "duration_greedy = end_time_greedy - start_time_greedy\n",
    "print(f\"Greedy Decoding Time: {duration_greedy} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c17ab",
   "metadata": {},
   "source": [
    "## Sample Decoding Version\n",
    "For sample decoding, do_sample is set to True, and you can utilize top_p and top_k to control the sampling process. num_beams is not relevant in this scenario.\n",
    "\n",
    "This version introduces randomness into the generation process, resulting in more diverse and less predictable text. The top_p and top_k parameters allow you to fine-tune the balance between randomness and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83684862",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_sample = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,  # Sample decoding\n",
    "    top_p=0.50,  # Controls the randomness in sample decoding\n",
    "    top_k=50,  # Controls the randomness in sample decoding\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={'load_in_8bit': False}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b0f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain me what is a neural network.\n",
      "\n",
      "A neural network is a type of artificial intelligence (AI) model that is designed to simulate the way the human brain works. It is composed of interconnected nodes, called neurons, that work together to process and analyze data.\n",
      "\n",
      "Neural networks are trained on large amounts of data to recognize patterns and make predictions. The data is fed into the network, which processes it and outputs a result. The network is then adjusted based on the accuracy of the output to improve its performance\n",
      "Sample Decoding Time: 1.872246265411377 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the start time\n",
    "start_time_sample = time.time()\n",
    "\n",
    "# Generate text using Sample Decoding\n",
    "res_sample = generate_text_sample(\"Explain me what is a neural network.\")\n",
    "print(res_sample[0][\"generated_text\"])\n",
    "\n",
    "# Measure the end time and calculate the duration\n",
    "end_time_sample = time.time()\n",
    "duration_sample = end_time_sample - start_time_sample\n",
    "print(f\"Sample Decoding Time: {duration_sample} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce8004",
   "metadata": {},
   "source": [
    "## Beam Search Explained\n",
    "Multiple Beams: Instead of just considering the single most probable next word at each step (as in greedy decoding), beam search keeps track of multiple possible sequences (beams) at each time step.\n",
    "How It Works: At each step in the sequence, for each beam, the model considers multiple next-word options (each word is a possible extension of the beam). It then keeps only the most probable num_beams sequences for the next step.\n",
    "Trade-offs: Beam search balances between exploring a variety of possible sequences (thus potentially finding a more optimal or coherent overall sequence) and computational efficiency. However, it is more computationally intensive than greedy decoding because it evaluates multiple sequences in parallel.\n",
    "\n",
    "### Impact of Increasing num_beams\n",
    "#### Quality of Output:  \n",
    "Generally, increasing the number of beams can lead to better-quality outputs. The model has the chance to explore and compare more sequence options, potentially leading to more coherent and contextually appropriate text.\n",
    "\n",
    "#### Computation Time: \n",
    "More beams mean more sequences to evaluate at each step, leading to higher computational overhead. This typically results in slower text generation compared to greedy decoding.\n",
    "\n",
    "#### Balance Between Exploration and Determinism: \n",
    "With more beams, the model strikes a balance between the determinism of greedy decoding (which might miss contextually better but less obvious choices) and the randomness of sampling methods (which might be too diverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e1166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_beam_search = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,  # Using beam search, not greedy decoding\n",
    "    num_beams=5,  # Increasing beams for beam search\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={'load_in_8bit': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7e97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain me what is a neural network.\n",
      "\n",
      "A neural network is a type of artificial intelligence (AI) system that is modeled after the structure and function of the human brain. It is composed of interconnected nodes, called neurons, that process and transmit information in a way that allows the network to learn and make predictions or decisions based on input data.\n",
      "\n",
      "Neural networks are used in a variety of applications, including image and speech recognition, natural language processing, and predictive analytics. They are particularly well-suited\n",
      "Sample Decoding Time: 9.903196096420288 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the start time\n",
    "start_time_sample = time.time()\n",
    "\n",
    "# Generate text using Sample Decoding\n",
    "res_sample = generate_text_beam_search(\"Explain me what is a neural network.\")\n",
    "print(res_sample[0][\"generated_text\"])\n",
    "\n",
    "# Measure the end time and calculate the duration\n",
    "end_time_sample = time.time()\n",
    "duration_sample = end_time_sample - start_time_sample\n",
    "print(f\"Sample Decoding Time: {duration_sample} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed3407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab556910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221a4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
