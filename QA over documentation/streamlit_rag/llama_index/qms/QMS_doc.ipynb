{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from urllib.parse import urlparse, unquote\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import nltk\n",
    "from llama_index import Document\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Suppress only the InsecureRequestWarning from urllib3 needed for ignoring SSL warnings\n",
    "requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def get_all_links_recursive(url, depth=3):\n",
    "    if depth == 0:\n",
    "        return []\n",
    "    try:\n",
    "        response = requests.get(url, verify=True)  # Ignore SSL certificate verification\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        absolute_links = [urljoin(url, link['href']) for link in links]\n",
    "        # Print links for the current page\n",
    "        print(f\"Links from {url}:\")\n",
    "        for link in absolute_links:\n",
    "            print(link)\n",
    "        # Recursively get links from child pages\n",
    "        child_links = []\n",
    "        for link in absolute_links:\n",
    "            try:\n",
    "                child_links.extend(get_all_links_recursive(link, depth=depth-1))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error retrieving content from {link}: {e}\")\n",
    "\n",
    "        return absolute_links + child_links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving content from {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def remove_pdf_duplicates(file_list):\n",
    "    removed_list = []\n",
    "    for file_name in file_list:\n",
    "        file_name = file_name.rsplit('.', 1)[0]\n",
    "        # Check if the file has both \".html\" and \".pdf\" versions\n",
    "        pdf_version = f\"{file_name}.pdf\"\n",
    "        html_version = f\"{file_name}.html\"\n",
    "\n",
    "        if pdf_version in file_list and html_version in file_list:\n",
    "            # Remove the \".pdf\" version\n",
    "            file_list.remove(pdf_version)\n",
    "            removed_list.append(pdf_version)\n",
    "    return file_list, removed_list\n",
    "\n",
    "\n",
    "def download_document(url, destination):\n",
    "    try:\n",
    "        # Extract the directory path from the destination\n",
    "        directory = os.path.dirname(destination)\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            with open(destination, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Document downloaded successfully to {destination}\")\n",
    "        else:\n",
    "            print(f\"Failed to download document. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def create_directory_path(url, base_path):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    # Extract the path component and decode URL-encoded characters\n",
    "    path_components = unquote(parsed_url.path).split('/')\n",
    "    # Combine the base path with the path components\n",
    "    directory_path = os.path.join(base_path, *path_components)\n",
    "    return directory_path\n",
    "\n",
    "\n",
    "def filter_urls(urls, list_ext):\n",
    "    filtered_urls = [url for url in urls if 'gitlab' not in url]\n",
    "    filtered_urls = [url for url in filtered_urls if url.startswith('http://qms-toulouse.kratos.us/')]\n",
    "    filtered_urls = [url for url in filtered_urls if any(ext in url for ext in list_ext)]\n",
    "    return filtered_urls\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing unwanted characters like ¶\n",
    "    text = text.replace('¶', '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_local_html(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.warning(f\"File not found: {file_path}\")\n",
    "        return ''\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def fetch_html_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def get_path(html_content, h='h1'):\n",
    "    html_content.split(f'</{h}>', 0)\n",
    "    soup = BeautifulSoup(html_content.split(f'</{h}>')[0], 'html.parser')\n",
    "    h_text = soup.find(f'{h}').text.replace(\"¶\", \"\").replace(\"Â\", \" \")\n",
    "    return '/' + h_text\n",
    "\n",
    "\n",
    "def scrape_content_from_page(html_content, source, file_path):\n",
    "    h1_split = html_content.split('<h1')\n",
    "    list_split = list()\n",
    "    list_path = list()\n",
    "    list_source = list()  # For storing the documentation source\n",
    "    list_file_path = list()\n",
    "    path = ''\n",
    "\n",
    "    for i, h1 in enumerate(h1_split):\n",
    "        if i > 0:\n",
    "            h1 = '<h1' + h1\n",
    "            result = path.split('/')[:1]\n",
    "            path = '/'.join(result)\n",
    "            path += get_path(h1, h='h1')\n",
    "\n",
    "        h2_split = h1.split('<h2')\n",
    "        for j, h2 in enumerate(h2_split):\n",
    "            if j > 0:\n",
    "                h2 = '<h2' + h2\n",
    "                result = path.split('/')[:2]\n",
    "                path = '/'.join(result)\n",
    "                path += get_path(h2, h='h2')\n",
    "            h3_split = h2.split('<h3')\n",
    "            for k, h3 in enumerate(h3_split):\n",
    "                if k > 0:\n",
    "                    h3 = '<h3' + h3\n",
    "                    result = path.split('/')[:3]\n",
    "                    path = '/'.join(result)\n",
    "                    path += get_path(h3, h='h3')\n",
    "\n",
    "                h4_split = h3.split('<h4')\n",
    "                for l, h4 in enumerate(h4_split):\n",
    "                    if l > 0:\n",
    "                        h4 = '<h4' + h4\n",
    "                        result = path.split('/')[:4]\n",
    "                        path = '/'.join(result)\n",
    "                        path += get_path(h4, h='h4')\n",
    "\n",
    "                    h5_split = h4.split('<h5')\n",
    "                    # Look for href in the html content to add it to the link\n",
    "                    regex_pattern = r'<h[1-6](.*?)<\\/h[1-6]>'\n",
    "                    header = re.search(regex_pattern, h5_split[0])\n",
    "                    if header:\n",
    "                        regex_pattern = r'href=\"#([^\"]*)\"'\n",
    "                        match_href = re.search(regex_pattern, header.group(1))\n",
    "                        regex_pattern = r'id=\"([^\"]*)\"'\n",
    "                        match_id = re.search(regex_pattern, header.group(1))\n",
    "                        if match_href:\n",
    "                            href = '#' + match_href.group(1)\n",
    "                        elif match_id:\n",
    "                            href = '#' + match_id.group(1)\n",
    "                        else:\n",
    "                            href = ''\n",
    "                    else:\n",
    "                        href = ''\n",
    "                    # get the text from html content\n",
    "                    soup = BeautifulSoup(h5_split[0].split('\\n', 1)[1], \"lxml\")\n",
    "                    h5_split = soup.get_text()\n",
    "                    list_split.append(h5_split)\n",
    "                    list_path.append(path)\n",
    "                    list_source.append(source)  # Add the documentation source for each content\n",
    "                    list_file_path.append(f'{file_path}{href}')\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe['documentation'] = list_source  # Add the documentation source column\n",
    "    dataframe['path'] = list_path\n",
    "    dataframe['text'] = list_split\n",
    "    dataframe['file_path'] = list_file_path  # adding local path to the df for each row\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def process_links_from_list(links, documentation_label):\n",
    "    logging.info(f\"Found {len(links)} links to process for {documentation_label}.\")\n",
    "    data_frames = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            page_content = fetch_html_from_url(link)\n",
    "            df_temp = scrape_content_from_page(page_content, documentation_label, link)\n",
    "\n",
    "            # Check if df_temp is not empty or None\n",
    "            if df_temp is not None and not df_temp.empty:\n",
    "                data_frames.append(df_temp)\n",
    "            else:\n",
    "                logging.warning(f\"Empty dataframe returned from {link}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {link}: {e}\")\n",
    "\n",
    "    logging.info(f\"Processed {len(data_frames)} dataframes for {documentation_label}.\")\n",
    "    return data_frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb169412856b33b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All links"
   ],
   "metadata": {},
   "id": "d3738d29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qms_path = \"http://qms-toulouse.kratos.us/\"\n",
    "qms_link = get_all_links_recursive(qms_path, depth=3)\n",
    "qms_link = list(set(qms_link))\n",
    "len(qms_link)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffb2c2d04f0eff2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filtered_files, removed_list = remove_pdf_duplicates(qms_link)\n",
    "for file in removed_list:\n",
    "    print(file)\n",
    "print(len(filtered_files))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23733c6593d9aa2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_links = list(set(filtered_files))\n",
    "print(len(all_links))\n",
    "\n",
    "# Lists to store categorized links\n",
    "html_links = []\n",
    "pdf_links = []\n",
    "xlsx_links = []\n",
    "xls_links = []\n",
    "docx_links = []\n",
    "doc_links = []\n",
    "pptx_links = []\n",
    "zip_links = []\n",
    "xlsm_links = []\n",
    "\n",
    "# Categorize links\n",
    "for link in all_links:\n",
    "    if link.endswith('.html'):\n",
    "        html_links.append(link)\n",
    "    elif link.endswith('.pdf'):\n",
    "        pdf_links.append(link)\n",
    "    elif link.endswith('.xlsx'):\n",
    "        xlsx_links.append(link)\n",
    "    elif link.endswith('.xlsm'):\n",
    "        xlsm_links.append(link)\n",
    "    elif link.endswith('.xls'):\n",
    "        xls_links.append(link)\n",
    "    elif link.endswith('.docx'):\n",
    "        docx_links.append(link)\n",
    "    elif link.endswith('.doc'):\n",
    "        doc_links.append(link)\n",
    "    elif link.endswith('.pptx'):\n",
    "        pptx_links.append(link)\n",
    "    elif link.endswith('.zip'):\n",
    "        zip_links.append(link)\n",
    "\n",
    "list_links = [html_links, pdf_links, xlsx_links, xlsm_links, xls_links, docx_links, doc_links, pptx_links, zip_links]\n",
    "for extension in list_links:\n",
    "    print(len(extension))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f087d4e30e67a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PDF and DOCX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b593ac6943b3ddc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "list_extension = ['.pdf']\n",
    "\n",
    "# Example usage\n",
    "pdf_doc_links = filter_urls(all_links, list_extension)\n",
    "print(len(pdf_doc_links))\n",
    "pdf_doc_links"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a7858263817133b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for url in pdf_doc_links:\n",
    "    current_path = os.getcwd()\n",
    "    destination_path = create_directory_path(url.split('http://qms-toulouse.kratos.us/')[-1], base_path= current_path + \"\\\\data\\\\qms data\")\n",
    "    download_document(url, destination_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2984d21c6dab0842",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"./data/qms data\", recursive=True)\n",
    "docs = reader.load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cba2be3173eb60e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "remove_list = []\n",
    "for doc in docs:\n",
    "    if len(doc.text) < 100:\n",
    "        remove_list.append(doc)\n",
    "    doc.metadata['url'] = 'http://qms-toulouse.kratos.us/' + doc.metadata['file_path'][5:].replace('\\\\', '/')\n",
    "    doc.excluded_llm_metadata_keys.remove('file_name')\n",
    "    doc.excluded_embed_metadata_keys.remove('file_name')\n",
    "    doc.metadata['manual'] = \"QMS Documents\"\n",
    "docs_pdf = [item for item in docs if item not in remove_list]\n",
    "docs_pdf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc29be105728b00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for file in remove_list:\n",
    "    print(file.text)\n",
    "print(len(remove_list))\n",
    "print(len(docs_pdf))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e69727bc37d6cb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HTML"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69ffaf147ddced4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "list_extension = ['.html']\n",
    "\n",
    "# Example usage\n",
    "html_links = filter_urls(all_links, list_extension)\n",
    "html_links =  [link for link in html_links if '#' not in link]\n",
    "print(len(html_links))\n",
    "html_links"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3b074498b7576d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = process_links_from_list(html_links, \"QMS Documents\")\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38ee6fdc27aacd01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Apply this function to the entire 'content' column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "# Remove rows where the text is empty\n",
    "df = df[df['text'].notnull() & (df['text'].str.len() > 0)]\n",
    "# Remove rows where the text is empty or the path is an empty string\n",
    "df = df[(df['path'].notnull()) & (df['path'] != \"\") & (df['text'].str.len() > 0)]\n",
    "# Remove rows where path equals \"/Table Of Contents\"\n",
    "df = df[df['path'] != '/Table Of Contents']\n",
    "# Remove duplicates based on the 'text' column\n",
    "df = df.drop_duplicates(subset='text', keep='first')\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dea042081a25625",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add a new column for word counts\n",
    "nltk.download('punkt')\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(tokens)\n",
    "    return num_tokens\n",
    "\n",
    "df['token_count'] = df['text'].apply(count_tokens)\n",
    "\n",
    "df = df.sort_values(by='token_count', ascending=False)\n",
    "lower_bound = 11  # or any other value based on your visualization or requirements\n",
    "df = df[df['token_count'] > lower_bound]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e60fc2b26e1c7f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "has_nan_or_none = df['path'].isnull().any()\n",
    "print(f\"Has NaN or None values in 'path': {has_nan_or_none}\")\n",
    "count_nan_or_none = df['path'].isnull().sum()\n",
    "print(f\"Number of NaN or None values in 'path': {count_nan_or_none}\")\n",
    "has_empty_string = (df['path'] == \"\").any()\n",
    "print(f\"Has empty strings in 'path': {has_empty_string}\")\n",
    "count_empty_string = (df['path'] == \"\").sum()\n",
    "print(f\"Number of empty strings in 'path': {count_empty_string}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a7278e566f9f3c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_metadata_dict(row):\n",
    "    metadata = dict()\n",
    "    if len(row['path'].split('/')) > 1:\n",
    "        metadata[\"title\"] = row['path'].split('/')[1]\n",
    "    if len(row['path'].split('/')) > 2:\n",
    "        metadata[\"sub_title\"] = row['path'].split('/')[2]\n",
    "    if len(row['path'].split('/')) > 3:\n",
    "        metadata[\"part\"] = row['path'].split('/')[3]\n",
    "    if len(row['path'].split('/')) > 4:\n",
    "        metadata[\"chapter\"] = row['path'].split('/')[4]\n",
    "    if len(row['path'].split('/')) > 5:\n",
    "        metadata[\"section\"] = row['path'].split('/')[5]\n",
    "    if len(row['path'].split('/')) > 6:\n",
    "        metadata[\"paragraph\"] = row['path'].split('/')[6]\n",
    "    metadata[\"url\"] = row['file_path']\n",
    "    metadata[\"manual\"] = row['documentation']\n",
    "    return metadata\n",
    "\n",
    "docs_html = list()\n",
    "for _, row in df.iterrows():\n",
    "    metadata = create_metadata_dict(row)\n",
    "    new_doc = Document(\n",
    "        text=row['text'],\n",
    "        metadata=metadata,\n",
    "        excluded_llm_metadata_keys=[\"url\"],\n",
    "        excluded_embed_metadata_keys = [\"url\"],\n",
    "        metadata_seperator=\"::\",\n",
    "        metadata_template=\"{key}=>{value}\",\n",
    "        text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
    "    )\n",
    "    docs_html.append(new_doc)\n",
    "len(docs_html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f3b382bcd6cfc1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('docs_pdf.pkl', 'wb') as f:\n",
    "    pickle.dump(docs_pdf, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da722b85cf66101"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "docs = docs_pdf + docs_html\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4975d445bfa4a568",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('docs.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b15b302ce8b1d4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('docs_pdf.pkl', 'wb') as f:\n",
    "    pickle.dump(docs_pdf, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8c048394b1569cd",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
