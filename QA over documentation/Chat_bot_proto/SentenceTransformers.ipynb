{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81b917e6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4e0a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#  Scraping\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "\n",
    "# Langchain\n",
    "from langchain.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d56b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f528f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts all the links from the given URL.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The URL from which links are to be extracted.\n",
    "    \n",
    "    Returns:\n",
    "    - List of unique URLs present on the webpage.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    links = set()\n",
    "    base_url = urlparse(url).scheme + \"://\" + urlparse(url).hostname\n",
    "\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        href = a_tag.attrs[\"href\"]\n",
    "        full_url = urljoin(base_url, href)\n",
    "        links.add(full_url)\n",
    "    \n",
    "    return list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657b039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_webpages(urls, use_async=False, rps=2, verify_ssl=True, proxies=None):\n",
    "    \"\"\"\n",
    "    Scrapes multiple webpages and returns them as documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - urls (list): A list of URLs to be scraped.\n",
    "    - use_async (bool): Whether to load the URLs asynchronously.\n",
    "    - rps (int): Requests per second, for asynchronous loading.\n",
    "    - verify_ssl (bool): Whether to verify SSL certificates during requests.\n",
    "    - proxies (dict): Dictionary containing http and https proxies.\n",
    "    \n",
    "    Returns:\n",
    "    - List of documents corresponding to the URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    loader = WebBaseLoader(urls)\n",
    "    loader.requests_kwargs = {'verify': verify_ssl}\n",
    "    \n",
    "    if proxies:\n",
    "        loader.requests_kwargs['proxies'] = proxies\n",
    "\n",
    "    if use_async:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        \n",
    "        loader.requests_per_second = rps\n",
    "        return loader.aload()\n",
    "    else:\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d6e0a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd91b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define origine\n",
    "website_url = \"https://www.sbert.net/docs/package_reference/SentenceTransformer.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43305292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap all url\n",
    "all_links = extract_links_from_url(website_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8201ce3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sbert.net/evaluation.html',\n",
       " 'https://www.sphinx-doc.org/',\n",
       " 'https://www.sbert.net/examples/unsupervised_learning/README.html',\n",
       " 'https://twitter.com/Nils_Reimers',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.save',\n",
       " 'https://www.sbert.net/examples/training/quora_duplicate_questions/README.html',\n",
       " 'https://www.sbert.net/examples/applications/image-search/README.html',\n",
       " 'https://www.sbert.net/examples/training/nli/README.html',\n",
       " 'https://github.com/readthedocs/sphinx_rtd_theme',\n",
       " 'https://www.sbert.net/pretrained_cross-encoders.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.tokenizer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.tokenize',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.get_max_seq_length',\n",
       " 'https://www.sbert.net/losses.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.evaluate',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.fit',\n",
       " 'https://www.sbert.net/examples/applications/paraphrase-mining/README.html',\n",
       " 'https://www.sbert.net/examples/training/multilingual/README.html',\n",
       " 'https://www.sbert.net/examples/applications/cross-encoder/README.html',\n",
       " 'https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html',\n",
       " 'https://www.sbert.net/models.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.max_seq_length',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.encode',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.encode_multi_process',\n",
       " 'https://www.sbert.net/examples/applications/retrieve_rerank/README.html',\n",
       " 'https://www.sbert.net/examples/applications/computing-embeddings/README.html',\n",
       " 'https://www.sbert.net',\n",
       " 'https://www.sbert.net/datasets.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.stop_multi_process_pool',\n",
       " 'https://www.sbert.net/pretrained_models.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.start_multi_process_pool',\n",
       " 'https://www.sbert.net/util.html',\n",
       " 'https://www.sbert.net/training/overview.html',\n",
       " 'https://www.sbert.net/examples/training/cross-encoder/README.html',\n",
       " 'https://www.sbert.net/docs/contact.html',\n",
       " 'https://www.sbert.net/examples/training/paraphrases/README.html',\n",
       " 'https://www.sbert.net/index.html',\n",
       " 'https://www.sbert.net/publications.html',\n",
       " 'https://www.sbert.net/examples/domain_adaptation/README.html',\n",
       " 'https://www.sbert.net/usage/semantic_textual_similarity.html',\n",
       " 'https://www.sbert.net/examples/applications/clustering/README.html',\n",
       " 'https://www.sbert.net/installation.html',\n",
       " 'https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/SentenceTransformer.md',\n",
       " 'https://www.sbert.net/quickstart.html',\n",
       " 'https://www.sbert.net/cross_encoder.html',\n",
       " 'https://www.sbert.net/examples/applications/semantic-search/README.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.save_to_hub',\n",
       " 'https://readthedocs.org',\n",
       " 'https://www.sbert.net/examples/training/ms_marco/README.html',\n",
       " 'https://www.sbert.net#sentencetransformer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.device',\n",
       " 'https://www.sbert.net/examples/training/data_augmentation/README.html',\n",
       " 'https://www.sbert.net/evaluation.html#sentence_transformers.evaluation.SentenceEvaluator',\n",
       " 'https://www.sbert.net/examples/training/sts/README.html',\n",
       " 'https://www.sbert.net/examples/training/distillation/README.html',\n",
       " 'https://www.sbert.net/hugging_face.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.smart_batching_collate']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cda9a3",
   "metadata": {},
   "source": [
    "## Filter web data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a8879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(all_links):\n",
    "    to_verify = []\n",
    "    filtered_links = []\n",
    "    \n",
    "    for link in all_links:\n",
    "        if link.startswith(\"https://www.sbert\"):\n",
    "            filtered_links.append(link)\n",
    "        else:\n",
    "            to_verify.append(link)\n",
    "    \n",
    "    return filtered_links, to_verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270b7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links, to_verify = filter_links(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31cd5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sbert.net/evaluation.html',\n",
       " 'https://www.sbert.net/examples/unsupervised_learning/README.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.save',\n",
       " 'https://www.sbert.net/examples/training/quora_duplicate_questions/README.html',\n",
       " 'https://www.sbert.net/examples/applications/image-search/README.html',\n",
       " 'https://www.sbert.net/examples/training/nli/README.html',\n",
       " 'https://www.sbert.net/pretrained_cross-encoders.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.tokenizer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.tokenize',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.get_max_seq_length',\n",
       " 'https://www.sbert.net/losses.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.evaluate',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.fit',\n",
       " 'https://www.sbert.net/examples/applications/paraphrase-mining/README.html',\n",
       " 'https://www.sbert.net/examples/training/multilingual/README.html',\n",
       " 'https://www.sbert.net/examples/applications/cross-encoder/README.html',\n",
       " 'https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html',\n",
       " 'https://www.sbert.net/models.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.max_seq_length',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.encode',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.encode_multi_process',\n",
       " 'https://www.sbert.net/examples/applications/retrieve_rerank/README.html',\n",
       " 'https://www.sbert.net/examples/applications/computing-embeddings/README.html',\n",
       " 'https://www.sbert.net',\n",
       " 'https://www.sbert.net/datasets.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.stop_multi_process_pool',\n",
       " 'https://www.sbert.net/pretrained_models.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.start_multi_process_pool',\n",
       " 'https://www.sbert.net/util.html',\n",
       " 'https://www.sbert.net/training/overview.html',\n",
       " 'https://www.sbert.net/examples/training/cross-encoder/README.html',\n",
       " 'https://www.sbert.net/docs/contact.html',\n",
       " 'https://www.sbert.net/examples/training/paraphrases/README.html',\n",
       " 'https://www.sbert.net/index.html',\n",
       " 'https://www.sbert.net/publications.html',\n",
       " 'https://www.sbert.net/examples/domain_adaptation/README.html',\n",
       " 'https://www.sbert.net/usage/semantic_textual_similarity.html',\n",
       " 'https://www.sbert.net/examples/applications/clustering/README.html',\n",
       " 'https://www.sbert.net/installation.html',\n",
       " 'https://www.sbert.net/quickstart.html',\n",
       " 'https://www.sbert.net/cross_encoder.html',\n",
       " 'https://www.sbert.net/examples/applications/semantic-search/README.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.save_to_hub',\n",
       " 'https://www.sbert.net/examples/training/ms_marco/README.html',\n",
       " 'https://www.sbert.net#sentencetransformer',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.device',\n",
       " 'https://www.sbert.net/examples/training/data_augmentation/README.html',\n",
       " 'https://www.sbert.net/evaluation.html#sentence_transformers.evaluation.SentenceEvaluator',\n",
       " 'https://www.sbert.net/examples/training/sts/README.html',\n",
       " 'https://www.sbert.net/examples/training/distillation/README.html',\n",
       " 'https://www.sbert.net/hugging_face.html',\n",
       " 'https://www.sbert.net#sentence_transformers.SentenceTransformer.smart_batching_collate']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74676ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sphinx-doc.org/',\n",
       " 'https://twitter.com/Nils_Reimers',\n",
       " 'https://github.com/readthedocs/sphinx_rtd_theme',\n",
       " 'https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/SentenceTransformer.md',\n",
       " 'https://readthedocs.org']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53b820",
   "metadata": {},
   "source": [
    "## Etract text  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da921a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can use previously defined function to scrape these links:\n",
    "documents = scrape_multiple_webpages(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b684fc63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a41426af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/evaluation.html', 'title': '404 Not Found', 'language': 'No language found.'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ffd141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n\\n\\n\\nUnsupervised Learning — Sentence-Transformers  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Sentence-Transformers\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\nInstallation\\nQuickstart\\nPretrained Models\\nPretrained Cross-Encoders\\nPublications\\nHugging Face ðŸ¤—\\n\\nUsage\\n\\nComputing Sentence Embeddings\\nSemantic Textual Similarity\\nSemantic Search\\nRetrieve & Re-Rank\\nClustering\\nParaphrase Mining\\nTranslated Sentence Mining\\nCross-Encoders\\nImage Search\\n\\nTraining\\n\\nTraining Overview\\nMultilingual-Models\\nModel Distillation\\nCross-Encoders\\nAugmented SBERT\\n\\nTraining Examples\\n\\nSemantic Textual Similarity\\nNatural Language Inference\\nParaphrase Data\\nQuora Duplicate Questions\\nMS MARCO\\n\\nUnsupervised Learning\\n\\nUnsupervised Learning\\nTSDAE\\nSimCSE\\nCT\\nCT (In-Batch Negative Sampling)\\nMasked Language Model (MLM)\\nGenQ\\nGPL\\nPerformance Comparison\\n\\n\\nDomain Adaptation\\n\\nPackage Reference\\n\\nSentenceTransformer\\nutil\\nModels\\nLosses\\nEvaluation\\nDatasets\\ncross_encoder\\n\\n\\n\\n\\n\\n\\n\\nSentence-Transformers\\n\\n\\n\\n\\n\\n »\\nUnsupervised Learning\\n\\n Edit on GitHub\\n\\n\\n\\n\\n\\n\\n\\nUnsupervised LearningÂ¶\\nThis page contains a collection of unsupervised learning methods to learn sentence embeddings. The methods have in common that they do not require labeled training data. Instead, they can learn semantically meaningful sentence embeddings just from the text itself.\\nNote: Unsupervised learning approaches are still an activate research area and in many cases the models perform rather poorly compared to models that are using training pairs as provided in our training data collection. A better approach is Domain Adaptation where you combine unsupervised learning on your target domain with existent labeled data. This gives the best performance on your specific corpus.\\n\\nTSDAEÂ¶\\nIn our work TSDAE (Tranformer-based Denoising AutoEncoder) we present an unsupervised sentence embedding learning method based on denoising auto-encoders:\\n\\nWe add noise to the input text, in our case, we delete about 60% of the words in the text. The encoder maps this input to a fixed-sized sentence embeddings. A decoder then tries to re-create the original text without the noise. Later, we use the encoder as the sentence embedding methods.\\nSee TSDAE for more information and training examples.\\n\\n\\nSimCSEÂ¶\\nGao et al. present in SimCSE: Simple Contrastive Learning of Sentence Embeddings a method that passes the same sentence twice to the sentence embedding encoder. Due to the drop-out, it will be encoded at slightly different positions in vector space.\\nThe distance between these two embeddings will be minized, while the distance to other embeddings of the other sentences in the same batch will be maximized.\\n\\nSee SimCSE for more information and training examples.\\n\\n\\nCTÂ¶\\nCarlsson et al. present in Semantic Re-Tuning With Contrastive Tension (CT) an unsupervised method that uses two models: If the same sentences are passed to Model1 and Model2, then the respective sentence embeddings should get a large dot-score. If the different sentences are passed, then the sentence embeddings should get a low score.\\n\\nSee CT for more information and training examples.\\n\\n\\nCT (In-Batch Negative Sampling)Â¶\\nThe CT method from Carlsson et al. provides sentence pairs to the two models. This can be improved by using in-batch negative sampling: Model1 and Model2 both encode the same set of sentences. We maximize the scores for matching indexes (i.e. Model1(S_i) and Model2(S_i)) while we minimize the scores for different indexes (i.e. Model1(S_i) and Model2(S_j) for i != j).\\nSee CT_In-Batch_Negatives for more information and training examples.\\n\\n\\nMasked Language Model (MLM)Â¶\\nBERT showed that Masked Language Model (MLM) is a powerful pre-training approach. It is advisable to first run MLM a large dataset from your domain before you do fine-tuning. See MLM for more information and training examples.\\n\\n\\nGenQÂ¶\\nIn our paper BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models  we present a method to learn a semantic search method by generating queries for given passages. This method has been improved in GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval.\\nWe pass all passages in our collection through a trained T5 model, which generates potential queries from users. We then use these (query, passage) pairs to train a SentenceTransformer model.\\n\\nSee GenQ for more information and training examples. See GPL for the improved version that uses a multi-step training approach.\\n\\n\\nGPLÂ¶\\nIn GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval we show an improved version go GenQ, which combines the generation with negative mining and pseudo labeling cosing a Cross-Encoder. It leads to significantly improved results. See Domain Adaptation for more information.\\n\\n\\n\\nPerformance ComparisonÂ¶\\nIn our paper\\r\\nTSDAE we compare approaches for sentence embedding tasks, and in GPL we compare them for semantic search tasks (given a query, find relevant passages). While the unsupervised approach achieve acceptable performances for sentence embedding tasks, they perform poorly for semantic search tasks.\\n\\n\\n\\n\\n\\n\\nNext \\n Previous\\n\\n\\n\\n\\r\\n        \\r\\n        © Copyright 2022, Nils Reimers\\r\\n\\r\\n       • Contact\\n\\n\\r\\n    \\r\\n    \\r\\n    \\r\\n    Built with Sphinx using a\\r\\n    \\r\\n    theme\\r\\n    \\r\\n    provided by Read the Docs. \\r\\n\\r\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.sbert.net/examples/unsupervised_learning/README.html', 'title': 'Unsupervised Learning — Sentence-Transformers  documentation', 'language': 'en'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1361b",
   "metadata": {},
   "source": [
    "## Clean Text data\n",
    "- Remove 404 error content\n",
    "- Clean text\n",
    "   - Removing HTML tags and content\n",
    "   - Removing Markdown-specific syntax\n",
    "   - Converting Unicode characters to their actual representation\n",
    "   - Removing URLs\n",
    "   - Removing extra white spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2d9d1",
   "metadata": {},
   "source": [
    "### Filter 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5ce3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_documents(documents):\n",
    "    manual_check = []\n",
    "    filtered_documents = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc.metadata['title'] == '404 Not Found':\n",
    "            manual_check.append(doc)\n",
    "        else:\n",
    "            filtered_documents.append(doc)\n",
    "    \n",
    "    return filtered_documents, manual_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592a131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, manual_check = filter_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd42dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unsupervised Learning — Sentence-Transformers  documentation'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7337c67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\nUnsupervised Learning — Sentence-Transformers  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Sentence-Transformers\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\nInstallation\\nQuickstart\\nPretrained Models\\nPretrained Cross-Encoders\\nPublications\\nHugging Face ðŸ¤—\\n\\nUsage\\n\\nComputing Sentence Embeddings\\nSemantic Textual Similarity\\nSemantic Search\\nRetrieve & Re-Rank\\nClustering\\nParaphrase Mining\\nTranslated Sentence Mining\\nCross-Encoders\\nImage Search\\n\\nTraining\\n\\nTraining Overview\\nMultilingual-Models\\nModel Distillation\\nCross-Encoders\\nAugmented SBERT\\n\\nTraining Examples\\n\\nSemantic Textual Similarity\\nNatural Language Inference\\nParaphrase Data\\nQuora Duplicate Questions\\nMS MARCO\\n\\nUnsupervised Learning\\n\\nUnsupervised Learning\\nTSDAE\\nSimCSE\\nCT\\nCT (In-Batch Negative Sampling)\\nMasked Language Model (MLM)\\nGenQ\\nGPL\\nPerformance Comparison\\n\\n\\nDomain Adaptation\\n\\nPackage Reference\\n\\nSentenceTransformer\\nutil\\nModels\\nLosses\\nEvaluation\\nDatasets\\ncross_encoder\\n\\n\\n\\n\\n\\n\\n\\nSentence-Transformers\\n\\n\\n\\n\\n\\n »\\nUnsupervised Learning\\n\\n Edit on GitHub\\n\\n\\n\\n\\n\\n\\n\\nUnsupervised LearningÂ¶\\nThis page contains a collection of unsupervised learning methods to learn sentence embeddings. The methods have in common that they do not require labeled training data. Instead, they can learn semantically meaningful sentence embeddings just from the text itself.\\nNote: Unsupervised learning approaches are still an activate research area and in many cases the models perform rather poorly compared to models that are using training pairs as provided in our training data collection. A better approach is Domain Adaptation where you combine unsupervised learning on your target domain with existent labeled data. This gives the best performance on your specific corpus.\\n\\nTSDAEÂ¶\\nIn our work TSDAE (Tranformer-based Denoising AutoEncoder) we present an unsupervised sentence embedding learning method based on denoising auto-encoders:\\n\\nWe add noise to the input text, in our case, we delete about 60% of the words in the text. The encoder maps this input to a fixed-sized sentence embeddings. A decoder then tries to re-create the original text without the noise. Later, we use the encoder as the sentence embedding methods.\\nSee TSDAE for more information and training examples.\\n\\n\\nSimCSEÂ¶\\nGao et al. present in SimCSE: Simple Contrastive Learning of Sentence Embeddings a method that passes the same sentence twice to the sentence embedding encoder. Due to the drop-out, it will be encoded at slightly different positions in vector space.\\nThe distance between these two embeddings will be minized, while the distance to other embeddings of the other sentences in the same batch will be maximized.\\n\\nSee SimCSE for more information and training examples.\\n\\n\\nCTÂ¶\\nCarlsson et al. present in Semantic Re-Tuning With Contrastive Tension (CT) an unsupervised method that uses two models: If the same sentences are passed to Model1 and Model2, then the respective sentence embeddings should get a large dot-score. If the different sentences are passed, then the sentence embeddings should get a low score.\\n\\nSee CT for more information and training examples.\\n\\n\\nCT (In-Batch Negative Sampling)Â¶\\nThe CT method from Carlsson et al. provides sentence pairs to the two models. This can be improved by using in-batch negative sampling: Model1 and Model2 both encode the same set of sentences. We maximize the scores for matching indexes (i.e. Model1(S_i) and Model2(S_i)) while we minimize the scores for different indexes (i.e. Model1(S_i) and Model2(S_j) for i != j).\\nSee CT_In-Batch_Negatives for more information and training examples.\\n\\n\\nMasked Language Model (MLM)Â¶\\nBERT showed that Masked Language Model (MLM) is a powerful pre-training approach. It is advisable to first run MLM a large dataset from your domain before you do fine-tuning. See MLM for more information and training examples.\\n\\n\\nGenQÂ¶\\nIn our paper BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models  we present a method to learn a semantic search method by generating queries for given passages. This method has been improved in GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval.\\nWe pass all passages in our collection through a trained T5 model, which generates potential queries from users. We then use these (query, passage) pairs to train a SentenceTransformer model.\\n\\nSee GenQ for more information and training examples. See GPL for the improved version that uses a multi-step training approach.\\n\\n\\nGPLÂ¶\\nIn GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval we show an improved version go GenQ, which combines the generation with negative mining and pseudo labeling cosing a Cross-Encoder. It leads to significantly improved results. See Domain Adaptation for more information.\\n\\n\\n\\nPerformance ComparisonÂ¶\\nIn our paper\\r\\nTSDAE we compare approaches for sentence embedding tasks, and in GPL we compare them for semantic search tasks (given a query, find relevant passages). While the unsupervised approach achieve acceptable performances for sentence embedding tasks, they perform poorly for semantic search tasks.\\n\\n\\n\\n\\n\\n\\nNext \\n Previous\\n\\n\\n\\n\\r\\n        \\r\\n        © Copyright 2022, Nils Reimers\\r\\n\\r\\n       • Contact\\n\\n\\r\\n    \\r\\n    \\r\\n    \\r\\n    Built with Sphinx using a\\r\\n    \\r\\n    theme\\r\\n    \\r\\n    provided by Read the Docs. \\r\\n\\r\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89eadbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/evaluation.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/pretrained_cross-encoders.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/losses.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/models.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/datasets.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/pretrained_models.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/util.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/training/overview.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/publications.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/usage/semantic_textual_similarity.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/installation.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/quickstart.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/cross_encoder.html', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/evaluation.html#sentence_transformers.evaluation.SentenceEvaluator', 'title': '404 Not Found', 'language': 'No language found.'}),\n",
       " Document(page_content='\\n\\n404 Not Found\\n\\nNot Found\\nThe requested URL was not found on this server.\\n\\nApache/2.4.29 (Ubuntu) Server at www.sbert.net Port 443\\n\\n', metadata={'source': 'https://www.sbert.net/hugging_face.html', 'title': '404 Not Found', 'language': 'No language found.'})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744d8a9",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35dbfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the provided text by:\n",
    "    - Removing HTML tags and content\n",
    "    - Removing Markdown-specific syntax\n",
    "    - Converting Unicode characters to their actual representation\n",
    "    - Removing URLs\n",
    "    - Removing extra white spaces\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input string to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The cleaned string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    no_html = soup.get_text(separator=' ')\n",
    "    \n",
    "    # 2. Remove Markdown Syntax\n",
    "    no_markdown = re.sub(r'\\!\\[.*?\\]\\(.*?\\)|\\[(.*?)\\]\\(.*?\\)|\\*\\*.*?\\*\\*|\\*.*?\\*|#[^\\n]*', '', no_html)\n",
    "    \n",
    "    # 3. Convert Unicode characters (for common entities; can be expanded further)\n",
    "    no_unicode = re.sub(r'&amp;', '&', no_markdown)\n",
    "    no_unicode = re.sub(r'&lt;', '<', no_unicode)\n",
    "    no_unicode = re.sub(r'&gt;', '>', no_unicode)\n",
    "    \n",
    "    # 4. Remove URLs\n",
    "    no_urls = re.sub(r'http[s]?://\\S+', '', no_unicode)\n",
    "    \n",
    "    # 5. Remove extra white spaces\n",
    "    clean_string = ' '.join(no_urls.split())\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1def40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_documents(documents):\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3d9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = clean_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87b6182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unsupervised Learning — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Quora Duplicate Questions — Sentence-Transformers  documentation', 'Image Search — Sentence-Transformers  documentation', 'Natural Language Inference — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Paraphrase Mining — Sentence-Transformers  documentation', 'Multilingual-Models — Sentence-Transformers  documentation', 'Cross-Encoders — Sentence-Transformers  documentation', 'Translated Sentence Mining — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Retrieve & Re-Rank — Sentence-Transformers  documentation', 'Computing Sentence Embeddings — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Cross-Encoders — Sentence-Transformers  documentation', 'Contact — Sentence-Transformers  documentation', 'Paraphrase Data — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Domain Adaptation — Sentence-Transformers  documentation', 'Clustering — Sentence-Transformers  documentation', 'Semantic Search — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'MS MARCO — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation', 'Augmented SBERT — Sentence-Transformers  documentation', 'Semantic Textual Similarity — Sentence-Transformers  documentation', 'Model Distillation — Sentence-Transformers  documentation', 'SentenceTransformers Documentation — Sentence-Transformers  documentation']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [doc.metadata['title'] for doc in cleaned_documents]\n",
    "print(titles)\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4676dd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unsupervised Learning — Sentence-Transformers documentation Sentence-Transformers Overview Installation Quickstart Pretrained Models Pretrained Cross-Encoders Publications Hugging Face ðŸ¤— Usage Computing Sentence Embeddings Semantic Textual Similarity Semantic Search Retrieve & Re-Rank Clustering Paraphrase Mining Translated Sentence Mining Cross-Encoders Image Search Training Training Overview Multilingual-Models Model Distillation Cross-Encoders Augmented SBERT Training Examples Semantic Textual Similarity Natural Language Inference Paraphrase Data Quora Duplicate Questions MS MARCO Unsupervised Learning Unsupervised Learning TSDAE SimCSE CT CT (In-Batch Negative Sampling) Masked Language Model (MLM) GenQ GPL Performance Comparison Domain Adaptation Package Reference SentenceTransformer util Models Losses Evaluation Datasets cross_encoder Sentence-Transformers » Unsupervised Learning Edit on GitHub Unsupervised LearningÂ¶ This page contains a collection of unsupervised learning methods to learn sentence embeddings. The methods have in common that they do not require labeled training data. Instead, they can learn semantically meaningful sentence embeddings just from the text itself. Note: Unsupervised learning approaches are still an activate research area and in many cases the models perform rather poorly compared to models that are using training pairs as provided in our training data collection. A better approach is Domain Adaptation where you combine unsupervised learning on your target domain with existent labeled data. This gives the best performance on your specific corpus. TSDAEÂ¶ In our work TSDAE (Tranformer-based Denoising AutoEncoder) we present an unsupervised sentence embedding learning method based on denoising auto-encoders: We add noise to the input text, in our case, we delete about 60% of the words in the text. The encoder maps this input to a fixed-sized sentence embeddings. A decoder then tries to re-create the original text without the noise. Later, we use the encoder as the sentence embedding methods. See TSDAE for more information and training examples. SimCSEÂ¶ Gao et al. present in SimCSE: Simple Contrastive Learning of Sentence Embeddings a method that passes the same sentence twice to the sentence embedding encoder. Due to the drop-out, it will be encoded at slightly different positions in vector space. The distance between these two embeddings will be minized, while the distance to other embeddings of the other sentences in the same batch will be maximized. See SimCSE for more information and training examples. CTÂ¶ Carlsson et al. present in Semantic Re-Tuning With Contrastive Tension (CT) an unsupervised method that uses two models: If the same sentences are passed to Model1 and Model2, then the respective sentence embeddings should get a large dot-score. If the different sentences are passed, then the sentence embeddings should get a low score. See CT for more information and training examples. CT (In-Batch Negative Sampling)Â¶ The CT method from Carlsson et al. provides sentence pairs to the two models. This can be improved by using in-batch negative sampling: Model1 and Model2 both encode the same set of sentences. We maximize the scores for matching indexes (i.e. Model1(S_i) and Model2(S_i)) while we minimize the scores for different indexes (i.e. Model1(S_i) and Model2(S_j) for i != j). See CT_In-Batch_Negatives for more information and training examples. Masked Language Model (MLM)Â¶ BERT showed that Masked Language Model (MLM) is a powerful pre-training approach. It is advisable to first run MLM a large dataset from your domain before you do fine-tuning. See MLM for more information and training examples. GenQÂ¶ In our paper BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models we present a method to learn a semantic search method by generating queries for given passages. This method has been improved in GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. We pass all passages in our collection through a trained T5 model, which generates potential queries from users. We then use these (query, passage) pairs to train a SentenceTransformer model. See GenQ for more information and training examples. See GPL for the improved version that uses a multi-step training approach. GPLÂ¶ In GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval we show an improved version go GenQ, which combines the generation with negative mining and pseudo labeling cosing a Cross-Encoder. It leads to significantly improved results. See Domain Adaptation for more information. Performance ComparisonÂ¶ In our paper TSDAE we compare approaches for sentence embedding tasks, and in GPL we compare them for semantic search tasks (given a query, find relevant passages). While the unsupervised approach achieve acceptable performances for sentence embedding tasks, they perform poorly for semantic search tasks. Next Previous © Copyright 2022, Nils Reimers • Contact Built with Sphinx using a theme provided by Read the Docs.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dbbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
