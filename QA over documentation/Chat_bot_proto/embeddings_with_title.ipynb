{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e720fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan_2\\anaconda3\\envs\\Dolly\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# vector store set up \n",
    "\n",
    "import chromadb\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#Langchain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0081b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars_before_first_letter(input_string):\n",
    "    for i, char in enumerate(input_string):\n",
    "        if char.isalpha():\n",
    "            return input_string[i:]\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cee6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source(source):\n",
    "    path = str()\n",
    "    parts = source.split('/')\n",
    "    title = remove_chars_before_first_letter(parts[1]) if len(parts) > 1 else \"\"  # First part before the first '/'\n",
    "    \n",
    "    # Assuming that the 'source' always contains at least one '/'\n",
    "    chapter = remove_chars_before_first_letter(parts[2]) if len(parts) > 2 else \"\"  # Text after the first '/' until the next '/'\n",
    "    \n",
    "    # If there is another '/', everything after it is considered the 'Paragraph'\n",
    "    paragraph = remove_chars_before_first_letter(parts[3]) if len(parts) > 3 else \"\"  # Text after the second '/'\n",
    "    \n",
    "    return title, chapter, paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35a98c",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365c8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = client = chromadb.PersistentClient(path='C:/Users/Nathan_2/DL2_Kratos_data-Science/Chroma/v6')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2',  device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b2d7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=EPOCH),\n",
       " Collection(name=Skyminer-T),\n",
       " Collection(name=QMS),\n",
       " Collection(name=EPOCH-T),\n",
       " Collection(name=Skyminer),\n",
       " Collection(name=QMS-T)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cc2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name=\"Skyminer-T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35fa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma\n",
    "vectorstore = chroma_client.get_or_create_collection(name=\"Skyminer\",  metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35141f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the new information for documents\n",
    "new_documents_list = []\n",
    "new_embeddings_list = []\n",
    "new_metadatas_list = []\n",
    "new_ids_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15776fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_documents = vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fee8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(old_documents),len(old_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6fe38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids = 473\n",
      "embeddings = None (no elements to count)\n",
      "metadatas = 473\n",
      "documents = 473\n"
     ]
    }
   ],
   "source": [
    "for key, value in old_documents.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key} = {len(value)}\")\n",
    "    else:\n",
    "        print(f\"{key} = None (no elements to count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ff661",
   "metadata": {},
   "source": [
    "# Create new Vector Store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f40fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the lists of metadata and documents\n",
    "old_metadatas = old_documents['metadatas']\n",
    "old_docs_content = old_documents['documents']\n",
    "\n",
    "# Verifying that both lists are of the same length\n",
    "if len(old_metadatas) != len(old_docs_content):\n",
    "    raise ValueError(\"Metadata and document content lists are not the same length!\")\n",
    "\n",
    "# Lists for new data\n",
    "new_documents_list = []\n",
    "new_embeddings_list = []\n",
    "new_metadatas_list = []\n",
    "new_ids_list = []\n",
    "\n",
    "# Now, we loop through each metadata and document pair and process them\n",
    "for metadata, content in zip(old_metadatas, old_docs_content):\n",
    "    # Extracting metadata components\n",
    "    documentation = metadata['documentation']\n",
    "    source = metadata['source']\n",
    "    file_path = metadata['file_path']\n",
    "    word_count = metadata['word_count']\n",
    "\n",
    "    # Parse the 'source' to extract 'Title', 'Chapter', and 'Paragraph'\n",
    "    title, chapter, paragraph = parse_source(source)\n",
    "\n",
    "    # Construct the new context string\n",
    "    context_elements = []\n",
    "    \n",
    "    # Adding elements to context only if they exist\n",
    "    if documentation:\n",
    "        context_elements.append(f\"Documentation = {documentation}\")\n",
    "    if title:\n",
    "        context_elements.append(f\"Title = {title}\")\n",
    "    if chapter:  # Only add if chapter is not empty\n",
    "        context_elements.append(f\"Chapter = {chapter}\")\n",
    "    if paragraph:  # Only add if paragraph is not empty\n",
    "        context_elements.append(f\"Paragraph = {paragraph}\")\n",
    "\n",
    "    # Constructing the context string based on the existing elements\n",
    "    context_info = \"Context : (\" + \", \".join(context_elements) + \") \" if context_elements else \"\"\n",
    "\n",
    "    # Combine new context with the old content\n",
    "    full_content_with_context = context_info + content\n",
    "\n",
    "    # Create a new embedding\n",
    "    new_embedding = model.encode(full_content_with_context)\n",
    "\n",
    "    # Reconstruct the metadata (if there are any changes or additions, make them here)\n",
    "    new_metadata = {\n",
    "        \"source\": source,\n",
    "        \"documentation\": documentation,\n",
    "        \"file_path\": file_path,\n",
    "        \"word_count\": word_count\n",
    "    }\n",
    "\n",
    "    # Add the modified document, new embedding, and metadata to the lists\n",
    "    new_documents_list.append(full_content_with_context)\n",
    "    new_embeddings_list.append(new_embedding.tolist())\n",
    "    new_metadatas_list.append(new_metadata)\n",
    "\n",
    "# Generate new IDs for the documents (assuming you want unique new IDs)\n",
    "new_ids_list = [\"v\" + str(i + 1) for i in range(len(new_documents_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6347af1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Context : (Documentation = Administrastion Manual, Title = Technology for the Satellite Ground Ecosystem, Chapter = Cutting Edge Technology) … including satellite ground apps that can be reconfigured as mission shifts and orchestrated services that keep the ground in sync with new software-defined satellites and soon , connect with 5G .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Technology for the Satellite Ground Ecosystem, Chapter = World Class NOC) … supported by Kratos’ global RF sensor network . Collectively , they deliver critical Space Domain Awareness ( SDA ) services for military , government and commercial missions . Learn More',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Technology for the Satellite Ground Ecosystem, Chapter = Fueling Progress) … for an increasingly competitive and contested space environment , Kratos delivers today on advances in satellite technology , ensuring you will be ready for what’s next . Learn More',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Contact Us) 5971 Kingstowne Village Pkwy , Suite 200 Alexandria , VA 22315 Phone : ( 703 ) 254-2000 Fax : ( 703 ) 254-2010 Map and Directions',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Contact Us, Chapter = Systems & Platforms) Unmanned Systems Tactical UAVs Small UAS Swarming Aethon Aerial Targets High Mobility Ground Vehicle Target M-PAK Leader/Follower System Autonomous Truck Mounted Attenuator Space Systems Training Systems C5ISR Systems Sub-Orbital Vehicles And Targets',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Contact Us, Chapter = Systems & Platforms, Paragraph = Products) Press Releases Events & Presentations Financial Information Stock Information Investor FAQs LinkedInTwitterYouTubeConstellations Podcast NASDAQ : KTOS $17 . 25 0 . 08 10 : 00 AM EDT ( Data Provided by Refinitiv . Minimum 15 minutes delayed .) © Copyright 2006-2023 Kratos Defense & Security Solutions , Inc . Privacy Policy CCPA Terms of Use',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Administration Guide) This documentation contains the Skyminer administration and maintenance manual and API documentation . Notices regarding this document Skyminer Introduction System Requirements Components Versions Skyminer Skyminer Lite Installation Skyminer Architecture Introduction Docker Prerequisites System Using Docker compose v2+ Firewall configuration Disable Linux Swap NTP Configuration Security settings Docker network Installation files Data & filesystems Disks Filesystems Cassandra OpenSearch Moving data to appropriate filesystem Mapped directories ( volumes ) List of mapped volumes : Installing & starting Skyminer Installing on a standalone server Installing on a 2 nodes cluster Installing on a 3+ nodes cluster Post-installation steps Upgrade protocol to 2 . 7 Prepare 2 . 7 . x installation Shutdown all Grafana instances Backup databases Shutdown Skyminer older version Install Skyminer new version Migrate data Migrate PostgreSQL Restore OpenSearch Snapshot Check Skyminer configuration Restart Skyminer RollBack Procedure Managing Skyminer license Overview Activating Skyminer Configuration Sharing Resources between different instances Use a shared filesystem for Skyminer data Configure a NFS server Grafana Login Unsigned plugins Skyminer datasource Detailed configuration Skyminer Health Monitor Health Monitor operation Health Monitor module configuration Enabling the Health Monitor module after an upgrade Disabling the Health Monitor module High availability / Load Balancer configuration Prerequisite Architecture Configuration Create high availability service Configure front-end service Share jupyter notebooks KairosDB Skyminer Parameters Enabling authentication Host Features Selection Web services filter Administration Checking the status of Skyminer Starting & Stopping Skyminer Starting & Stopping Skyminer alone Starting & Stopping Skyminer and all associated services Starting & Stopping Skyminer Demo Data Generator Starting & Stopping Cassandra on a node Backing up & Restoring Backing up Skyminer configuration Backing up Skyminer time series data Backing up Grafana data Taking and restoring OpenSearch snapshot Deleting Data from Skyminer Using the API Using the delete_metrics script Using delete_metric_data python script Delete all data from the datastore Troubleshooting Service is down Check the log files Fix corrupted Cassandra data How to use hostnames whithout DNS on docker-compose Procedure for each host Extra hosts definitions format Time series datastores Cassandra datastore System Requirements for Cassandra Architecture Cluster Configuration General Guidelines Consistency checking How to improve performances How to add a new node to a cluster How to remove a node from a cluster How to erase all data Backup & Restore of Cassandra data Indexed Cassandra datastore Requirements Activating the datastore Deactivating the datastore Prometheus read-only datastore ( Experimental ) Architecture Configuration RocksDB local storage datastore ( Experimental ) Architecture Configuration Throwaway ( no storage ) datastore Architecture Configuration OpenSearch database OpenSearch cluster configuration Query processor Data Retrieval Data Grouping Horizontal Aggregation Vertical Aggregation Prediction Outlier detection API Documentation REST API Overview OpenAPI Add Data Points Aggregators Correlations API Delete Data Points Delete Metric Event sourcing Features Grouping Health Checks List Metric Names Metadata Query CSV Query Metric Tags Query Metrics Skyminer Time Series Indexer Module API Skyminer Version Version Telnet API Overview Put Putm Special Tags Version Python API ( for sending data ) Skyminer Time Series Python Connector Optional Modules Skyminer Time Series Indexer Module Time series indexer requirements Time series indexer configuration Time series indexer mechanisms Time series indexer metrics Enabling the time series indexer after an upgrade Disabling the time series indexer Skyminer time series indexer module API Skyminer document Module Skyminer document module requirements Skyminer document module configuration Skyminer document module API Other Modules Baseline modules Additional modules FAQ How can I start/stop Skyminer ? How can I check the status of Skyminer ? Can I upgrade Grafana or Jupyter myself ? Can I add my own Grafana or Jupyter plugins ? Why is there a difference between the values of the timeseries on Skyminer and the ones on Monics ? What should I send to the Kratos support team if there is an issue with my Skyminer system ? Can I push custom data to Skyminer ? Dependencies and licenses Integrated third-party software KairosDB Information Documentation Cassandra Information Documentation Cassandra Range Repair Script Grafana Information Documentation Jupyter Information Documentation Nginx Information Documentation PostgreSQL Information Documentation Collectd Information Documentation Opensearch Information Documentation Python packages User Interface Angular 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Notices regarding this document) The information contained in this document is considered reliable and subject to change without notice . Illustrations and screenshots may not represent the latest software version but are included to best explain related text . The information in this document does not represent any commitment or warranty on the part of KratosÂ®. No part of this document or any part of the software described herein may be reproduced by any means without the express written consent of Kratos . For the purposes of these disclaimers , â\\x80\\x9cKratosâ\\x80\\x9d refers to Kratos Communications , Inc . and its wholly owned subsidiaries . Software License Agreement : Software is defined as the computer programs with which the Software License Agreement is included and any maintenance or update releases thereto . This Agreement sets forth the terms and conditions for licensing of the software , and installing the software indicates that the Agreement has been read and understood , and users accept its terms and conditions . MonicsÂ®, EpochÂ®, CompassÂ® and satIDÂ® are registered trademarks of Kratos Communications , Inc . All other trademarks are property of their respective owners . Copyright Â© Kratos Communications , Inc . All Rights Reserved . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Introduction) Skyminer system is a Big Data storage and analytics engine integrated with our corporate products , systems and solutions . It is capable of storing billions of samples with different data types , while maintaining efficient storage and outstanding write and read performances . Skyminer provides features to analyze data over time , organisational , or geospatial dimensions within and/or between data series .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Introduction, Chapter = System Requirements) The requirements to operate the Skyminer system are : A Linux OS with recent kernel ( compatible with docker containers ) Docker and docker-compose ( version 2 . 2 ) utilities installed on the OS If using Cassandra , see System Requirements for Cassandra To install Skyminer on another operating system , contact Kratos for requirements and specific documentation .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Introduction, Chapter = Components) Skyminer uses a data store ( usually Apache Cassandra ) to store all time series data , cf . Time series datastores Skyminer is composed of : A server that offers a REST web service An intuitive web client ( web UI ) for querying and visualizing the data . This client generates queries that match the servers API and displays the results . The server : Interprets the queries it receives on its API endpoints Gathers the corresponding data from Cassandra Processes the data Returns the result of the queries The client : Provides a user friendly UI Generates queries and sending them to the server Interprets the server response and showing the results The benefits of using a REST API are that most forms of clients can connect to the web service , and that the server is not bound to its default web UI . Skyminer is also integrated with high-quality open source tools for data visualization : Grafana - A data analytics dashboard Jupyter - A web application capable of running live code and visualization BIRT ( Business Intelligence and Reporting Tools ) - a report designer and generation engine . BIRT Plugin is not part of the server environment .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Introduction, Chapter = Versions, Paragraph = Skyminer) Skyminer ( full version ) contains all the baseline Skyminer features , analytics features , and optional modules that can be activated on-demand . c . f . Optional Modules',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Introduction, Chapter = Versions, Paragraph = Skyminer Lite) Skyminer Lite ( limited version ) contains all the baseline Skyminer features excepted the following ones : Predictive analysis Outlier detection Correlation analysis Optional modules that can be activated on-demand can also be activated on Skyminer Lite . c . f . Optional Modules 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Installation) Contents : Skyminer Architecture Introduction Docker Prerequisites System Using Docker compose v2+ Firewall configuration Disable Linux Swap NTP Configuration Security settings Docker network Installation files Data & filesystems Disks Filesystems Cassandra OpenSearch Moving data to appropriate filesystem Mapped directories ( volumes ) List of mapped volumes : Installing & starting Skyminer Installing on a standalone server Installing on a 2 nodes cluster Installing on a 3+ nodes cluster Post-installation steps Upgrade protocol to 2 . 7 Prepare 2 . 7 . x installation Shutdown all Grafana instances Backup databases Shutdown Skyminer older version Install Skyminer new version Migrate data Migrate PostgreSQL Restore OpenSearch Snapshot Check Skyminer configuration Restart Skyminer RollBack Procedure Managing Skyminer license Overview Activating Skyminer 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Configuration) Contents : Sharing Resources between different instances Use a shared filesystem for Skyminer data Configure a NFS server Grafana Login Unsigned plugins Skyminer datasource Detailed configuration Skyminer Health Monitor Health Monitor operation Health Monitor module configuration Enabling the Health Monitor module after an upgrade Disabling the Health Monitor module High availability / Load Balancer configuration Prerequisite Architecture Configuration Create high availability service Configure front-end service Share jupyter notebooks KairosDB Skyminer Parameters Enabling authentication Host Features Selection Web services filter Note Please refer to Optional Modules for optional modules configuration 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Administration) Contents : Checking the status of Skyminer Starting & Stopping Skyminer Starting & Stopping Skyminer alone Starting & Stopping Skyminer and all associated services Starting & Stopping Skyminer Demo Data Generator Starting & Stopping Cassandra on a node Backing up & Restoring Backing up Skyminer configuration Backing up Skyminer time series data Backing up Grafana data Taking and restoring OpenSearch snapshot Deleting Data from Skyminer Using the API Using the delete_metrics script Using delete_metric_data python script Delete all data from the datastore Troubleshooting Service is down Check the log files Fix corrupted Cassandra data How to use hostnames whithout DNS on docker-compose Procedure for each host Extra hosts definitions format 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Time series datastores) Datastores are data & storage providers for Skyminer time series . Cassandra datastore Indexed Cassandra datastore Prometheus read-only datastore ( Experimental ) RocksDB local storage datastore ( Experimental ) Throwaway ( no storage ) datastore 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = OpenSearch database) Skyminer sets an OpenSearch database up with its dashboards utility . This database allows you to store and index JSON documents for fast retrievals and display in dashboards .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = OpenSearch database, Chapter = OpenSearch cluster configuration) OpenSearch cluster configuration is currently incomplete as done by Skyminer quick_start . sh script . In order to configure proper operational cluster you may want to create a docker-compose . override . yml file ( this file needs to be in the same location as the Skyminer docker-compose . yml file , typically <install-dir>/services/skyminer ), with the following contents , replacing the following placeholders <HOST_ID> : unique identifier for the host ( e . g . skyminer-opensearch-0 , or same as host name ) <ALL_NODES> : all the IP addresses of the cluster separated by a comma ( e . g . 192 . 0 . 2 . 1 , 192 . 0 . 2 . 2 , 192 . 0 . 2 . 3 <CLUSTER_SEED_NODES> : IP addresses for seed nodes of the cluster separated by a comma ( e . g . 192 . 0 . 2 . 1 , 192 . 0 . 2 . 3 ) â\\x80¦ It is fine to use the same addresses as Cassandra seed nodes <HOST_IP> : public IP address of the local host ( same as cassandra broadcast address ) <MEMORY> : Number of MiB of memory to allocate to OpenSearch . Set to minimal 512 MB , no more than 8000 MB - typically 1/8 to 1/5 of available system memory when running together with Cassandra . <OPENSEARCH_CLUSTER_NAME> : name of the cluster - it is critical to put the same cluster name here and in skyminerdocumentmodules . properties configuration file The template contents for the docker-compose . override . yml file is as follows : version : \\'2 . 2\\' services : skyminer-opensearch : restart : always image : skyminer-opensearch :<version> container_name : skyminer-opensearch environment : - node . name=<HOST_ID> - cluster . initial_master_nodes=<ALL_NODES> - cluster . name=<OPENSEARCH_CLUSTER_NAME> - transport . tcp . port=9300 - bootstrap . memory_lock=true - http . cors . enabled=true - http . cors . allow-origin=* - http . cors . allow-headers=Content-Type , Access-Control-Allow-Headers , Authorization , X-Requested-With - \"ES_JAVA_OPTS=-Xms<MEMORY>m -Xmx<MEMORY>m\" - DISABLE_INSTALL_DEMO_CONFIG=true - discovery . seed_hosts=<CLUSTER_SEED_NODES> - network . host=0 . 0 . 0 . 0 - node . roles=master , ingest , data - NODE_TLS_REJECT_UNAUTHORIZED=0 - network . publish_host=<HOST_IP> ulimits : memlock : soft : -1 hard : -1 volumes : - ./opensearch/esdata :/usr/share/opensearch/data : z - ./opensearch/backup :/usr/share/opensearch/backup : z - ./opensearch/config :/usr/share/opensearch/config : z ports : - 9200 : 9200 - 9300 : 9300 - 9600 : 9600 logging : driver : \"json-file\" options : max-size : \"50m\" max-file : \"10\" Then apply the new settings by running docker-compose up -d command . ( or docker compose up -d for docker compose V2+) Once nodes are restarted you can check cluster status by opening this URL : http ://<IP_ADDRESS>/_cluster/stats 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor) The Skyminer query processor is the processing chain that is executed when running a query . It breaks down queries into seven stages , with all of them except Data retrieval and Results serialization being optional . Data retrieval Data grouping Horizontal aggregation Vertical aggregation Prediction Outlier detection Results serialization',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor, Chapter = Data Retrieval) The query provides a list of metric names , a time span , and optionally a filtering by tag . Skyminer will retrieve all data from the datastore that matches the query .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor, Chapter = Data Grouping) If the query provides group-by definitions , the query processor generates as many data groups as required . Each data group is then processed as an individual time series . Note If no group-by is defined by the query , Skyminer will merge data from all possible groups into a single time series .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor, Chapter = Horizontal Aggregation) We call this stage â\\x80\\x9cHorizontal Aggregationâ\\x80\\x9d because each group is processed individually over time , on a typical time series chart each data series is an horizontal line . Usually those aggregators are used to downsample data ( e . g . averaging every hour , or computing standard deviation for each 5 minutes bucket ). There are many aggregation options available ( see Aggregators ). As many aggregators as needed can be chained ( e . g . daily sum of the 5 minutes average ).',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor, Chapter = Vertical Aggregation) We call this stage â\\x80\\x9cVertical Aggregationâ\\x80\\x9d because several groups are processed together over time ( e . g . finding the minimum or maximum from a selection of sources ).',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query processor, Chapter = Outlier detection) The outlier detection stage would return outliers in a data set using different algorithms or models . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = API Documentation) REST API Overview OpenAPI Add Data Points Aggregators Correlations API Delete Data Points Delete Metric Event sourcing Features Grouping Health Checks List Metric Names Metadata Query CSV Query Metric Tags Query Metrics Skyminer Time Series Indexer Module API Skyminer Version Version Telnet API Overview Put Putm Special Tags Version Python API ( for sending data ) Skyminer Time Series Python Connector Note Please refer to Optional Modules for optional modules API 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Optional Modules) These optional modules are available with Skyminer : Skyminer Time Series Indexer Module Time series indexer requirements Time series indexer configuration Time series indexer mechanisms Time series indexer metrics Enabling the time series indexer after an upgrade Disabling the time series indexer Skyminer time series indexer module API Skyminer document Module Skyminer document module requirements Skyminer document module configuration Skyminer document module API Other Modules Baseline modules Additional modules 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I upgrade Grafana or Jupyter myself?) It is not possible to upgrade Grafana or Jupyter yourself . However you can signal to Kratos that you would like an upgrade .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I add my own Grafana or Jupyter plugins?) It is not possible to add your own Grafana or Jupyter plugins , however you can run your own Grafana or Jupyter instances and ask Kratos for the Skyminer integration plugins . Suggestions and enhancement requests are welcome so such plugins might be added in later versions of Skyminer .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Why is there a difference between the values of the timeseries on Skyminer and the ones on Monics?) Check the configuration of the agent Monics2Skyminer to make sure the data sent to Skyminer is coming from the correct table of the database .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = FAQ, Chapter = Can I push custom data to Skyminer?) You can submit custom data to Skyminer by using the REST protocol . See Add Data Points . Custom data types are supported via extensions . Please contact Kratos for more information . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Dependencies and licenses) Note This list is only available in the HTML version of the Skyminer administration documentation 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software) Skyminer uses the following third-party open source software . We are very careful that no third-party license would include any kind of incompatibility or impediment for a commercial environment usage . Please check the link to each third-party software for more details . A copy of the open-source licenses is provided . With the exception of KairosDB , Kratos would not directly support the third-party software that are listed below . Kratos provides Skyminer support within the scope of Kratos system designs . For advanced configurations and use cases , if needed , Commercial support for the third-party software is available from other companies that are involved in their development .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = KairosDB, Paragraph = Information) KairosDB is an open source scalable time series database . License : Apache 2 . 0 URL : http ://kairosdb . github . io/ Usage in Skyminer : runtime , mandatory component - Skyminer is built on top of KairosDB time series database . Commercial support available : No , but supported by Kratos for Skyminer deployments Version : 1 . 3 . 0-0 . 5',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = KairosDB, Paragraph = Documentation) Applicable KairosDB documentation is already integrated to Skyminer documentation . However , you can find KairosDB documentation online at https ://kairosdb . github . io/docs/build/html/index . html',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Cassandra, Paragraph = Information) Apache cassandra is an open source high-performances scalable column-family database . License : Apache 2 . 0 URL : http ://cassandra . apache . org/> Usage in Skyminer : runtime , only required if cassandra is the system used to save Skyminer data Commercial support available : Yes Version : 3 . 11 . 13',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Cassandra Range Repair Script) A Python utility to repair the primary range of a Cassandra node in N discrete steps using best practice . License : MIT URL : https ://github . com/BrianGallew/cassandra_range_repair Usage in Skyminer : Integrated to Cassandra container to perform more efficient and trustworth repair operations Commercial support available : No',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Grafana, Paragraph = Information) Grafana is an open source software for time series dashboard & analytics . License : Apache 2 . 0 URL : https ://grafana . com/ Usage in Skyminer : runtime , only for dashboards ( optional feature ), can be removed Commercial support available : Yes Version : 7 . 5 . 15',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Jupyter, Paragraph = Information) Jupyter is an open-source web application used to create and share documents that contain live code and visualizations . License : BSD URL : https ://jupyter . org/ Usage in Skyminer : runtime , only for analytics ( optional feature ), can be removed Commercial support available : No Version : 7 . 16 . 1',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Nginx, Paragraph = Information) Nginx is an open source high performance web services proxy , load balancer and web server . License : Apache 2 . 0 URL : https ://nginx . org/ Usage in Skyminer : runtime , can be removed Commercial support available : Yes Version : 1 . 18 . 0',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = PostgreSQL, Paragraph = Information) PostgreSQL is an open source relational database management system . License : PostgreSQL License , a liberal Open Source license , similar to the BSD or MIT licenses URL : https ://www . postgresql . org/ Usage in Skyminer : runtime , only required if used to save Grafana dashboards ( even optional for Grafana ) Commercial support available : Yes Version : 14-alpine',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Collectd, Paragraph = Information) Collectd is a daemon which collects system and application performance metrics periodically and provides mechanisms to store the values in a variety of ways License : Collectd License , MIT license URL : https ://collectd . org/ Usage in Skyminer : runtime , used to collect Cassandra metrics ( Optional ) Commercial support available : No Version : 5 . 7 . 2',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Opensearch, Paragraph = Information) opensearch is a distributed RESTful search engine built for the cloud . License : openSearch License , Apache 2 license URL : https ://opensearch . org/ Usage in Skyminer : runtime - Optional , required only for document data types and related services Commercial support available : Yes Version : 2 . 9 . 0',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = Python packages) The Python environment uses several packages with different licenses . The list of each package with its version and license can be found here ( in the HTML version of the documentation )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Integrated third-party software, Chapter = User Interface, Paragraph = Angular) Angular is an open-source framework used to design dynamic web applications . License : MIT URL : https ://angular . io/ Usage in Skyminer : Framework used to build the query building UI All licenses of used packages can be found HERE 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Architecture, Chapter = Introduction) Skyminer is a data storage & analytics service . It relies on other server-side high-quality services : Grafana ( dashboard ) and usually Cassandra ( datastore ). Note that third-party software are optional and can be replaced by alternatives to match any need or requirement . cf . Integrated third-party software .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Architecture, Chapter = Docker) Skyminer is usually deployed as a composition of independent services deployed as containers . These containers are managed by Docker , and have the following characteristics : They are isolated from each other and bundle their own application , tools , libraries and configuration files They can easily communicate with each other through well-defined channels All containers are run by a single operating-system kernel and are thus more lightweight than virtual machines Containers are created from â\\x80\\x9cimagesâ\\x80\\x9d that specify their contents By using containers Skyminer benefits from easier deployment and maintainability , as well as increased security . On a stand-alone server there can be up to 10 containers : skyminer-cassandra ( main datastore ) skyminer-collectd ( Cassandra data collection ) skyminer-demo-data-generator ( Demo data ) skyminer-opensearch ( Documents storage ) skyminer-opensearch-dashboard ( opensearch UI ) skyminer-front ( Web UI ) skyminer-grafana ( Grafana dashboard ) skyminer-jupyter ( Analytics ) skyminer-nginx ( HTTP proxy ) skyminer-service ( Skyminer server ) postgresql_grafana ( Grafana database ) Refer to the following links for more details about Docker and containers : https ://www . docker . com/ https ://en . wikipedia . org/wiki/Linux_containers https ://en . wikipedia . org/wiki/Operating-system-level_virtualization https ://en . wikipedia . org/wiki/Docker_ ( software ) 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = System) To install Skyminer , the following items are required : A Linux server with a recent kernel Docker and docker-compose or docker compose v2+ A Firewall configuration with the required ports open ( see Firewall configuration ). It is not recommended to disable the firewall as it would prevent the system from working properly . Disable Linux swap ( see Disable Linux Swap ) It is also highly recommended to synchronize the host ( s ) to a reliable time reference using NTP or any similar protocol . See NTP Configuration . You may need unzip tool to extract the delivery package . The installation script requires the following standard linux tools : awk bash curl openssl ping sed For agents , you must have Java 8 or 11 installed .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Using Docker compose v2+) docker-compose utility has been deprecated by Docker and replaced by the docker compose command which supports the same features . Unfortunately the new docker compose command is not as compatible as expected , and moreover docker is not supported anymore by in baseline RHEL-based releases . This guide still use the older docker-compose commands , but Skyminer scripts will use docker-compose or docker compose , depending on docker version .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Firewall configuration) If a firewall is active on the host it may affect the connection to Skyminer , and the communication between its different internal services .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Firewall configuration, Paragraph = Required TCP Ports) The following ports are required by nginx-proxy : 443/TCP ( HTTPS ) - if running services in HTTPS 80/TCP ( HTTP ) - if running services in HTTP The folllowing ports are required for a Cassandra cluster by skyminer-cassandra ( not required for a standalone server ): 7000/TCP internode communication for a nominal Cassandra configuration 7001/TCP ( optional ) internode communication if cassandra is configured using TLS ( reduces performances ) 9042/TCP CQL native transport 7199/TCP instrumentation port The following ports are required for Grafana database by skyminer-grafana-postgres ( only for hosts running postGreSQL ): 5432/TCP ( PostGreSQL ) The following ports are required for OpenSearch : 9200/TCP ( API ) 9300/TCP ( Transport ) 9600/TCP ( Monitoring - optional )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Firewall configuration, Paragraph = Optional TCP Ports) The following ports are optionally required for supporting other protocols that may be used to populate data : 4242/TCP ( Telnet protocol data API ) 2003-2004/TCP ( Graphite protocol data API )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Firewall configuration, Paragraph = Red Hat 8 and derivatives (RHEL 8, Oracle 8â\\x80¦)) For Oracle Linux 8 , RHEL 8 , or CentOS 8 distribution , an additional command is required to allow Docker ingress : firewall-cmd --zone=public --add-masquerade --permanent',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Firewall configuration, Paragraph = Example configuration of firewall) For example , on an Oracle Linux 8 , RHEL 8 , or CentOS 8 Linux server server , the following commands run as root ( or using sudo ) would open the required ports for a full standalone Skyminer system : firewall-cmd --permanent --add-port=5432/tcp firewall-cmd --permanent --add-port=80/tcp firewall-cmd --permanent --add-port=443/tcp firewall-cmd --permanent --add-port=4242/tcp firewall-cmd --permanent --add-port=7000/tcp firewall-cmd --permanent --add-port=9042/tcp firewall-cmd --permanent --add-port=7199/tcp firewall-cmd --permanent --add-port=9200/tcp firewall-cmd --permanent --add-port=9300/tcp firewall-cmd --permanent --add-port=9600/tcp firewall-cmd --reload',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Disable Linux Swap) Make sure OS has enough memory ( RAM ) and disable swap . Swap usage creates important slowdown of system performances , and creates additional I/O problems if the same storage device is used for swap file and data . For example , on a Oracle Linux 8 , RHEL 8 , or CentOS 8 linux server : Run swapoff -a command as root then Remove swap entries from /etc/fstab file And/or add the line vm . swappiness = 0 in the file /etc/sysctl . conf file Reboot and check swap usage is zero using free -h command',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = NTP Configuration) The Network Time Protocol ( NTP ) is a networking protocol for clock synchronization between computer systems over packet-switched , variable-latency data networks . NTP is one of the oldest Internet protocols in current use . Skyminer server uses local time to record its own internal metrics or to compute start/end time for relative time queries . Therefore it is important ( but not critical , since data is timestamped by the data sources ) for the server hosting Skyminer to be configured with a reliable time source to avoid uncontrolled clock drift . For example NTP might be configured as follows on a Oracle Linux 8 , RHEL 8 , or CentOS 8 Linux system as root user : NTP server package is provided by default from official repositories and can be installed by issuing the following command : dnf install chrony Edit the Chrony NTP daemon configuration file /etc/chrony . conf to make sure the list of servers is correct . Ensure the chronyd daemon ( service ) is up and running . # Check status of NTP daemon systemctl status chronyd # Enable daemon if required systemctl enable chronyd # Run NTP daemon systemctl start chronyd At any time , you may check the time synchronization by issuing the following command : chronyc tracking',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Security settings, Paragraph = Open Ports) Securing a network requires to configure firewall and network routing policies appropriately . In order to support multiple kinds of installations , a few ports that may not be required are open . Please refer to the section on Firewall configuration for the list of open ports and their usage . The ports open to the outside world are defined in the docker-compose files . This configuration is additional to a proper firewall configuration and network system design .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Security settings, Paragraph = Authentication) Skyminer built-in authentication can be enabled , please see chapter on Enabling authentication . Grafana authentication can be enforced , please follow instructions in Detailed configuration for details . PostGreSQL authentication can be modified . On a new system the passwords are defined , for a live system please check PostgreSQL documentation .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Security settings, Paragraph = Host features selection (specialization)) A server can be specialized for specific activities only . For example a single node may only provide : Storage ( e . g . Cassandra if Cassandra datastore is used ) Queries and user interface Data processing and data management ( push/delete ) This is described in the chapter : Host Features Selection',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prerequisites, Chapter = Docker network) You may need to ensure that Docker bridge network is not able to reserve addresses that are conflicting with your LAN . For instance Docker uses by default some network addresses in the 172 . 17 . 0 . 0/16 and 192 . 168 . 0 . 0/16 pools , if your LAN addresses are conflicting with these pools an appropriate configuration is required to ensure correct operation . Please refer to Docker documentation https ://docs . docker . com/network/ in order to configure appropriate network addresses pool for docker . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Installation files) The delivered files are : container-images skyminer-v . r . image . tar , the Skyminer docker image with v . r being its version ( e . g . Â skyminer-2 . 0 . image . tar ) cassandra-skyminer-v . r . image . tar , the Cassandra docker image with v . r being its version ( e . g . Â cassandra-skyminer-2 . 0 . image . tar ) grafana-skyminer-v . r . image . tar , the Grafana docker image with v . r being its version ( e . g . Â grafana-skyminer-2 . 0 . image . tar ) postgres-skyminer-v . r . image . tar , the PostgreSQL docker image with v . r being its version ( e . g . Â postgres-skyminer-2 . 0 . image . tar ) nginx-skyminer-v . r . image . tar , the Nginx docker image with v . r being its version ( e . g . Â nginx-skyminer-2 . 0 . image . tar ) installation , the folder with all the files needed to install Skyminer conf , the folder containing Skyminerâ\\x80\\x99s configuration files grafana_skyminer_plugin , the Skyminer plugin for Grafana nginx_config , the configuration file for Nginx reverse proxy postgres_docker , the folder with the docker-compose . yml for the PostgreSQL database docker-compose . tmpl , a pre-configured template for building docker-compose . yml install_and_upgrade_common_functions . sh , the file containing common functions for installing and upgrading a release quick_start_skyminer . sh , the shell script to execute when you run Skyminer for the first time wait-for-http-service . sh , a script needed by quick_start_skyminer . sh This installation guide 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Data & filesystems, Chapter = Disks) SDD drives are highly recommended . No RAID is required , although a RAID-0 is useful to benefit from larger volume of storage . Skyminer is also designed to operate on HDD , in such cases fast HDD with RAID10 is recommended . Each Skyminer node is designed hold up an amount of about 4TiB of data and requires a margin for maintenance operations ( hence it is required to provision 50% of additional storage space ). It is recommended to have large storage for big data distributed database systems ( Cassandra data , OpenSearch ) - there is no need for disk redundancy ( RAID ) It is recommended to have large redundant storage for RocksDB if enabled ( alternative to Cassandra ) It is recommended to have redundant storage on a NFS server for Jupyter notebooks',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Data & filesystems, Chapter = Moving data to appropriate filesystem, Paragraph = Move data) Move data files to the other filesystem . For example for moving cassandra data . For example : mv <path-to-original-cassandra-data-volume> <path-to-cassandra-data-volume-on-another-partition> Then create a logical link from the original location to the new one . For example : ln -sf <path-to-cassandra-data-volume-on-another-partition> <path-to-original-cassandra-data-volume> cf . Mapped directories ( volumes ) section to locate the data directories likely to me moved away .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Data & filesystems, Chapter = Moving data to appropriate filesystem, Paragraph = Restart Skyminer) See Starting & Stopping Skyminer and all associated services 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Mapped directories (volumes)) There are several mapped directories . For convenience , those directories will be created in the <installation_directory> folder , whose path is asked by quick_start_skyminer . sh . Volumes are classified as follow : ( big data ) : Data folder that requires large storage ( e . g . more than 1TiB ) - these might need to be relocated to a larger disk partition ( stateful ) : Folder that would benefit to be shared across nodes of a cluster ( config ) : Configuration file , or configuration data ( data ) : Data folder that does not require large storage ( e . g . less than 100GiB ) Typically : Actions are usually required to relocate ( big data ) storage to appropriate disk/partitions Actions are usually required to relocate ( stateful ) storage to shared filesystem ( only if operating on a cluster ) No actions are usually required for other storage',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Mapped directories (volumes), Chapter = List of mapped volumes:) Skyminer <installation_directory>/skyminer/conf : ( config ) Skyminer configuration files <installation_directory>/skyminer/queue : ( config ) Skyminer cache data <installation_directory>/skyminer/license : ( config ) folder where the Skyminer license should be placed after the installation . It must be named signature . Cassandra <installation_directory>/skyminer/cassandra/data : ( big data ) Cassandra data storage <installation_directory>/skyminer/cassandra/commitlog : ( big data ) Cassandra commit-log storage <installation_directory>/skyminer/cassandra_jmx/jmxremote . password : ( config ) file containing the jmx credentials for Cassandra Nginx <installation_directory>/skyminer/nginx_volumes/conf : ( config ) Nginx configuration took from the default . conf or ssl . conf , depending on the protocol <installation_directory>/skyminer/nginx_volumes/certs : ( config ) Nginx certificate for SSL <installation_directory>/skyminer/nginx_volumes/dhparam : ( config ) Nginx Diffieâ\\x80\\x93Hellman exchange key Grafana <installation_directory>/skyminer/grafana-dashboards : ( config ) folder that contains provisioned dashboards and related information for Grafana ( several volumes point to this directory ) <installation_directory>/skyminer/grafana-plugins : ( config ) folder that contains the Skyminer plugins for Grafana Postgre SQL <installation_directory>/postgresql/data : ( data ) folder that contains the data of the PostgreSQL database ( exists only if you choose to use the embeded PostgreSQL ). Skyminer Data Demo Generator <installation_directory>/ddgt/cache : ( data ) folder that contains the data of Skyminer Data Demo Generator ( exists only if you choose to use the Skyminer Data Demo Generator ). <installation_directory>/ddgt/checkpoints : ( data ) folder that contains the checkpoint files of Skyminer Data Demo Generator ( exists only if you choose to use the Skyminer Data Demo Generator ). <installation_directory>/ddgt/log : ( data ) folder that contains the Skyminer Data Demo Generator logs ( exists only if you choose to use the Skyminer Data Demo Generator ). OpenSearch <installation_directory>/skyminer/opensearch/esdata :: ( big data ) folder that contains the data for OpenSearch documents database <installation_directory>/skyminer/opensearch/backup :: ( big data ) folder that contains the backup data for OpenSearch documents database <installation_directory>/skyminer/opensearch/config :: ( config ) folder that contains the configuration information for OpenSearch documents database <installation_directory>/skyminer/opensearch-dashboard/config :: ( config ) folder that contains the configuration information for OpenSearch Dashboard tool Jupyter notebook <installation_directory>/skyminer/jupyter/jupyter_notebook_config . py : ( config ) Jupyter notebooks configuration file <installation_directory>/skyminer/jupyter/delete_metric_data . py : ( config ) Jupyter ancillary script <installation_directory>/skyminer/jupyter/notebooks : ( config ) & ( stateful ) folder that contains example Jupyter notebooks Front <installation_directory>/skyminer/front/default . conf : ( config ) Nginx configuration for the front-end of Skyminer <installation_directory>/skyminer/front/config : ( config ) Configuration directory for the front-end of Skyminer <installation_directory>/skyminer/front/openapi : ( config ) Contains Skyminer OpenAPI description Collectd <installation_directory>/skyminer/collectd/collectd . conf : ( config ) collectd default configuration file for Skyminer Customization <installation_directory>/skyminer/customization/customer_logo : ( config ) & ( stateful ) folder that contains the logo displayed in the top left corner of the Skyminer UI . The file must be named logo . png <installation_directory>/skyminer/customization/custom_resources : ( config ) & ( stateful ) folder that contains shared web resources to be available to Skyminer UI components ( e . g . images to put in dashboards )`` Webhook <installation_directory>/skyminer/webhook/webhook-service-conf . json :: ( config ) folder that contains the config file for the webhook service Please refer to Data & filesystems to setup appropriate disks type and data relocation of these volumes . Please refer to Sharing Resources between different instances to setup shared filesystem for some of these volumes . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Installing & starting Skyminer) Run the quick_start_skyminer . sh script as root , or using the sudo command ./quick_start_skyminer . sh Follow the script instructions . It allows you to choose : The path of the working directory , which is where all the data will be stored An IP or hostname on which the environment will be installed Whether https is used or not . If if is , the script will automatically generate self-signed certificates and keys Whether the Data Demo Generator is enabled or not . If it is , demo dashboards will be available in Grafana A PostgreSQL database different from the provided one What JMX credentials should be used to connect to Cassandra A docker-compose . yml will be generated from the docker-compose . tmpl in <installation_directory>/skyminer . This file allows to configure and run all docker containers for Skyminer services . At the end of this script you should have a working Skyminer environment accessible via browser at <protocol>://<hostname>',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Installing on a 3+ nodes cluster) For a cluster you will select the following answers : local postgresql database only on one node ( first to be installed ) for other nodes select remote postgresql database and enter postgresql information from first node ( host , user , password ) Set all nodes for database cluster nodes ( typically Cassandra and OpenSearch ) Set only a part of the nodes as seed nodes ( not all nodes )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Post-installation steps) The installation deploys and starts Skyminer with some default settings , some tuning might be requested . Please read carefully the following instructions : Mapped directories ( volumes ) to properly understand and configure your data volumes Cluster Configuration to properly configure your Cassandra cluster ( if using Cassandra cluster ) Consistency checking to enable routine cluster consistency checking ( if using Cassandra ) High availability / Load Balancer configuration to operate Skyminer in a proper transparent high-availability OpenSearch cluster configuration to configure your OpenSearch cluster ( if running as a cluster ) 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7) This document explain how to manually upgrade to Skyminer 2 . 7 . X . This protocol applied from version >= 2 . 5 . Note This section still use the older docker-compose command , but if you have docker compose V2+, you must use docker compose command instead .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Prepare 2.7.x installation) This step has to be done on each server on cluster : Unzip the 2 . 7 . X package in a separate folder from installation : <path_to_skyminer2 . 7>. Execute init_environment_conf_file . sh ( with path of skyminer older installation services as parameter ) <path_to_skyminer2 . 7>/installation/upgrade/init_environment_conf_file . sh <path_to_older_skyminer_install>/services/ This script will generate skyminer-custom-settings . conf based on current installation . In this configuration , you will find the followings properties : Skyminer configuration propertiesÂ Property description LOCALHOST_IP IP address for the host PUBLIC_IP_ADDRESS IP address for the host CASSANDRA_SEEDS comma-separated list of IP addresses of each cassandra nodes GRAFANA_ADMIN_USER Grafana admin user GRAFANA_ADMIN_PASSWORD Grafana admin password POSTGRESQL_IP IP on which postgreSQL database is reachable POSTGRESQL_DATABASE_USER PostgreSQL database user POSTGRESQL_DATABASE_PASSWORD PostgreSQL database password USE_HTTPS If Y or y , use https , otherwise http Please check this file before continuing ( generated in <path_to_skyminer2 . 8>/installation/upgrade directory ).',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Shutdown all Grafana instances) Stop all grafana instances to avoid using a shared database with different versions of Grafana . For each server on cluster docker stop skyminer-grafana',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Backup databases, Paragraph = Backup Postgresql and Cassandra) Go on the server hosting PostgreSQL and backup Cassandra and PostgreSQL : <path_to_skyminer2 . 7>/installation/upgrade/generate_skyminer_backup_data . sh It will ask for the path to the previous installation service folder and the credential for cassandra . Repeat this step on another nodes on cluster , it will only backup cassandra . Note Remember the generated backup file , it will be asked while restoring .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Backup databases, Paragraph = Configure Opensearch for backup) Note This section applies only if you migrate from a 2 . 7 . 2 version or earlier and you want to keep some elasticsearch data If you are on 2 . 6 . X or earlier , replace opensearch with elasticsearch for each previous step Update docker-compose . yml , for opensearch container on environment element , add : ``- path . repo=/usr/share/opensearch/backup`` on volumes element , add : ``- ./opensearch/snapshots :/usr/share/opensearch/backup : z`` Create â\\x80\\x9csnapshotsâ\\x80\\x9d folder on <path_to_older_skyminer_install>/services/skyminer/opensearch mkdir <path_to_older_skyminer_install>/services/skyminer/opensearch/backup Change permission on this folder to allow snapshots creation chmod 777 <path_to_older_skyminer_install>/services/skyminer/opensearch/backup Restart container docker-compose -f <path_to_older_skyminer_install>/services/services/skyminer/docker-compose . yml up -d',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Backup databases, Paragraph = Take a snapshot for opensearch backup) If you want to backup some opensearch indices , you have to : Register the repository curl -XPUT \"http ://<OPENSEARCH_SEARCH_HOST_IP>: 9200/_snapshot/snapshots-repository\" -H \\'Content-Type : application/json\\' -d\\' { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/opensearch/backup\" } }\\' Take a snapshot of wanted indices curl -XPUT \"http ://<OPENSEARCH_SEARCH_HOST_IP>: 9200/_snapshot/snapshots-repository/skyminer-2 . 7-snapshot\" -H \\'Content-Type : application/json\\' -d\\' { \"indices\": \"*\", \"ignore_unavailable\": true , \"include_global_state\": false , \"partial\": false }\\' Instead of all indices (*), you can specify only indices you want to backup Wait until snapshot is done . Status can be checked with curl -XGET \"http ://<OPENSEARCH_SEARCH_HOST_IP>: 9200/_snapshot/snapshots-repository/skyminer-2 . 7-snapshot/_status\" Further information is available on https ://opensearch . org/docs/1 . 3/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/#take-snapshots .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Shutdown Skyminer older version) For each server on cluster , starting with the one hosting postgresql database : Shutdown Skyminer older version : docker-compose -f <path_to_older_skyminer_install>/services/skyminer/docker-compose . yml down Shutdown PostgreSQL if it is installed locally : docker-compose -f <path_to_older_skyminer_install>/services/postgresql/docker-compose . yml down Shutdown Demo Data Generator if it is installed locally : docker-compose -f <path_to_older_skyminer_install>/services/ddgt/docker-compose . yml down',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Install Skyminer new version) For each server on cluster , install Skyminer v2 . 7 : cd <path_to_skyminer2 . 7>/installation sudo ./quick_start_skyminer . sh --conf <path-to-custom-skyminer-settings-file> <path-to-custom-skyminer-settings-file> is path to the file generated on Prepare 2 . 7 . x installation step ( cf . Prepare 2 . 7 . x installation )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Migrate data) Shutdown Skyminer 2 . 7 : docker-compose -f <path_to_skyminer2 . 7>/services/skyminer/docker-compose . yml down Shutdown PostgreSQL if it is installed locally : docker-compose -f <path_to_skyminer2 . 7>/services/postgresql/docker-compose . yml down Shutdown Demo Data Generator if it is installed locally : docker-compose -f <path_to_skyminer2 . 7>/services/ddgt/docker-compose . yml down Copy the data from older Skyminer : cp -r <path_to_older_skyminer_install>/services/skyminer/license <path_to_skyminer2 . 7>/services/skyminer/ cp -rf <path_to_older_skyminer_install>/services/skyminer/jupyter/notebooks/ <path_to_skyminer2 . 7>/services/skyminer/jupyter/notebooks/ Since there can be a lot of data in cassandra move the data instead of copying mv <path_to_skyminer2 . 7>/services/skyminer/cassandra/data <path_to_skyminer2 . 7>/services/skyminer/cassandra/data-default mv <path_to_older_skyminer_install>/services/skyminer/cassandra/data <path_to_skyminer2 . 7>/services/skyminer/cassandra/',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Migrate PostgreSQL) If PostgreSQL is installed locally , run : <path_to_skyminer2 . 7>/installation/upgrade/migrate_postgres . sh Enter username ( same as GF_DATABASE_USER ), pwd ( same as GF_DATABASE_PASSWORD ) and backup file ( from Backup databases step cf . Backup databases )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Restore OpenSearch Snapshot) Note If you are on 2 . 6 . X or earlier , replace opensearch with elasticsearch for each previous step If you want to restore Opensearch data , you can proceed as follow : Copy previously created elastic search snapshot ( from Backup opensearch step cf . Take a snapshot for opensearch backup ): cp -r <path_to_older_skyminer_install>/services/skyminer/opensearch/backup/* <path_to_skyminer2 . 7>/services/skyminer/opensearch/backup/ Register Snaphot repository : curl -XPUT \"http ://<ELASTIC_SEARCH_HOST_IP>: 9200/_snapshot/snapshots-repository\" -H \\'Content-Type : application/json\\' -d\\' { \"type\": \"fs\", \"settings\": { \"location\": \"/usr/share/opensearch/backup\" } }\\' To see all snapshots in the repository : curl -XGET \"http ://<OPENSEARCH_HOST_IP>: 9200/_snapshot/backup-repository/_all\" Then restore snaphot : curl -XPOST http ://<OPENSEARCH_HOST_IP>: 9200/_snapshot/backup-repository/skyminer-2 . 7-snapshot/_restore',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Check Skyminer configuration) Check the files in <path_to_skyminer2 . 7>/services/skyminer/conf/ to report any customized configuration parameter in the current Skyminer installation ( for examples cassandra cluster configuration in kairosDB . conf ). If you have docker-compose . override or a custom docker-compose configuration , please report it to the current Skyminer installation .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = Restart Skyminer) Once all data have been migrated restart skyminer Start Skyminer 2 . 7 : docker-compose -f <path_to_skyminer2 . 7>/services/skyminer/docker-compose . yml up -d Start Demo Data Generator if it is installed locally : docker-compose -f <path_to_skyminer2 . 7>/services/ddgt/docker-compose . yml up -d',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Upgrade protocol to 2.7, Chapter = RollBack Procedure) If an error occur during the upgrade process : Shutdown Skyminer 2 . 7 : docker-compose -f <path_to_skyminer2 . 7>/services/skyminer/docker-compose . yml down Shutdown PostgreSQL if it is installed locally : docker-compose -f <path_to_older_skyminer_install>/services/postgresql/docker-compose . yml down Move back the cassandra data : mv <path_to_skyminer2 . 7>/services/skyminer/cassandra/data <path_to_older_skyminer_install>/services/skyminer/cassandra Start PostgreSQL if it is installed locally : docker-compose -f <path_to_older_skyminer_install>/services/postgresql/docker-compose . yml up -d Start Skyminer previous version : docker-compose -f <path_to_older_skyminer_install>/services/skyminer/docker-compose . yml up -d Start Demo Data Generator if it is installed locally : docker-compose -f <path_to_older_skyminer_install>/services/ddgt/docker-compose . yml up -d If an error staying that a docker image is not found , run docker load -i <path_to_older_skyminer_install>/container-images/<imagename>. image . tar 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Managing Skyminer license, Chapter = Overview) Using Skyminer requires a valid license which is not shipped with the system . This license is provided separately by any secured medium at your convenience . An invalid , missing , or expired license will start a four-hour grace period during which the features from KairosDB can be used , but not the ones from Skyminer . Once the grace period is over , queries to Skyminer will be blocked until a valid license is provided .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Managing Skyminer license, Chapter = Activating Skyminer) To activate Skyminer , simply place the license file named signature in the license folder of the Skyminer installation directory : cp <path_to_the_license_file> <installation_directory>/skyminer/license Note This command must be run with elevated privileges 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances) You may want to use a NFS Server to share notebooks and other resources between different nodes . It requires one NFS server which can be one of the nodes of the Skyminer cluster ( cf . Configure a NFS server example configuration below if needed ). A NFS server might not be a highly available device and creates a single point of failure , hence it is particularly important to have a proper backup policy of the data . However backups are required for all data . NFS protocol is not suitable for databases , but is useful to share common data like Jupyter notebooks , some configutation or customization data . Also , please do not forget to backup the NFS data often enough , in order to avoid data loss in case of failure of the NFS Server .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Use a shared filesystem for Skyminer data, Paragraph = Install NFS (Client)) Example on an Oracle 8/ RHEL 8 / CentOS 8 Linux server or equivalent : Install NFS package dnf install nfs-utils',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Use a shared filesystem for Skyminer data, Paragraph = Mount a NFS shared directory) Example on an Oracle 8/ RHEL 8 / CentOS 8 Linux server or equivalent : Backup existing directory mv /<installation_client_directory>/directory /<installation_client_directory>/directory . backup Mount the shared directory mount -t nfs <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/directory Permanent mounting Edit /etc/fstab and add the following line : <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/directory nfs defaults 0 0',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Use a shared filesystem for Skyminer data, Paragraph = Example with Jupyter notebook) Stop Skyminer Backup existing directory mv /<installation_client_directory>/jupyter/notebooks /<installation_client_directory>/jupyter/notebooks . backup Mount the shared directory mount -t nfs <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/jupyter/notebooks Permanent mounting Edit /etc/fstab and add the following line : <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/jupyter/notebooks nfs defaults 0 0 Edit skyminer docker-compose . yml file Replace INSTALLATION_DIRECTORY/skyminer/jupyter/notebooks :/jupyter : z By INSTALLATION_DIRECTORY/skyminer/jupyter/notebooks :/jupyter',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Use a shared filesystem for Skyminer data, Paragraph = Example with custom resources) Stop Skyminer Backup existing directory mv /<installation_client_directory>/skyminer/customization/custom_resources /<installation_client_directory>/skyminer/customization/custom_resources . backup Mount the shared directory mount -t nfs <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/skyminer/customization/custom_resources Permanent mounting Edit /etc/fstab and add the following line : <nfs_server_ip>:/<path_to_shared_directory> /<installation_client_directory>/skyminer/customization/custom_resources nfs defaults 0 0 Edit skyminer docker-compose . yml file Replace INSTALLATION_DIRECTORY/skyminer/skyminer/customization/custom_resources :/var/www/img/custom_resources : z By INSTALLATION_DIRECTORY/skyminer/skyminer/customization/custom_resources :/var/www/img/custom_resources',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Configure a NFS server, Paragraph = Install NFS package if needed) Example on an Oracle 8/ RHEL 8 / CentOS 8 Linux server or equivalent : dnf install nfs-utils Add the NFS service override in firewall-cmd public zone service firewall-cmd --permanent --zone=public --add-service=nfs firewall-cmd --permanent --zone=public --add-service=mountd firewall-cmd --permanent --zone=public --add-service=rpc-bind firewall-cmd --reload Start the services and enable them to be started at boot time systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Sharing Resources between different instances, Chapter = Configure a NFS server, Paragraph = Export NFS shared directory) Example on an Oracle 8/ RHEL 8 / CentOS 8 Linux server or equivalent : Create shared folder and grant permissions mkdir <path_to_shared_directory> chown nfsnobody : nfsnobody <path_to_shared_directory> chmod -R 777 <path_to_shared_directory> chmod g+s <path_to_shared_directory> Export shared directory using NFS Edit /etc/exports and add as many client IP as there are Skyminer nodes ( replace client_ip_1 , etc , by Skyminer node IP ) <path_to_shared_directory> <client_ip_1>( rw , sync , no_root_squash , no_all_squash ) <client_ip_2>( rw , sync , no_root_squash , no_all_squash ) <client_ip_N>( rw , sync , no_root_squash , no_all_squash ) Restart NFS service systemctl restart nfs-server 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana) Skyminer is integrated with Grafana so all Grafana features are available . Grafana is a third-party application not directly supported by Kratos . Please check the third-party software chapter on Grafana for more information . Skyminer may be deployed with a Grafana server with an easy-to-use configuration , however that configuration may need some customization to be fully compliant with project-specific requirements .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana, Chapter = Login) Grafana login is by default optional , anonymous access is enabled with the capability to unlogged users to view , create and edit dashboards of the default organization . The default user login/password is admin user with a password that is set at installation . It is recommended to change this setting in Grafana once the system is deployed ( login ad admin and change to a new password ). To enable Grafan Login just change the line in the docker-compose . yml file - \"GF_AUTH_ANONYMOUS_ENABLED=true\" to - \"GF_AUTH_ANONYMOUS_ENABLED=false\". Now Grafana will require used to login to use the dashboard system .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana, Chapter = Unsigned plugins) Grafana plugin signature is complex and would require access to Grafana servers to generate the signature specific to the instance . As Skyminer is designed to operate on systems not connected to the public Internet , the relevant plugins will be displayed as unsigned in Grafana ( Unsigned external plugin ). This is perfectly normal .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana, Chapter = Skyminer datasource, Paragraph = Skyminer datasource target) Login in Grafana as administrator Go to configuration -> Data sources Edit the Skyminer default data source You may change the target URL',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana, Chapter = Skyminer datasource, Paragraph = Skyminer datasource authentication) Skyminer may require authentication on its API . If Authentication is enabled on APIs Grafana data source has to be configured accordingly : Login in Grafana as administrator Go to configuration -> Data sources Edit the Skyminer default data source Enable basic auth Enter username and password in basic auth details ( as defined in authentication configuration ( see Enabling authentication ). Click on â\\x80\\x9cSave & Testâ\\x80\\x9d',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grafana, Chapter = Detailed configuration) Please refer to Grafana Documentation for complete detailed configuration . You can select the documentation related to the delivered version on the top-right corner of the page . In particular each configuration parameter can be defined using an environment variable : GF_<SectionName>_<KeyName>. Where the section name is the text within the brackets . Everything should be upper case , . should be replaced by _ . For example , given these configuration settings : # default section instance_name = ${HOSTNAME} [ security ] admin_user = admin [ auth . google ] client_secret = 0ldS3cretKey Then you can override them using : export GF_DEFAULT_INSTANCE_NAME=my-instance export GF_SECURITY_ADMIN_USER=true export GF_AUTH_GOOGLE_CLIENT_SECRET=newS3cretKey For example you may want to modify the following settings : Basic Grafana configuration optionsÂ parameter description default value GF_DATABASE_HOST Database host , usually in the format <host>:<port None GF_DATABASE_USER Grafana Database user grafana GF_DATABASE_PASSWORD Grafana Database password varies GF_DATABASE_TYPE Database type in use for Grafana postgres GF_AUTH_ANONYMOUS_ENABLED Enable ( or disable is set to false ) anonynous access to Grafana dashboards true GF_AUTH_ANONYMOUS_ORG_ROLE Role of anonymous users , can be Editor , Admin or Viewer Editor GF_ADMIN_PASSWORD Default password for admin user , set once at first run admin 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Health Monitor) The Health Monitor module checks if subcomponents are operating properly , and stops Skyminer service if they remain in failed state for too long ( configurable delay ). This module is not necessary to operate Skyminer . However , this module is recommended in order to increase resilience in case some submodules are faulty . For instance a defect of the backend database will become more visible by having the service stopped and not restarting properly . Moreover , an automated restart of the service can be operated by the underlying orchestration environment .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Health Monitor, Chapter = Health Monitor operation) At regular intervals the module polls for health of the services ( cf . Health Checks ) If some of the modules are unhealthy a grace period delay is initiated If system remains unhealthy after expiration of the grace period , a regular shutdown of the Skyminer service is initiated ( with a specific exit code ) If the shutdown fails , a â\\x80\\x9chard shutdownâ\\x80\\x9d is initiated at the next polling ( with the same specific exit code )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Health Monitor, Chapter = Health Monitor module configuration) The configuration is managed in a properties file , by default the file is named health-monitor . properties . Skyminer Helath Monitor module configurationÂ parameter description required default value kairosdb . service . skyminer Indicate the entry point of the Skyminer core module Required ( and value should not change ) com . skyminer . core . healthcheck . SkyminerHealthMonitorModule skyminer . health_monitor . check_delay_milliseconds Periodic delay in milliseconds for checking system health Required N/A skyminer . health_monitor . exit_code Exit code when grace period is elapsed in unhealthy state Required N/A skyminer . health_monitor . grace_period_milliseconds Grace period to wait for an unealthy state to become healthy again before termination Required N/A',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Health Monitor, Chapter = Enabling the Health Monitor module after an upgrade) The Health Monitor may not automatically be enabled after a system upgrade . It has to be done on every node running Skyminer service . The Health Monitor module can be enabled as follows on every node running Skyminer service : Add the health-monitor . properties file in Skyminer configuration folder Restart the Skyminer service ( Starting & Stopping Skyminer alone ) to apply the new configuration',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Health Monitor, Chapter = Disabling the Health Monitor module) The Health Monitor module can be disabled by applying the following steps on every node running Skyminer service : Remove ( delete ) the configuration file called health-monitor . properties Restart the Skyminer service ( Starting & Stopping Skyminer alone ) to apply the new configuration 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Load Balancer configuration) This is an example of configuration for Skyminer high availability , command lines given in example correspond to an Oracle Linux 8 / RHEL 8 Linux distribution . Commands and configuration depend on Operating System ( Linux distributions ) and on the software version ( i . e . Keepalived and HaProxy ) in use .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Prerequisite) At least two Skyminer servers The Skyminer servers must be in the same subnet Execute all the steps as root An additional IP virtual address A temporary Internet access could be require to install additional packages mentioned below',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Architecture) The purpose is to have high available Skyminer servers . The users will be able to access any available Skyminer server from only one URL . Here a descriptive schema of what will be in place : Only one load balancer is enough to access any Skyminer server , but we highly recommend using at least two load balancers in case the master get down . Note that the load balancer servers can be Skyminer servers .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configuration) On each load balancer servers , execute the following steps to install KeepAlived and HAProxy : dnf check-update dnf update dnf install keepalived haproxy systemctl enable keepalived . service systemctl enable haproxy . service',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configuration, Paragraph = Configure firewall) On each load balancer servers , execute commands ( do not forget to replace <INTERFACE>): firewall-cmd --zone=public --add-port=82/tcp --permanent firewall-cmd --reload',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configuration, Paragraph = Configure keepalived) On the master load balancer , edit file /etc/keepalived/keepalived . conf and replace content by ( do not forget to replace <INTERFACE>, <VIRTUAL_IP>, <MASK>, <BROADCAST_IP>, <MASTER_IP>, <BACKUP_N_IP> and <BACKUP_N+1_IP>): ! Configuration File for keepalived MASTER vrrp_script chk_haproxy { script \"/usr/sbin/pidof haproxy\" interval 2 } vrrp_instance VI_1 { virtual_router_id 100 state MASTER priority 200 advert_int 1 interface <INTERFACE> authentication { auth_type PASS auth_pass secret } virtual_ipaddress { <VIRTUAL_IP>/<MASK> brd <BROADCAST_IP> scope global } unicast_src_ip <MASTER_IP> unicast_peer { <BACKUP_N_IP> <BACKUP_N+1_IP> } track_script { chk_haproxy } } On the backup load balancer , edit file /etc/keepalived/keepalived . conf and replace content by ( do not forget to replace <INTERFACE>, <VIRTUAL_IP>, <MASK>, <BROADCAST_IP>, <MASTER_IP>, <BACKUP_N_IP> and <BACKUP_N+1_IP>): ! Configuration File for keepalived BACKUP vrrp_script chk_haproxy { script \"/usr/sbin/pidof haproxy\" interval 2 } vrrp_instance VI_1 { virtual_router_id 100 state BACKUP priority 100 advert_int 1 interface <INTERFACE> authentication { auth_type PASS auth_pass secret } virtual_ipaddress { <VIRTUAL_IP>/<MASK> brd <BROADCAST_IP> scope global } unicast_src_ip <BACKUP_N_IP> unicast_peer { <MASTER_IP> <BACKUP_N_IP> <BACKUP_N+1_IP> } track_script { chk_haproxy } }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configuration, Paragraph = Configure haproxy) On each load balancers edit file /etc/haproxy/haproxy . cfg and replace the content by ( do not forget to replace <skyminer_server_name>, <skyminer_server>): global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin . sock mode 660 level admin stats timeout 30s user root group root daemon defaults log global mode http timeout connect 50000 timeout client 50000 timeout server 50000 frontend ft_http bind : 80 transparent default_backend bk_http backend bk_http balance source cookie SERVERID insert indirect nocache server <server_name_N> <server_N_IP>: 82 check observe layer7 error-limit 10 on-error mark-down cookie <server_name_N> server <server_name_N+1> <server_N+1_IP>: 82 check observe layer7 error-limit 10 on-error mark-down cookie <server_name_N+1>',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configuration, Paragraph = Configure docker-compose) On each Skyminer servers listed in /etc/haproxy/haproxy . cfg , execute these steps : Go to directory <skyminer_root_directory>/service/skyminer Stop Skyminer with command : docker-compose down Edit file <skyminer_root_directory>/service/skyminer/docker-compose . yml , go to services/nginx-proxy/ports replace 80 : 80 by 82 : 80 . Start Skyminer with command : docker-compose up -d Note This section still use the older docker-compose command , but if you have docker compose V2+, you must use docker compose command instead .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Create high availability service) On each load balancers , execute these steps : create script /root/init_skyminer_high_availability . sh with content : #!/bin/sh mkdir /run/haproxy setsebool -P haproxy_connect_any=1 systemctl start haproxy . service systemctl start keepalived . service run command : chmod +x /root/init_skyminer_high_availability . sh create file /etc/systemd/system/skyminer_high_availability . service with content : [ Unit ] Description=Skyminer high availability service . [ Service ] Type=simple ExecStart=/bin/bash /root/init_skyminer_high_availability . sh [ Install ] WantedBy=multi-user . target run command : chmod 644 /etc/systemd/system/skyminer_high_availability . service systemctl enable skyminer_high_availability . service systemctl start skyminer_high_availability . service',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Configure front-end service) Edit the file named <Skyminer installation directory>/services/skyminer/front/config/config . json . Change the value of apiUrl to the virtual IP of the load balancer . \"apiUrl\": \"http ://192 . 0 . 2 . 1\" You may also put empty value so the Web UI will connect to the same host as the UI . \"apiUrl\": \"\"',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = High availability , Chapter = Share jupyter notebooks) In order to have same Jupyter notebooks on each Skyminer instances you need to create a shared filesystem , for example you might want to use NFS - see : Sharing Resources between different instances 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = KairosDB) KairosDB is configured using the kairosdb . conf files in the conf directory . KairosDB configuration is done using HOCON ( Human-Optimized Config Object Notation ) format ( https ://github . com/lightbend/config/blob/master/HOCON . md ). Comments in HOCON start with # or // characters and are finished at the end of the line . Some parameters may be changed to tune or configure the system properly , please refer to the configuration file itself that is exhaustively commented . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Parameters) Skyminer uses KairosDB so all KairosDB configuration items are applicable ( see KairosDB ). Skyminer offers some more specific configuration parameters over KairosDB . It is configured in the skyminermodules . properties configuration file . Skyminer main configuration optionsÂ parameter description required default value kairosdb . datapoints . factory . spectrum_trace Spectrum trace factory entry point Required ( and value should not change ) com . skyminer . core . datapoints . SpectrumTraceDataPointFactory kairosdb . service . skyminer Indicate the entry point of the Skyminer core module Required ( and value should not change ) com . skyminer . core . SkyminerModule skyminer . correlations . thread_pool The default number of threads used in correlations Optional 8 skyminer . csv . max_records Limit of number of records in CSV file generation Required 1000000 skyminer . dlm . max_datapoints The maximum number of points tolerated by the DLM predictor Optional 200 skyminer . license_certificate_path Skyminer license filename Required ( and value should not change ) /opt/skyminer/license/signature skyminer . metricnames . filter . regexp Regular expression used to filter list of metrics in metricnames API - so to avoid returning internal metrics by default Required N/A skyminer . query . max_metrics_per_query Maximal number of metrics allowed into a single query ( zero = disabled ) Optional 100 skyminer . saving_folder The folder where Skyminer stores the zipped JSON copies of the pushes when using the /skyminer/savingdatapoints api . Optional /opt/skyminer/data skyminer . script_aggregator . max_batch The maximum sized stored when using the JavaScript range aggregator with array allocation . Optional 1000 Skyminer also uses OpenSearch for managing documents . It is configured in the skyminerdocumentmodules . properties configuration file . Skyminer for Opensearch configuration optionsÂ parameter description required default value kairosdb . service . skyminerdocument Skyminer document entry point Required ( and value should not change ) com . skyminer . documentmodule . SkyminerDocumentModule skyminerdocument . opensearch . network . host Opensearch cluster domain Required skyminer-Opensearch skyminerdocument . opensearch . http . port Opensearch port used by the Java client Required 9300 skyminerdocument . opensearch . index . store . type Opensearch store type Required simplefs skyminerdocument . opensearch . index . number_of_shards Opensearch shards number Required 3 skyminerdocument . opensearch . index . number_of_replicas Opensearch replicas number Required 0 skyminerdocument . opensearch . cluster Cluster name Required kairos-elastic skyminerdocument . kairosqueuemanager . runners Number of threads started to run requests Required 2 skyminerdocument . manager . elasticbatch Bulk API max number of documents by call Required 50000 skyminerdocument . max_batch_size_bytes Maximum batch size accepted by server in bytes Optional 10000000 skyminerdocument . max_document_size_bytes Maximum document size accepted by server in bytes Optional 500000 skyminerdocument . opensearch . timeout . connect Opensearch connect timeout in ms Optional 10000 skyminerdocument . opensearch . timeout . socket Opensearch socket timeout in ms Optional 30000',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Enabling authentication, Paragraph = Simple HTTP Authentication setup) There are several methods to enable and provide authentication on Skyminer . The simplest method is to enable authentication module with HTTP basic authentication . The Authentication on Skyminer services is provided as an optional module that can be activated by configuration : authentication . properties configuration file in the Skyminer conf directory First of all it is necessary to activate authentication mechanisms , this is done by activating the plugins by uncommenting the following lines : # Uncomment the following lines to enable authentication module with simple HTTP authentication kairosdb . service . auth=org . kairosdb . security . auth . core . AuthenticationManagerModule kairosdb . security . auth . authenticators . token=org . kairosdb . security . token . TokenBasedModule kairosdb . service . token=org . kairosdb . security . token . TokenBasedModule Once authentication is enabled it is possible to define static users / passwords on the configuration file . e . g . to limit authentication to Web interface : kairosdb . security . token . filter_path . landing=/ kairosdb . security . token . filter_path . index=/index . html kairosdb . security . token . filter_path . query=/query . html kairosdb . security . token . filter_path . correlations=/correlations . html e . g . to apply authentication to all ( UI and API ): kairosdb . security . token . filter_path . all=/*',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Enabling authentication, Paragraph = Add user in Simple HTTP Authentication) Use the script encodeUserPassword . sh with two arguments : User name Password This script encodes the user login information and includes it in the authentication . properties configuration file .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Enabling authentication, Paragraph = Remove user from Simple HTTP Authentication) Open the configuration file authentication . properties , delete the line starting by : kairosdb . security . token .<username>= where <username> is the name of the user to delete .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Host Features Selection) By default all Skyminer hosts would be idempotent . However it is possible to tune the system so the nodes can have reduced features set , it allows for increasing security of the system as well as separation of features . On large or critical systems it is recommended to allow users to access to query-only nodes , and data processing tools to data processing nodes . The following chapters explaing different possible host feature selection . Please refer to Web services filter for how the filtering of the Skyminer web services can be applied accordingly .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Host Features Selection, Paragraph = Storage host) A storage host typically doesnâ\\x80\\x99t run Skyminer core services , it only provides storage service ( e . g . Cassandra if cassandra datastore is used ). For such hosts , you only need storage container . For example , if you are using Cassandra , only run Cassandra container - i . e . you should remove any other container ( skyminer , nginx , grafanaâ\\x80¦) from docker-compose . yml or service orchestration if you are not using docker-compose .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Host Features Selection, Paragraph = Query host) A query host doesnâ\\x80\\x99t allow data insertion or deletion , it only provides query capabilities and user interface . For disabling data ingest , you need to enable filtering of data API .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Host Features Selection, Paragraph = Data  management host) A data management hosts only provide data processing and data management ( push/delete ). It typically doesnâ\\x80\\x99t require user interface . For removing UI and query API If data management may use the query API , just enable the file contents for filtering the Web UI . Otherwise you can enable filtering of web UI AND queries API . If Grafana container is configured , it should be removed from the docker-compose . yml configuration file .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer, Chapter = Web services filter) Web services filter configuration is defined in the configuration file named filtermodule . properties . The contents of the configuration file is commented so itâ\\x80\\x99s configuration is as self-explanative as possible . To enable the service uncomment the line containing : kairosdb . service . filter=com . skyminer . filtermodule . FilterModule Then uncomment the contents of the sections in the same file that match the intended features selection . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Checking the status of Skyminer) You can check the status by executing the following command : docker-compose ps The command output is as follows : $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------- skyminer-service <cmd> <status> <ports> skyminer-grafana <cmd> <status> <ports> skyminer-nginx <cmd> <status> <ports> The important part is the <status> field that might be Up , Down , or Restarting .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Starting & Stopping Skyminer, Chapter = Starting & Stopping Skyminer alone) On any server where you want to stop Skyminer , use the following command : docker-compose stop skyminer-service On any server where you want to stop Skyminer , use the following command : docker-compose start skyminer-service',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Starting & Stopping Skyminer, Chapter = Starting & Stopping Skyminer and all associated services) On any server where you want to start Skyminer , use the following command : docker-compose up -d On any server where you want to stop Skyminer , use the following command : docker-compose stop',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Starting & Stopping Skyminer, Chapter = Starting & Stopping Skyminer Demo Data Generator) If Skyminer Demo Data Generator was installed by the installation script , you can start it by executing the following command in the Demo Data Generator installation folder : docker-compose up -d You can stop Skyminer Demo Data Generator by executing the following command : docker stop skyminer-demo-data-generator',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Starting & Stopping Skyminer, Chapter = Starting & Stopping Cassandra on a node) Warning Skyminer requires its datastore to run when the service is active . It is not recommended to stop all Cassandra nodes ( nor many of them ) while Skyminer service is active . Note The following instructions are applicable only if Cassandra is running as part of Skyminer composition of docker services . Please refer to Cassandra user manual for other kinds of setup . On any server where you want to stop Cassandra , use the following commands : docker-compose stop skyminer-cassandra On any server you want to stop Cassandra use the following commands : docker-compose start skyminer-cassandra 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Backing up Skyminer configuration) You need to backup all directory identified as config in section List of mapped volumes : and the docker-compose . yml files . Specific instructions are provided below to backup/restore data on database storage systems .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Backing up Skyminer time series data) Backup of Skyminer time series data is documented as part of the datatore documentation . C . f . Backup & Restore of Cassandra data for backup and restore of Cassandra data .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Backing up Grafana data) Grafana setup and dashboard is saved in a PostgreSQL database . On a docker installation you can proceed to a backup of PostgreSQL database as follows ( in this example the backup file is named backup_file . sql ): docker exec -i skyminer-grafana-postgres pg_dump grafana -U grafana --clean > backup_file . sql You may restore PostgreSQL data as follows ( if the database doesnâ\\x80\\x99t exist please ensure Grafana has been executed to create it , or follow PostgreSQL manuals to create it ): docker exec -i skyminer-grafana-postgres psql -U grafana grafana < backup_file . sql Please refer to PostgreSQL documentation for detailed information .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Taking and restoring OpenSearch snapshot) The following instructions provide guidance and examples in order to backup and restore data in OpenSearch database . For more details or specific needs , please refer to the relevant official OpenSearch documentation .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Taking and restoring OpenSearch snapshot, Paragraph = Configuring snapshot environment) Note In docker compose distribution , this phase is already done since skyminer-opensearch version 1 . 3 . 1 (/usr/share/opensearch/backup is mounted in <installation_path>/skyminer/opensearch/backup ). If you are using Skyminer 2 . 7 . 3 or higher , please go directly to step 4 1 . create a shared file system First , create a directory in the docker image : docker exec -ti skyminer-opensearch bash -c \"mkdir <path_to_repo>; chmod 777 <path_to_repo>\" where <path_to_repo> is the path to the directory in the docker image usually something like /usr/share/opensearch/backup . Create a volume to link the docker directory to an accessible directory in your file system In <installation_path>/skyminer/docker-compose . yml under skyminer-opensearch : volumes add the line - ./opensearch/backup :<path_to_repo>: z 2 . Declare this folder as a backup repository in open search In <installation_path>/skyminer/docker-compose . yml under skyminer-opensearch : environment , add this option ( Already done since Skyminer 2 . 7 . 3 ) : - path . repo=<path_to_repo> 3 . reload containers cd <installation_path>/skyminer docker-compose up -d to take those changes into account . Note This section still use the older docker-compose command , but if you have docker compose V2+, you must use docker compose command instead . 4 . Register the repository Inform the opensearch rest API that the repo is available by sending an HTTP Put request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>, where <opensearch_host> is the address of Skyminer , <opensearch_port> is the opensearch port ( default 9200 ) and <http_protocol> is http or https and <repo_name> is the name you wish to use for backup repository . The request body should be : { \"type\":\"fs\", \"settings\":{ \"location\":\"<path_to_repo>\" } } The answer should be {\"acknowledged\"=true} Example with Curl : curl -X PUT -H \\'Content-Type : application/json\\' -d \\'{\"type\":\"fs\",\"settings\":{\"location\":\"<path_to_repo>\"}}\\' http ://<opensearch_host>:<opensearch_port>/_snapshot/skyminer-open-search-backup',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Taking and restoring OpenSearch snapshot, Paragraph = Taking snapshot) Once the repository is configured , send an HTTP put request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>/<snapshot_name> to create a snapshot . Example with Curl : curl -X PUT http ://<opensearch_host>:<opensearch_port>/_snapshot/skyminer-open-search-backup/snapshot_20221005 The request accept an optional body with the options : indices : The indices you want to include in the snapshot . You can use , to create a list of indices , * to specify an index pattern , and - to exclude certain indices . Donâ\\x80\\x99t put spaces between items . Default is all indices . ignore_unavailable : If an index from the indices list doesnâ\\x80\\x99t exist , whether to ignore it rather than fail the snapshot . Default is false . include_global_state : Whether to include cluster state in the snapshot . Default is true . partial : Whether to allow partial snapshots . Default is false , which fails the entire snapshot if one or more shards fails to store . An HTTP GET request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>/<snapshot_name> will return information about the snapshot including \"state\" this option will be IN_PROGRESS until the snapshot is finished : this can take a long time . \"state\" will change to : SUCCESS : The snapshot successfully stored all shards . IN_PROGRESS : The snapshot is currently running . PARTIAL : At least one shard failed to store successfully . Can only occur if you set partial to true when taking the snapshot . FAILED : The snapshot encountered an error and stored no data . INCOMPATIBLE : The snapshot is incompatible with the version of OpenSearch running on this cluster . See Conflicts and compatibility .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Taking and restoring OpenSearch snapshot, Paragraph = Restoring snapshot) To restore a snapshot simply send an HTTP POST request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>/<snapshot_name>/_restore the request body can contain the following options : indices : The indices you want to restore . You can use , to create a list of indices , * to specify an index pattern , and - to exclude certain indices . Donâ\\x80\\x99t put spaces between items . Default is all indices . ignore_unavailable : If an index from the indices list doesnâ\\x80\\x99t exist , whether to ignore it rather than fail the restore operation . Default is false . include_global_state : Whether to restore the cluster state . Default is false . include_aliases : Whether to restore aliases alongside their associated indices . Default is true . partial : Whether to allow the restoration of partial snapshots . Default is false . rename_pattern : If you want to rename indices as you restore them , use this option to specify a regular expression that matches all indices you want to restore . Use capture groups (()) to reuse portions of the index name . rename_replacement : If you want to rename indices as you restore them , use this option to specify the replacement pattern . Use $0 to include the entire matching index name , $1 to include the content of the first capture group , etc index_settings : If you want to change index settings on restore , specify them here . ignore_index_settings : Rather than explicitly specifying new settings with index_settings , you can ignore certain index settings in the snapshot and use the cluster defaults on restore N . B .: When restoring a snapshot , restored indices must not exists . You have 2 solutions to restore an existing index : Close existing index to restore : send POST request on http ://<opensearch_host>:<opensearch_port>/<index_name>/_close Delete existing index to restore : send DELETE request on http ://<opensearch_host>:<opensearch_port>/<index_name> Note that you can get all snapshot on the repository by sending a HTTP GET request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>/_all',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Backing up & Restoring, Chapter = Taking and restoring OpenSearch snapshot, Paragraph = Deleting snapshot) To delete a snapshot , you can send an HTTP DELETE request to <http_protocol>://<opensearch_host>:<opensearch_port>/_snapshot/<repo_name>/<snapshot_name> 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer) Although Skyminer is designed to be a safe and extensive data storage , it is sometimes required to cleanup data . It is therefore possible to cleanup data from a Skyminer system , several methods are available from targetted to extensive deletions . By design , no user interface feature is provided to delete data .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using the delete_metrics script, Paragraph = Prerequisite) To delete time series data from Skyminer , the following items are required : A Linux server with a recent kernel Docker and docker-compose Firewall configuration with required ports open ( see Firewall configuration ), it is not recommended to disable the firewall moreover on some operating systems like Red Hat family of linux systems ( Oracle Linux , RHEL , CentOSâ\\x80¦) disabling the firewall is providing docker with the ability of setting up its networking functions - disabling the firewall would just prevent the system from working properly . Moreover it is highly recommended having the host ( s ) synchronized to reliable time reference using NTP or similar protocol . See NTP Configuration . curl bash',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using the delete_metrics script, Paragraph = Deletion script) The delivered file is : tools delete_metrics . sh , this script can be used to delete time series data from Skyminer',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using the delete_metrics script, Paragraph = Usage of the script) Run the delete_metrics . sh script as root , or using sudo command ./delete_metrics . sh $skyminer_url $start_date $end_date $start_date and $end_date must be in the following format : YYYY/MM/DD $skyminer_url , url of skyminer without â\\x80\\x9c/â\\x80\\x9d at the end and with â\\x80\\x9chttpâ\\x80\\x9d or â\\x80\\x9chttpsâ\\x80\\x9d at start . This command will delete all metrics between $start_date 00h00m00s and $end_date 00h00m00s',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using delete_metric_data python script, Paragraph = Deletion script) The delivered file is inside skyminer-jupyter container : /root/. jupyter/ delete_metric_data . py , this script can be used to delete time series data from Skyminer',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using delete_metric_data python script, Paragraph = Usage of the script) You can either connect to the skyminer-jupyter container or access to the script from the Jupyer terminal new -> Terminal then cd /root/. jupyter . Usage : delete_metric_data . py [-h ] -u URL -s START -e END -m METRIC [-t TAG ] [-v VALUE ] [-T TAGS ] [-n ] arguments : -u URL , --url URL Url of the API -s START , --start START Absolute start time ( timestamp with millis ) -e END , --end END Absolute end time ( timestamp with millis ) -m METRIC , --metric METRIC Name of the metric optional arguments : -t TAG , --tag TAG Tagâ\\x80\\x99s name -v VALUE , --value VALUE Tagâ\\x80\\x99s value -T TAGS , --tags TAGS Set of tags key/values as json . Example : {â\\x80\\x9ctag1-keyâ\\x80\\x9d:[ â\\x80\\x9ctag1-value1â\\x80\\x9d, â\\x80\\x9dtag1-value2â\\x80\\x9d], â\\x80\\x9dtag2-keyâ\\x80\\x9d:[ â\\x80\\x9ctag2-value1â\\x80\\x9d]} -n , --notag Do not filter on tag - mandatory if you do not want to filter on tag -h , --help Show this help message and exit',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Deleting Data from Skyminer, Chapter = Using delete_metric_data python script, Paragraph = Examples) python delete_metric_data . py --url http ://192 . 168 . 0 . 1 : 80 --start 1618182000000 --end 1618268400000 --metric skyminer . demo . carrier_eirp --notag python delete_metric_data . py --url http ://192 . 168 . 0 . 1 : 80 --start 1618182000000 --end 1618268400000 --metric skyminer . demo . carrier_eirp --tag carrier --value carrier_1 python delete_metric_data . py --url http ://192 . 168 . 0 . 1 : 80 --start 1618182000000 --end 1618268400000 --metric skyminer . demo . carrier_eirp --tags {\\\\\"carrier\\\\\":[\\\\\"carrier_1\\\\\",\\\\\"carrier_100\\\\\"],\\\\\"satellite\\\\\":[\\\\\"satellite_1\\\\\"]}',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Troubleshooting) You may check if the service is up or down and the contents of the log files to get a better picture . Check that there is free disk space on the host Check that properties are appropriate ion mapped volumed ( as defined in docker-compose . yml file ) Look at the log files ( cf . Check the log files )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Troubleshooting, Chapter = Service is down) Check if the skyminer-service container is up . See Checking the status of Skyminer for details . Skyminer might be composed with other services , you may want to check if the other services containers are also active . cf . Checking the status of Skyminer and Docker .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Troubleshooting, Chapter = Check the log files) If deployed using docker , you may use the docker logs CONTAINER command as follows : e . g . see the Skyminer service logs for the last 10 minutes showing timestamps and following new entries : docker logs -f -t -since 10m skyminer-service docker logs command options : --details : Show extra details provided to logs -f or --follow : Follow log output --since : Show logs since timestamp ( e . g . 2013-01-02T13 : 23 : 37 ) or relative ( e . g . 42m for 42 minutes ) --tail : Number of lines to show from the end of the logs ( by default all ) -t or --timestamps : Show timestamps --until : Show logs before a timestamp ( e . g . 2013-01-02T13 : 23 : 37 ) or relative ( e . g . 42m for 42 minutes ) cf . containers list in Docker . please see also : https ://docs . docker . com/engine/reference/commandline/logs/',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Troubleshooting, Chapter = Fix corrupted Cassandra data) Symptom : the Cassandra log on one or more hosts shows CorruptSSTableException : ERROR [ CompactionExecutor : 7 ] 2014-03-20 12 : 00 : 07 , 454 CassandraDaemon . java ( line 192 ) Exception in thread Thread [ CompactionExecutor : 7 , 1 , main ] org . apache . cassandra . io . sstable . CorruptSSTableException : org . apache . cassandra . io . compress . CorruptBlockException : (/raid0/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-Data . db ): corruption detected , chunk at 41674041 of length 47596 . at org . apache . cassandra . io . compress . CompressedRandomAccessReader . reBuffer ( CompressedRandomAccessReader . java : 89 ) at org . apache . cassandra . io . compress . CompressedThrottledReader . reBuffer ( CompressedThrottledReader . java : 45 ) at org . apache . cassandra . io . util . RandomAccessReader . read ( RandomAccessReader . java : 355 ) at java . io . RandomAccessFile . readFully ( RandomAccessFile . java : 397 You need to login on the faulty server to fix it using a terminal and get administrative privileges . First approach is to use nodetool scrub to fix it : docker exec -i skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> scrub skyminer With <username> and <password> being the JMX credentials set during the installation of Skyminer ( by default : skyminer skyminer ). If it doesnâ\\x80\\x99t work follow this procedure : Get the list of tables ( a . k . a column families ) from the skyminer keyspace : docker exec -it skyminer-cassandra cqlsh -e \"select table_name FROM system_schema . tables WHERE keyspace_name =\\'skyminer\\';\" Connect to the container with a terminal docker exec -it skyminer-cassandra /bin/bash Switch to cassandra user : su cassandra Scrub Cassandra SSTables for all tables listed in the above step : sstablescrub skyminer <tablename> the output looks like this for each column family ( data_points , row_key_index and string_index ): Pre-scrub sstables snapshotted into snapshot pre-scrub-1395327387317 Scrubbing SSTableReader ( path=\\'/data/cassandra/data/keyspace/cf/keyspace-cf-ic-5273-Data . db\\') ... Scrubbing SSTableReader ( path=\\'/data/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-Data . db\\') WARNING : Non-fatal error reading row ( stacktrace follows ) WARNING : Row at 85207395 is unreadable ; skipping to next WARNING : Non-fatal error reading row ( stacktrace follows ) WARNING : Row at 106721044 is unreadable ; skipping to next Error scrubbing SSTableReader ( path=\\'/data/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-Data . db\\'): org . apache . cassandra . io . compress . CorruptBlockException : (/data/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-Data . db ): corruption detected , chunk at 52001433 of length 23873 . ... This command rewrites all of the other valid SSTables to new files , leaving only the corrupted ones in un-touched state and making them the only remaining of the column familyâ\\x80\\x99s directory ( all the other SSTable files got new consecutive -ic-xxxx suffixes ). Remove corrupted SSTable ( s ) Grab the prefix of your SSTable files , in this case /data/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-, and move the files to a backup folder somewhere , just in case we need them later ( we should already have a snapshot created by sstablescrub anyway ): mkdir -p /var/lib/cassandra/data/backups/corrupt-sstables mv /var/lib/cassandra/data/cassandra/data/keyspace/cf/keyspace-cf-ic-4698-* /data/backups/corrupt-sstables/ then exit the container shell using exit command . Restart Cassandra on the node and start a repair using nodetool for that purpose : docker exec -it skyminer-cassandra nodetool repair skyminer Final verification . Verify that all of the logs are clear of corruption exceptions and that things are looking normal . To clean up , you might want to remove the snapshot created by sstablescrub using docker exec -it skyminer-cassandra nodetool clearsnapshot and remove your backup files docker exec -it skyminer-cassandra rm -r /var/lib/cassandra/data/backups/corrupt-sstables 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = How to use hostnames whithout DNS on docker-compose) Ideally a DNS server should be available and able to resolve hostnames . It is possible to use Skyminer features using IP addresses and to not use hostnames . Jupyter and Grafana require proper host resolution , it might also be required by other containers .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = How to use hostnames whithout DNS on docker-compose, Chapter = Procedure for each host) However some deployments may require to use Skyminer by hostname without a DNS service in place . This is possible by using the following steps on each Skyminer host . Create a docker-compose . override . yml file ( or edit the file if it exists already ) using the format described below Place it in the <installation_directory>/services/skyminer folder ( next to the docker-compose . yml file ) Run the command docker-compose up -d from this directory in order to re-generate modified containers Note This section still use the older docker-compose command , but if you have docker compose V2+, you must use docker compose command instead .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = How to use hostnames whithout DNS on docker-compose, Chapter = Extra hosts definitions format) In the file docker-compose . override . yml , for each container ( example with jupyter and an hypothetical service named skyminer-otherservice ) add a definition of extra_hosts in the form <HOSTNAME>:<IP_ADDRESS>: version : \\'2 . 2\\' services : skyminer-jupyter : extra_hosts : - \"SERVER_1 : 198 . 51 . 100 . 1\" - \"SERVER_2 : 198 . 51 . 100 . 2\" - \"SERVER_3 : 198 . 51 . 100 . 3\" - \"SERVER4 . mydomain : 203 . 0 . 113 . 4\" skyminer-otherservice : extra_hosts : - \"SERVER_1 : 198 . 51 . 100 . 1\" - \"SERVER_12 : 198 . 51 . 100 . 2\" - \"SERVER_3 : 198 . 51 . 100 . 3\" - \"SERVER14 . mydomain : 203 . 0 . 113 . 4\" Note It is important to use the same service names as within Skyminer docker-compose . yml file ( e . g . skyminer-jupyter and skyminer-otherservice in the example above ) and the same compose version ( e . g . 2 . 2 in the example above ). 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore) Apache Cassandra is a high-performances distributed database . Thanks to its linear scaling , it allows easy administration and virtually unlimited scalability in terms of data amount and throughput . For detailed manuals and information about Cassandra operations please refer to Cassandra documentation Cassandra is a third-party application not directly supported by Kratos . Please check the third-party software chapter on Cassandra for more information .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = System Requirements for Cassandra) Cassandra runs on regular Linux OS into a docker container , and therefore has no particular strong requirements in addition to Skyminer ( see System Requirements ). However some recommendations apply : On Cassandra clusters , it is highly discouraged to setup redundant RAID volumes to store Cassandra data RAID5 ( and RAID50 ) is unsafe : must be avoided ( not only for Skyminer but for all use cases ) RAID6 and its nested levels such as RAID60 decrease performances : should be avoided RAID0 and RAID1 offer no advantage to Cassandra replication ( increase node failure rate / decrease storage and performances ): should be avoided RAID10 can be used to add another level of redundancy over Cassandra replication security and is recommended in some cases On Cassandra single node systems and HDD , it is recommended to have a RAID1/RAID10 array for redundancy of data On Cassandra single node systems and SSD , it is recommended to have RAID-1 , or RAID-6 array On Cassandra cluster and HDD , it is recommended to have RAID-10 arrays On Cassandra cluster and SSD , it is recommended to have no RAID or RAID-0 arrays Cassandra recommends using on each node at least two file systems operating on different physical volumes ( i . e . different hard disks ): one for Cassandra data , and another one for Cassandra commit log . Commitlog is most recommended to operate on an SSD ( if a choice is required ) with a recommended size of 100GB free disk space Data is recommended to operate on SSD but is more tolerant with lower-latency HDD disks',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Architecture) The default deployment of a Skyminer system includes the Skyminer and Cassandra services , collocated on the same hosts . While this default architecture provides an easy setup and a quick start , it is possible to define any other architecture because Skyminer and Cassandra are two independent services with unlimited scaling capabilities . Hence , Skyminer and Cassandra offer a lot of flexibility in system architecture that can evolve over time . Cassandra can run either on a single node or in a cluster environment . A Cassandra cluster is a set of nodes that are sharing the same databases ( Cassandra keyspaces ). All hosts of a Cassandra cluster are idempotent with the exception of the registration of nodes to a cluster , which is handled by seed nodes . A seed node will not connect to other cluster members , instead other nodes will connect to the seed nodes to join a cluster . A Cassandra cluster must have at least one â\\x80\\x9cseed nodeâ\\x80\\x9d A Cassandra cluster must not have all nodes to be seeds ( otherwise every node will be a single cluster on its own ) On a cluster that is hosting more than 3 servers it is recommended to configure at least two seed nodes ( to avoid single point of failure ) Cassandra is a distributed system with high availability , meaning that even in the case where parts of the cluster are unavailable , the system will still be able to answer the queries . This level of availability and resilience implies some data consistency checking operations ( named â\\x80\\x98repair operationsâ\\x80\\x99 by Cassandra ) to ensure that replica nodes always hold the exact same data .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Cluster Configuration) The Cassandra configuration requires each of the following parameters to be defined : CASSANDRA_CLUSTER_NAME : The name of the cluster ( by default SKYMINER ) CASSANDRA_SEEDS : The comma-separated list of seed nodes IP addresses of the cluster ; it is highly recommended that all the hosts share the same seed nodes list CASSANDRA_BROADCAST_ADDRESS : The publicly available IP address on which Cassandra operates on the current node On systems deployed using docker containers those options are located in the related docker-compose . yml file . On systems installed without docker containers those are settings in the Cassandra configuration file . Please refer to Cassandra documentation',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Cluster Configuration, Paragraph = Setting the replication factor) The replication factor is the number of copies of each data element that is stored on a cluster . The replication factor of data is therefore an important element in your design for high-availability , reliability and consistency . Typical setups are : SimpleStrategy with replication_factor set to 2 : two replica ( for 2 servers that act as a mirror ) SimpleStrategy with replication_factor set to 3 : three replica ( for a cluster on a single data center ) NetworkTopologyStrategy with different replication factors on data centers : for multiple data-centers systems The replication factor can be changed after having started the system with the following command ( use docker exec -it skyminer-cassandra followed by the command line below for docker deployments ) For example the following command will set Skyminer data replication to 3 using the SimpleStrategy cqlsh -e \"ALTER KEYSPACE Skyminer WITH REPLICATION={ \\'class\\' : \\'SimpleStrategy\\', \\'replication_factor\\' : 3 };\" Here an example to execute this command from outside the container docker exec -ti skyminer-cassandra cqlsh -e \"ALTER KEYSPACE Skyminer WITH REPLICATION={ \\'class\\' : \\'SimpleStrategy\\', \\'replication_factor\\' : 2 };\" To verify that the replication factor have been take in account , run the command bellow and look at the column replication of the keyspace_name Skyminer docker exec -ti skyminer-cassandra cqlsh -e \"SELECT * FROM system_schema . keyspaces ;\"',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Cluster Configuration, Paragraph = Modifying the hosts list for Skyminer service) Edit the kairosdb . conf configuration file In the write_cluster section , change cql_host_list value to a comma separated list of hosts and ports of actual nodes part of Cassandra cluster . For instance if cluster is 198 . 51 . 100 . 2` to ``98 . 51 . 100 . 6 change default cql_host_list : [\"skyminer-cassandra : 9042\"] by cql_host_list : [\"98 . 51 . 100 . 2 : 9042\",\"98 . 51 . 100 . 3 : 9042\",\"98 . 51 . 100 . 4 : 9042\",\"98 . 51 . 100 . 5 : 9042\",\"98 . 51 . 100 . 6 : 9042\"]) .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Cluster Configuration, Paragraph = Setting the compaction strategy) The compaction strategy should be changed to Leveled Compaction Strategy ( LCS ). A Compaction operation is a Cassandra routine operation to merge data into bigger consistent files and removing deleted data on the way . The default compaction strategy ( Size-Tiered or STCS ) requires to have 50% of the disk storage always available , using LCS will reduce the need for disk margin and make Cassandra compaction operations more frequent but faster and less I-O intensive . For example the following command will set Skyminer data points compaction to LeveledCompactionStrategy when running Cassandra in a docker container docker exec -ti skyminer-cassandra cqlsh -e \"ALTER TABLE skyminer . data_points WITH compaction = { \\'class\\' : \\'LeveledCompactionStrategy\\' };\"',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Cluster Configuration, Paragraph = Special case about 2-nodes cluster) If the 2-nodes cluster is used to mirror the data ( replication_factor=2 ) it is important to change the write consistency level . By default the write consistency level is set to QUORUM ( half the replica + one ), so in this case the quorum would write on a 2-nodes cluster and thus would not allow failover as it needs both replicas to be available at all time . For high availability and to avoid data loss : Edit the kairosdb . conf configuration file In the write_cluster section , change write_consistency_level : \"QUORUM\" to write_consistency_level : \"ONE\".',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = General Guidelines) Cluster size : A Cassandra cluster is recommended to have at least 5 nodes and a replication factor of 3 , guaranteeing the high availability and consistency of data A 2-hosts cluster with replication factor of 2 can be built to have high availability with mirroring Storage density : The servers of a Cassandra cluster should not have a high storage density It is recommended to favor more nodes ( inexpensive hardware with less resources and storage ) than a few nodes holding a large amount of data Because Skyminer is using Cassandra in a write-once read-many-times way , the recommended limit is above general Cassandra limitation : it is recommended to keep data size below 3TB ( rotating disks ) or 4TB ( SSD ) per Cassandra node when operating Cassandra with Skyminer When to add a node to a Cluster : Add an additional node to the cluster whenever any of the following cases is encountered : Cassandra average disk space per node > 2 . 5TB Used disk space > 65% ( if using Leveled Compaction Strategy ) or > 40% ( using Size-Tiered Compaction strategy ) on some nodes',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Consistency checking) Cassandra consistency checking on a cluster system is done via what are called repair operations . The routine maintenance operation on Cassandra is the scheduling of a repair . It is recommended for Skyminer to schedule such a repair once a month on each server , for example as a cron task , using the following command ( if the system is deployed using docker ): docker exec skyminer-cassandra range_repair . py -H localhost -k skyminer -w 8 -v -d --output-status=/tmp/repair_status . json It is very important to run this command on one node at a time ( versus running it on several nodes simultaneously ) in order to avoid overloading the cluster .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to improve performances) Performance fine tuning of Cassandra system for Skyminer can be done but is not recommended unless absolutely necessary . Please check with Kratos for the level of expected performances . The easiest way to improve performances with Cassandra is to add nodes , which will improve query performance and writing throughput .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to add a new node to a cluster) To add a new node to the cluster : Prepare a server according to the pre-requisites ( cf . System Requirements for Cassandra ) Follow the same installation instructions for the other nodes Once the service is started the new Cassandra node automatically joins the cluster',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to remove a node from a cluster, Paragraph = Removing a live node) To remove a live node from the cluster : Run nodetool decommission from the node itself ( login as root ) docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> decommission After decommission is finished , shut-down and disconnect the node Check that the node is not on the cluster anymore ( from any other node as root ) docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> status',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to remove a node from a cluster, Paragraph = Removing a dead node) To remove a failed or dead node from the cluster : Ensure that the node is shut-down and disconnected , login to any other node in the cluster ( as root ) Identify ID of the node to remove . ( â\\x80\\x9cHost IDâ\\x80\\x9d, for example d0844a21-3698-4883-ab66-9e2fd5150edd ) docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> status From any other node in the cluster ( as root ): remove the node docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> removenode <Host ID> Check the removal status until thereâ\\x80\\x99s no more token removal in progress docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> removenode Check that the node is not on the cluster anymore docker exec â\\x80\\x93it skyminer-cassandra nodetool -Dcom . sun . jndi . rmiURLParsing=legacy -u <username> -pw <password> status',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to erase all data, Paragraph = Erasing All Cassandra data) Warning The method described below will erase all data from a Cassandra database system . This deletion is immediate and irreversible . Moreover , if the Cassandra cluster is used for other data than Skyminer this method will also delete all other databases ( a . k . a . keyspaces ). If you just want to erase some data from Skyminer please see Deleting Data from Skyminer Stop the Skyminer system ( for a cluster stop all Skyminer instances ) - see Starting & Stopping Skyminer Stop Cassandra system if it runs on a different hosts than Skyminer - see Starting & Stopping Cassandra on a node Delete all the contents from the cassandra data and commitlog folders Restart Cassandra and Skyminer - see Starting & Stopping Skyminer',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = How to erase all data, Paragraph = Erasing all Skyminer data (but not other Cassandra data)) Warning The method described below will erase all Skyminer data from a Cassandra database system . This deletion is immediate and irreversible . If you just want to erase some data from Skyminer please see Deleting Data from Skyminer Note The commands below are provided for a Cassandra system delivered with Skyminer and running on docker . The docker exec -it skyminer-cassandra is telling docker to execute a command on the skyminer-cassandra container . If the Cassandra container is named differently , you have to replace skyminer-cassandra by the actual container name or ID . If Cassandra is not running in Container , you have to remove the docker exec -it skyminer-cassandra part . Stop the Skyminer system ( for a cluster stop all Skyminer instances ) - see Starting & Stopping Skyminer alone From any of the hosts execute the following command as root : docker exec -it skyminer-cassandra cqlsh -e \"DROP KEYSPACE <SKYMINER_KEYSPACE>;\" <SKYMINER_KEYSPACE> is the name of Skyminer keyspace on Cassandra . Usually the name is skyminer . Check that the data has been deleted by running the following command : docker exec -it skyminer-cassandra cqlsh -e \"DESCRIBE KEYSPACES ;\" The Skyminer keyspace ( usually skyminer ) should not appear . Restart Skyminer service - see Starting & Stopping Skyminer alone Warning When a keyspace is dropped by Cassandra , a snapshot of the data is automatically made and would be deleted later by Cassandra . If you need to ensure that the data is removed within a short time you need to delete the snapshots . This can be done by running the command docker exec -it skyminer-cassandra nodetool clearsnapshot on each and every node of the cluster .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Backup & Restore of Cassandra data, Paragraph = Backup) Run snapshots : docker exec skyminer-cassandra nodetool -h localhost -u <username> -pw <password> -p 7199 snapshot skyminer With <username> and <password> being the JMX credentials set during the installation of Skyminer ( by default : skyminer skyminer ). The snapshot is created in the <data_directory_location>/keyspace_name/<table_name>-<UUID>/snapshots/snapshot_name directory . Each snapshot directory contains numerous . db files that contain the data at the time of the snapshot . Once the backup is done , remove the existing snapshots : docker exec skyminer-cassandra nodetool -h localhost -u <username> -pw <password> -p 7199 clearsnapshot Fore more information please refer to Cassandra documentation .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Cassandra datastore, Chapter = Backup & Restore of Cassandra data, Paragraph = Restore) Note In the case where you are restoring the snapshot because some data has been accidentally deleted , it is necessary to empty Cassandra of all data before restoring the snapshot . For instance dropping the schema can be done by erasing all the contents of the Cassandra data and commitlog directories and restarting the services to recreate a new schema . To truncate specific table data please refer to the Cassandra administration manual . To restore data from snapshots , copy the most recent snapshot SSTable directories to the <data_directory>/<keyspace_name>/<table_name>-<UUID> directory (<data_directory> is the path or volume whete the Cassandra data is stored ). It is highly recommended that the snapshot comes from the same host as the one it is restored onto . Then run the nodetool refresh utility : docker exec skyminer-cassandra nodetool -u <username> -pw <password> refresh <keyspace_name> <table_name> Fore more information please refer to Cassandra documentation . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Indexed Cassandra datastore) Cassandra can show low performances on high tag cardinality of data . The indexed Cassandra datastore uses the Cassandra datastore and leverages on the Skyminer Time Series Indexer Module module to improve query performances on such data sets . All documentation related to Cassandra datastore is applicable .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Indexed Cassandra datastore, Chapter = Requirements) Indexed Cassandra relies on time series indexer to query data more efficientlyâ\\x80¦ Therefore it requires the Skyminer system to operate the time series indexer . See Skyminer Time Series Indexer Module .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Indexed Cassandra datastore, Chapter = Activating the datastore) In Skyminer or KairosDB configuration files ( typically kairosdb . conf ), set the service . datastore property to org . kairosdb . datastore . cassandra . IndexedCassandraModule',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Indexed Cassandra datastore, Chapter = Deactivating the datastore) In Skyminer or KairosDB configuration files ( typically kairosdb . conf ), set the service . datastore property to org . kairosdb . datastore . cassandra . CassandraModule 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prometheus read-only datastore (Experimental)) Prometheus is a popular and very efficient metrics monitoring platform . Thanks to its simple design ( each instance owns its own data ), it allows easy monitoring of systems . An instance of Skyminer can act as a proxy for Prometheus data , so to use Skyminer features transparently on prometheus data . Skyminer will not send data to Prometheus ( Prometheus is not design to support Skyminer features ; for example : configurable data types , rewrites , clever deletes , back-filling and out of order insertion ) , thus this datastore is read-only . Please note that this is an experimental feature .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prometheus read-only datastore (Experimental), Chapter = Architecture) One Skyminer front-end communicates with one prometheus node . Please note that Prometheus has some limitations of number of samples that can be retrieved per time series . There are several configurable modes for querying data from prometheus : Adaptive queries ( ADAPTIVE ) : Recommended , will use INSTANT_RAW_DATA strategy whenever possible , and RANGE_BY_STEPS strategy for query with limits of greater than configured maximal duration Range queries ( RANGE_BY_STEPS ): Faster , with data aligned in time , but returns data that has never been recorded . Ideal for low-granularity fast processing . It consistently uses Prometheus range queries ( https ://prometheus . io/docs/prometheus/latest/querying/api/#range-queries ) Instant queries ( INSTANT_RAW_DATA ) : Returns the ( true ) data that has been recorded , runs slower than range queries . It consistently uses Prometheus instant queries ( https ://prometheus . io/docs/prometheus/latest/querying/api/#instant-queries ) The ADAPTIVE mode works as follows : Default is to use instant queries ( retrieving real data samples ) A query limit set to a number greater than one will use Prometheus range queries ( limiting number of time steps to a number lower than a minute but using whole time units such as 1s , 1m , 10m , 1h , â\\x80¦) Any duration longer than maximum number of days for instant queries will also switch to range queries . There are also several configurable modes for processing prometheus results : Fast streaming parser ( FAST_STREAMING_PARSER ): Recommended mode . much faster and without memory bottleneck , data from prometheus is transformed in Skyminer results in a streaming model , however it relies on the relative position of the fields on the prometheus results ( with no guarantee that it will not change ) Memory allocated parser ( MEMORY_ALLOCATED_OBJECT_PARSER ) : uses more of memory and is slower to perform queries . Can be used if the other mode has issues . Limitations of the connector : The difference in concept between Prometheus and regular time series database imposes unintuitive strategies Query limit is not supported by prometheus , only a limit of 1 has a correct reliable behaviour ( instant query from prometheus ) but only in DESC order As a consequence fetch previous sample capability will only work reliably using a number of 1 sample , but will only work if the sample is not too old As another consequence trying to get the first sample in the lifetime of the system is not possible accurately A limit above one using INSTANT_RAW_DATA is not recommended because it will use an arbitrary duration instead A limit using ADAPTIVE mode will trigger a range query ( except limit of 1 in DESC order ) Prometheus is not robust again long trending queries requiring all data ( accurate results ). But is incredibly efficient at doing range queries on long term data ( inaccurate results ). Note on Range queries : Range queries ( return processed + resampled data ) are very different from Skyminer approach on data ( return raw data and allow custom processing ), but are can be interesting Range queries will return a number of steps smaller than maximal number but maximized in the time span provided ( using convenient whole units such as 1s , 1m , 10m , 1hâ\\x80¦etc ) Setting the limit can increases / decrease the number of steps requested , however the number will be maximized to the configured maximal number of steps',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prometheus read-only datastore (Experimental), Chapter = Configuration, Paragraph = Enabling prometheus datastore) Uncomment ( by removing the leading #) the datastore module line as follows : kairosdb . service . datastore=com . skyminer . core . datastore . prometheus . PrometheusDatastoreModule',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Prometheus read-only datastore (Experimental), Chapter = Configuration, Paragraph = Configuration parameters) skyminer . datastore . prometheus . url : URL of prometheus Server webroot , for example https ://demo . promlabs . com skyminer . datastore . prometheus . allow_insecure : Allow insecure connections or self-signed certificates for HTTPS , must be set to true or false skyminer . datastore . prometheus . retrieval_mode : Mode of retrieval , must be set to ADAPTIVE ( recommended ),``INSTANT_RAW_DATA`` or RANGE_BY_STEPS . C . f . details above . skyminer . datastore . prometheus . parser : Parser used to convert prometheus data results , must be set to FAST_STREAMING_PARSER or MEMORY_ALLOCATED_OBJECT_PARSER . C . f . details above . skyminer . datastore . prometheus . query_timeout : Query timeout used for Prometheus ( in seconds ) skyminer . datastore . prometheus . default_number_of_steps : Default maximal number of steps requested from Prometheus , it can be changed using the limit option . Used only for RANGE_BY_STEPS mode skyminer . datastore . prometheus . max_number_of_steps : Absolute maximal number of steps requested from Prometheus . Used only for RANGE_BY_STEPS mode . skyminer . datastore . prometheus . query_timeout : Query timeout used for Prometheus ( as a Prometheus duration string , e . g . â\\x80\\x9c5mâ\\x80\\x9d, â\\x80\\x9d60sâ\\x80\\x9dâ\\x80¦) skyminer . datastore . prometheus . max_instant_query_duration_days : Maximum time span for instant queries towards prometheus in days 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = RocksDB local storage datastore (Experimental)) RocksDb is a key-value storage system that supports high data throughput and efficient query times . It provides a local filesystem storage . Each instance of Skyminer that uses RocksDB will have a different datastore , so this will not give a distributed data storage . However some mechanisms can be applied ( e . g . aggregation of servers ) to imagine distributed systems using RocksDB . Please note that this is an experimental feature .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = RocksDB local storage datastore (Experimental), Chapter = Configuration, Paragraph = Enabling rocksdb datastore) Uncomment ( by removing the leading #) the datastore module line as follows : kairosdb . service . datastore=com . skyminer . core . datastore . rocksdb . RocksDbDatastoreModule',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = RocksDB local storage datastore (Experimental), Chapter = Configuration, Paragraph = Configuration parameters) datastore . rocksdb . path : Local filesystem path for rocksdb data datastore . rocksdb . flush_strategy : Allow secure data persistence ( SAFE_AND_SLOW ) or delayed flush ( DELAYED_AND_FAST ). A short flush delay ( e . g . 100ms ) may multiply insertion performances by an order of magnitude . datastore . rocksdb . max_background_jobs : Maximal number of background jobs operated by RocksDB . datastore . rocksdb . max_flush_delay_ms : Maximal delay between data flushes in milliseconds . Used for DELAYED_AND_FAST flush strategy . It is recommended to keep this value low ( e . g . 50 or 100ms ). 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Throwaway (no storage) datastore) This datastore is not saving any data . It can be used for testing performances or behaviour of Skyminer data points push interfaces and/or of the network under stress . It provides a single metric with a counter of datapoints received over the past hour .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Throwaway (no storage) datastore, Chapter = Configuration) There is no configuration possible other than enabling this datastore . To use it create a properties file in skyminer configuration directory with the following contents : kairosdb . service . datastore=com . skyminer . core . datastore . throwaway . ThrowawayDatastoreModule 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = REST API) Overview OpenAPI Add Data Points Aggregators Correlations API Delete Data Points Delete Metric Event sourcing Features Grouping Health Checks List Metric Names Metadata Query CSV Query Metric Tags Query Metrics Skyminer Time Series Indexer Module API Skyminer Version Version Note Please refer to Optional Modules for optional modules API 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Overview) This API provides operations to list existing metric names , list tag names and values , store metric data points , and query for metric data points . Data points have a metric name , a value , a timestamp , and a list of one or more tags . Tags are named properties that identify the data , such as its type and where it comes from . Metric names , tag names and values are case sensitive . If a data point names a metric that does not exist , the metric is created . The timestamp is the number of milliseconds since January 1st , 1970 UTC . You can query for data points by specifying their metric name and a time range , and optionally one or more tags . A query can perform data manipulation operations such as aggregation , averaging , min and max calculations , and downsampling . All posts and responses are in JSON format including error messages .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Overview, Chapter = What Data do I Store?) Donâ\\x80\\x99t store rate calculations or percentages . Send the actual numbers . Sending rates and percentages limit the amount of useful information that could otherwise be obtained from the raw values . If there is too much data to store as raw numbers , one approach at aggregation is to send a count of the data in buckets . For example , if you want to track the amount of time ( in seconds ) a request took , you could create 5 buckets in 10 second increments . Each time a request is performed , you add one to the bucket for that time interval . The metric values you then post are the counts for each bucket .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Overview, Chapter = API online documentation) The API documentation is also available in the query user interface : OpenAPi specifications Features ( aggregators , predictors , â\\x80¦) documentation Links can be found in the About section of the Query page : 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = OpenAPI) POST /skyminer/correlations/searchÂ Submit correlation query Compute the statistical relationship between a reference time series and multiple time series . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/correlations/matrixÂ Submit correlation matrix query Compute the statistical relationship between multiple time series two by two . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /v1/datapointsÂ Submit data points You can either use â\\x80\\x9ctimestampâ\\x80\\x9d with â\\x80\\x9cvalueâ\\x80\\x9d for a single data point or you can use â\\x80\\x9cdatapointsâ\\x80\\x9d to post multiple data points . Status Codes : 204 No Content â\\x80\\x93 Success 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /v1/datapoints/deleteÂ Delete data points Delete will perform the query specified in the body and delete all data points returned by the query . Aggregators and groupers have no effect on which data points are deleted . Delete is designed such that you could perform a query , verify that the data points returned are correct , and issue the delete with that query . Note : Delete works for the Cassandra and H2 data store only . Status Codes : 204 No Content â\\x80\\x93 Success 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/featuresÂ List metadata of all features Returns metadata about various components of Skyminer ( e . g . aggregators and group by operations ). Status Codes : 200 OK â\\x80\\x93 Success GET /v1/features/{feature}Â List metadata of one feature Returns metadata of a specified feature . Parameters : feature ( string ) â\\x80\\x93 Name of the feature Status Codes : 200 OK â\\x80\\x93 Success 404 Not Found â\\x80\\x93 Not found GET /v1/health/statusÂ Get status of health checks Two health checks are executed : the JVM thread deadlock check verifies that no deadlocks exist in the Skyminer JVM , and the datastore query check performs a query on the data store to ensure that the data store is responding . Status Codes : 200 OK â\\x80\\x93 Success GET /v1/health/checkÂ Check the health of the system Two health checks are executed : the JVM thread deadlock check verifies that no deadlocks exist in the Skyminer JVM , and the datastore query check performs a query on the data store to ensure that the data store is responding . Status Codes : 204 No Content â\\x80\\x93 Success . The code for success can be changed by editing the kairosdb . health . healthyResponseCode property . 500 Internal Server Error â\\x80\\x93 Failure . At least one check is unhealthy . GET /v1/metadata/{service}Â List service keys Returns all service keys for a given service , or an empty list if none exist . Parameters : service ( string ) â\\x80\\x93 Name of the service Status Codes : 200 OK â\\x80\\x93 Success 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/metadata/{service}/{serviceKey}Â List keys Returns all keys for a given service and service key , or an empty list if none exist . Parameters : service ( string ) â\\x80\\x93 Name of the service serviceKey ( string ) â\\x80\\x93 Name of the service key Status Codes : 200 OK â\\x80\\x93 Success 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/metadata/{service}/{serviceKey}/{key}Â Get metadata value Returns the metadata value of a given service . Parameters : service ( string ) â\\x80\\x93 Name of the service serviceKey ( string ) â\\x80\\x93 Name of the service key key ( string ) â\\x80\\x93 Name of the key Status Codes : 200 OK â\\x80\\x93 Success 500 Internal Server Error â\\x80\\x93 Internal server error POST /v1/metadata/{service}/{serviceKey}/{key}Â Add value to metadata Returns metadata of a specified feature . Parameters : service ( string ) â\\x80\\x93 Name of the service serviceKey ( string ) â\\x80\\x93 Name of the service key key ( string ) â\\x80\\x93 Name of the key Status Codes : 204 No Content â\\x80\\x93 Success 500 Internal Server Error â\\x80\\x93 Internal server error DELETE /v1/metadata/{service}/{serviceKey}/{key}Â Delete key Deletes the specified key . Parameters : service ( string ) â\\x80\\x93 Name of the service serviceKey ( string ) â\\x80\\x93 Name of the service key key ( string ) â\\x80\\x93 Name of the key Status Codes : 204 No Content â\\x80\\x93 Success 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/metricnamesÂ List metric names Returns a list of metric names filtered to exclude internal metrics . Query Parameters : prefix ( string ) â\\x80\\x93 If specified , only the metric names that start with the prefix are returned . Status Codes : 200 OK â\\x80\\x93 Success GET /v1/extended_metric_namesÂ List metric names Returns a list of all metric names including internal metrics . Query Parameters : prefix ( string ) â\\x80\\x93 If specified , only the metric names that start with the prefix are returned . Status Codes : 200 OK â\\x80\\x93 Success DELETE /v1/metric/{metric_name}Â Delete metric Deletes a metric and all data points associated with the metric . Note : Delete works for the Cassandra and H2 data stores only . Parameters : metric_name ( string ) â\\x80\\x93 Name of the metric to delete Status Codes : 204 No Content â\\x80\\x93 Success 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/datapoints/queryÂ Query metrics Returns a list of metric values based on a set of criteria . Also returns a set of all tag names and values that are found across the data points . Queries can be done using either a GET or POST method . The GET version requires the JSON to be passed as a parameter Query Parameters : query ( object ) â\\x80\\x93 Encoded as JSON ( Required ) Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /v1/datapoints/queryÂ Query metrics Returns a list of metric values based on a set of criteria . Also returns a set of all tag names and values that are found across the data points . Queries can be done using either a GET or POST method . The POST version takes the JSON query in the body of the request . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /v1/datapoints/query/tagsÂ Get the tags of a query result Similar to a query but only returns the tags ( no data points returned ). This can potentially return more tags than a query because it is optimized for speed and does not query all rows to narrow down the time range . This queries only the Row Key Index and thus the time range is the starting time range . When using Cassandra datastore , since the Cassandra row is set to 3 weeks , this can return tags for up to a 3 week period . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/query_csv/one_row_per_timestampÂ Get the query result in CSV format Each row contains the values of all the series for a specific timestamp . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/query_csv/one_row_per_value_detailedÂ Get the query result in CSV format Each row contains the values of all the series for a specific timestamp . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/query_csv/one_row_per_valueÂ Get the query result in CSV format Each row contains one value of one of the series . Status Codes : 200 OK â\\x80\\x93 OK 400 Bad Request â\\x80\\x93 Bad request 500 Internal Server Error â\\x80\\x93 Internal server error GET /v1/tagnamesÂ List tag names Returns a list of all tag names . Status Codes : 200 OK â\\x80\\x93 Success GET /v1/tagvaluesÂ List tag values Returns a list of all tag values . Status Codes : 200 OK â\\x80\\x93 Success GET /v1/versionÂ Get KairosDB version Returns the current version of KairosDB Status Codes : 200 OK â\\x80\\x93 OK GET /skyminer/versionÂ Get Skyminer version Returns the current version of Skyminer Status Codes : 200 OK â\\x80\\x93 OK . When unable to fetch the build version , the value â\\x80\\x98unknownâ\\x80\\x99 is returned . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Request) http ://[ host ]:[ port ]/api/v1/datapoints Note : you can gzip the json and upload with the content type set to application/gzip if you are batching large amounts of data .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Body) [ { \"name\": \"archive_file_tracked\", \"datapoints\": [[ 1359788400000 , 123 ], [ 1359788300000 , 13 . 2 ], [ 1359788410000 , 23 . 1 ]], \"tags\": { \"host\": \"server1\", \"data_center\": \"DC1\" }, \"ttl\": 300 }, { \"name\": \"impedance\", \"type\": \"complex-number\", \"datapoints\": [ [ 1359788400000 , { \"real\": 2 . 3 , \"imaginary\": 3 . 4 } ], [ 1359788300000 , { \"real\": 1 . 1 , \"imaginary\": 5 } ] ], \"tags\": { \"host\": \"server1\", \"data_center\": \"DC1\" } }, { \"name\": \"archive_file_search\", \"timestamp\": 1359786400000 , \"value\": 321 , \"tags\": { \"host\": \"server2\" } } ]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Description) You can either use â\\x80\\x9ctimestampâ\\x80\\x9d with â\\x80\\x9cvalueâ\\x80\\x9d for a single data point or you can use â\\x80\\x9cdatapointsâ\\x80\\x9d to post multiple data points . This example shows both approaches . name Metric names must be unique . Multiple words in a metric name are typically separated using an underscore ( â\\x80\\x9c_â\\x80\\x9d) to separate words such as archive_search . timestamp The timestamp is the date and time when the data was measured . Itâ\\x80\\x99s a numeric value that is the number of milliseconds since January 1st , 1970 UTC . value The value is a number ( i . e , 523 or 132 . 45 ). datapoints An array of data points . Each data point consists of a timestamp and value . tags The tags field is a list of named properties . At least one tag is required . The tags are used when querying metrics to narrow down the search . For example , if multiple metrics are measured on server1 , you could add the â\\x80\\x9chostâ\\x80\\x9d: â\\x80\\x9dserver1â\\x80\\x9d tag to each of the metrics and queries could return all metrics for the â\\x80\\x9chostâ\\x80\\x9d tagged with the value of â\\x80\\x9cserver1â\\x80\\x9d. type Type identifies data types . This field is only needed if the data value is something other than a number , however even for numbers it is recommended to specify whether the type is long or double . The type field is the name of the registered type for the custom data . Default available types are long , double , string and complex . For custom type see the section types : Special Types ttl Sets the Cassandra ttl for the data points . In the example above the data points for the metric archive_file_tracked will have the ttl set for 5 min . Leaving the ttl off or setting it to 0 will use the default TTL value specified in the settings .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Response) SuccessThe response will be 204 NO CONTENT with no body . Failure Response The response will be 400 Bad Request if the request is invalid . The response will be 500 Internal Server Error if an error occurs . { \"errors\": [ \"Connect to 10 . 92 . 4 . 1 : 4242 timed out\" ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Special Types, Paragraph = spectrum_trace) The spectrum_trace is a custom type provided by Skyminer which provides support for RF signal spectrum trace . When the spectrum_trace type is used , the value field is no longer a number but an object with the fields : start_frequency ( float ) stop_frequency ( float ) max_val ( int ) min_val ( int ) res_bw ( float ) ref_level ( int ) scale ( int ) trace ( string ) Spectrum trace data is a set of single byte integers encoded in hexadecimal . Each value represents the encoding of the best matching index between the 256 possibilities between min_val and max_val values . Example : [ { \"name\": \"monics . trace\", \"timestamp\": 1359786400000 , \"type\": \"spectrum_trace\", \"value\": { \"start_frequency\": 11699504305 . 9285 , \"stop_frequency\": 11700129094 . 1006 , \"max_val\": -10173 , \"min_val\": -10302 , \"res_bw\": 48943 . 01471 , \"video_bw\": 150 . 131947 , \"ref_level\": -60 , \"scale\": 10 , \"trace\": \"292B2D2D2F3133353537393B ... 5312F2D292723211F1B1915130F0B09050300\" }, \"tags\": { \"host\": \"test\" } }]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Add Data Points, Chapter = Special Types, Paragraph = constellation_diagram) The constellation_diagram is a custom type provided by Skyminer which provides support for RF signal constellation diagram . When the constellation_diagram type is used , the value field is no longer a number but an object with the fields : sweep_time_seconds ( float ) num_pts_constellation ( integer ) constellation_x ( array of integers ) constellation_y ( array of integers ) constellation_x and constellation_y data is normalized to a single byte integer value , between -128 to 127 . The contents of constellation x and y contains constellation diagram data in even indices , and eye pattern diagram data in odd indices . Example : [ { \"name\": \"constellation . diagram\", \"timestamp\": 1359786400000 , \"type\": \"constellation_diagram\", \"value\": { \"sweep_time_seconds\": 0 . 002 , \"num_pts_constellation\": 4 , \"constellation_x\": [ 24 , -4 , -53 , -64 ], \"constellation_y\": [ 12 , -5 , 14 , -6 ] }, \"tags\": { \"host\": \"test\" } }] 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators) Note The list of aggregators is incomplete and depends on enabled modules . This documentation will be generated automatically in the future using the features API .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Aggregator Parameters, Paragraph = Unit) unit Unit is a time unit represented as a string and must be one of ( MILLISECONDS , SECONDS , MINUTES , HOURS , DAYS , WEEKS , MONTHS , YEARS )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Aggregator Parameters, Paragraph = Sampling) sampling A sampling is a json object containing two values : a value and a unit . The value is of type long . For the unit , see Unit .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Range Aggregator) Many of the below aggregators inherit from the range aggregator . You can set the following parameters on any range aggregator . sampling ( Sampling {value ( long ), unit ( See Unit )}) Sampling is the length of the interval on which to aggregate data . \"aggregators\": [ { \"name\": \"sum\", \"align_sampling\": true , \"align_start_time\": true , \"align_end_time\": false , \"sampling\": { \"value\": 1 , \"unit\": \"minutes\" } }] align_start_time - ( boolean , optional , default value : false ) When set to true , the time for the aggregated data point for each range will fall on the start of the range instead of being the value for the first data point within that range . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one is set , unexpected results will occur . align_end_time - ( boolean , optional , default value : false ) Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The difference between align_start_time and align_end_time is that the latter sets the timestamp for the datapoint to the beginning of the following period , instead of setting it to the beginning of the current period . As with align_start_time , setting this to true will cause your data to take the same shape when graphed as you refresh the data . Note that align_start_time and align_end_time are mutually exclusive . If more than one is set , unexpected results will occur . align_sampling - ( boolean , optional , default value : false ) Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The effect of setting this to true is that your data will take the same shape when graphed as you refresh the data . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one is set , unexpected results will occur . start_time - ( long , optional , default value : 0 ) Start time to calculate the ranges from . Typically this is the start of the query . time_zone - ( long time zone format ) Time zone to use when doing time based calculations .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Histogram) histogram Calculates a probability distribution and returns the specified percentile for the distribution . The â\\x80\\x9cpercentileâ\\x80\\x9d value is defined as 0 < percentile <= 1 where . 5 is 50% and 1 is 100%. Note that this aggregator has been renamed to percentile in release 0 . 9 . 2 . See Percentile .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Least Squares) least_squares Returns two points for the range which represent the best fit line through the set of points . Extends Range Aggregator .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Percentile) percentile Finds the percentile of the data range . Calculates a probability distribution and returns the specified percentile for the distribution . The â\\x80\\x9cpercentileâ\\x80\\x9d value is defined as 0 < percentile <= 1 where . 5 is 50% and 1 is 100%. Extends Range Aggregator . Parameters : percentile ( double ) - Percentile to count .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Divide) div Returns each data point divided by a divisor . Requires a â\\x80\\x9cdivisorâ\\x80\\x9d property which is the value that all data points will be divided by . Parameters : divisor ( double ) - Value to divide the data points by .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Rate) rate Returns the rate of change between a pair of data points . Requires a â\\x80\\x9cunitâ\\x80\\x9d property which is the sampling duration ( i . e . rate in seconds , milliseconds , minutes , etcâ\\x80¦). Parameters : sampling ( See Sampling ) - Sets the sampling used to calculate the rate . unit ( See Unit ) - Shortcut for setting the sampling to a single unit . If you set the unit to SECONDS then the sampling is rated over one second . time_zone ( Long format time zone ) - Time zone used for time calculations .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Sampler) sampler Computes the sampling rate of change for the data points . Requires a â\\x80\\x9cunitâ\\x80\\x9d property which is the sampling duration ( i . e . rate in seconds , milliseconds , minutes , etcâ\\x80¦). Parameters : unit ( See Unit ) - Sets the sampling unit . If you set the unit to SECONDS then the sampling is rated over one second . time_zone ( Long format time zone ) - Time zone used for time calculations .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Scale) scale Scales each data point by a factor . Requires a â\\x80\\x9cfactorâ\\x80\\x9d property which is the scaling value . Parameters : factor ( double ) - Scale factor .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Trim) trim Trims off the first , last or both data points for the interval . Useful in conjunction with the save_as aggregator to remove partial intervals . Parameters : trim ( FIRST , LAST , BOTH ) - Trims either the first , last or both end data points .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Save As) save_as Saves the result to another metric . Any data point with a unique tag value will also have that tag set . So if a data point is returned with tags {\"dc\":[\"DC1\"],\"host\":[\"hostA\", \"hostB\"]} only the dc tag will be set when saved . If you do a group by query the group by tags are saved . Parameters : metric_name ( string ) - Metric name to save the results to . tags ( Map of key values ) - Additional tags to set on the metrics {\"tag1\":\"value1\",\"tag2\":\"value2\"}. ttl ( integer ) - Sets the ttl on the newly saved metrics . add_saved_from ( boolean ) - Tells the aggregator to add the saved_from tag to the new metric . Defaults to true .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = Filter) filter Filters out data points matching the given critera . Parameters : filter_op ( LTE , LT , GTE , GT , EQUAL ) - Defines what data points to filter in relation to the threshold . threshold ( double ) - Sets the threshold value used to filter the data points .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Aggregators, Chapter = JS Aggregator) js_function js_filter js_range The module requires Java 8 and provides a way to pass javascript code as the aggregator . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Correlations Search API) The web client allows the user to build Json queries to Skyminer . This section gives some further information on the parameter within the Json query when the Search mode is activated .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Correlations Search API, Paragraph = Query) Correlation search queries have five categories of parameters : reference_series : it is the query for the series that should be searched . It should be a complete Skyminer query metric with only one metric and that generate only one series ( no grouping without vertical aggregation ). searched_series : it is the query for the set of series that should be compared to the reference series . It should be a complete Skyminer query metric , possibly with many metrics and tag group bys . similarity_measure : similarity measure is the method used to measure similarity between two series . Discarding linear correlation is the classical Pearson correlation . DTW is the dynamic time warping distance measure . A similarity measure goes with a reliability threshold ( reliability_threshold ) that indicates the minimum number of samples used in the correlation that are necessary . If it is under the threshold , the correlation score is set to 0 . The data would typically be under this threshold if one of the series had only a few samples or if both series had unsynchronized sampling . You can find details on similarity measures and their parameters in Similarity Measures . result_format : it is the only optional parameter of the correlation query . It has several parameters : threshold : only returns correlated series that have correlation scores above this threshold . Usually a double between 0 and 1 . best : integer n indicating that only the best n series should be returned . computation_method : implemented are sequential and threaded . Threads uses every available cores on the server . The following Json shows the structure of the query : { \"reference_series\": {â\\x80¦ Skyminer query â\\x80¦}, \"searched_series\": {â\\x80¦ Skyminer query â\\x80¦}, \"similarity_measure\": { â\\x80¦ Similarity measure parameters (\"threshold\", â\\x80¦) â\\x80¦}, \"result_format\": { â\\x80¦ Result format parameters (\"best\",\"threshold\", â\\x80¦) â\\x80¦}, \"computation_method\": { â\\x80¦ Computation method parameters (\"name\", â\\x80¦) â\\x80¦ } }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Correlations Search API, Paragraph = Result) The result is divided into two parts : query : the query that actually generated this response . results : the actual response to the query . The â\\x80\\x9cresultsâ\\x80\\x9d sub-object has four parameters : reference_series : the description of the reference series , in the same format as in usual Skyminer responses . correlated_series : a table with p elements describing the p series that have been correlated to the reference series . correlation_score : a table with p numbers ( usually between 0 and 1 ) representing the correlation scores of the p correlated series . correlation_reliability : a table with p numbers representing an indicator of the reliability of the correlation scores of the p correlated series . Here is an example response : { \"query\": { â\\x80¦ A correlation query as presented before â\\x80¦ }, \"results\": { \"reference_series\": { \"name\": \"kairosdb . protocol . http_request_count\", \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"metricnames\",\"query\",\"search\",\"tags\"] }, \"query_index\": 0 }, \"correlated_series\": [ { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"tags\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"tags\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"query\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"query\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"metricnames\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [ \"metricnames\" ] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"search\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"search\"] }, \"query_index\": 0 } ], \"correlation_score\": [ 0 . 95011 , 0 . 89177 , 0 . 77440 , 0 ], \"correlation_reliability\": [ 60 , 60 , 60 , 1 ] } } The largest part of the response is the description of the correlated series . To avoid generating overly long responses it is crucial to properly define the best and threshold parameters in the result format .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Correlation Matrix API, Paragraph = Query) Correlation search queries have four parameters , because they do not have a reference series . { \"searched_series\": {â\\x80¦ KairosDB query â\\x80¦}, \"similarity_measure\": {â\\x80¦ Similarity measure parameters (\"threshold\", â\\x80¦) â\\x80¦}, \"result_format\": {â\\x80¦ Result format parameters (\"best\",\"threshold\",\"sparse\", â\\x80¦) â\\x80¦}, \"computation_method\": {â\\x80¦ Computation method parameters (\"name\", â\\x80¦) â\\x80¦} } There are 2 other differences between matrix query API and search query API : result_format has two extra boolean options called sparse and return_perfect_scores . When the best option and/or the threshold option are used , Skyminer only returns the correlations that match this criterion . But there will be cross correlations between the series involved in this selected correlations that might interest the user . By setting sparse to false , those are also returned in the response . computation_method : implemented are memory_cached and threaded_memory_cached . The second one is usually faster .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Correlation Matrix API, Paragraph = Result) The result is divided into the same two parts as for Search queries : query and results . The â\\x80\\x9cresultsâ\\x80\\x9d sub-object has two parameters : correlated_series : a table with p elements describing the p series that have been correlated to the reference series . correlation_score : a table with p numbers ( usually between 0 and 1 ) representing the correlation scores of the p correlated series . Here is an example response : { \"query\": { â\\x80¦ A correlation query as presented before â\\x80¦ }, \"results\": { \"correlated_series\": [ { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"tags\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"tags\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"query\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"query\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"metricnames\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [ \"metricnames\" ] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"search\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"search\"] }, \"query_index\": 0 } ], \"correlation_score\": [[ 0 , 1 , 0 . 95011 , 20 ], [ 0 , 2 , 0 . 86545 , 20 ], [ 1 , 2 , 0 . 47011 , 10 ]], } }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures) Similarity measure is the correlation measure used to compute the similarity between pairs of series . It defines the mathematical method that compares the behaviors of the series . The value returned by a similarity measure is between 0 and 1 , 0 meaning that they have nothing in common and 1 that the measured behavior is exactly the same for the two series . Implemented measures : Discarding linear correlation : This correlation method is often called linear or Pearson correlation . It compares the relative variations of the time series , but is only using pairs of data points that have exactly equal timestamps . DTW : Dynamic Time Warping is a method that originates from Speech Processing and that compares pairs of samples but allowing a flexibility of sample comparisons in a given time range ( defined by the parameter called Maximum delay ).',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures, Paragraph = Discarding linear correlations) Discarding linear correlation is the classical correlation , also called Pearson correlation , where samples that does not have a match with the same timestamp in the other series are discarded . Description Linear correlation is the most classical and widely adopted time series similarity measure . It is classically defined for equally sampled time series , but can be adapted to irregular sampling by discarding values that are not sampled for both series . Adaptations have been made with kernel based methods ( Rehfeld , Marwan , Heitzig , & Kurths , 2011 ) to give a higher hit rate when samples are almost synchronized but not entirely , but it is actually equivalent to pre-aggregate the series with the proper smoother , and is thus useless in our case . Mathematical model is summarized as follows :',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures, Paragraph = Discarding linear correlations) Reliability threshold : The reliability threshold specifies the minimum number of samples that need to be used in the correlation . If the requirement is not met the correlation score is set to 0 which allows you to easily discard this unreliable result .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures, Paragraph = DTW) reliability_threshold : Reliability threshold ( integer > 0 ) normalize : Normalize ( true or false ) beta : Beta parameter of DTW algorithm maximum_delay : Maximum delay parameter of DTW algorithm local_distance : Local Distance measure',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures, Paragraph = DTW) Reliability threshold : The reliability threshold specifies the minimum number of samples that need to be used in the correlation . If the requirement is not met the correlation score is set to 0 which allows you to easily discard this unreliable result . Normalize : To make it scale free , the normalized option should be checked . It centers the series around their means and divides them by their variance before applying DTW Beta : As DTW is originally a distance , it needs to be converted to a similarity measure . Beta allows to set the sensitivity of this sharpness . If it is close to 0 quite large distances will be represented as having non-negligible correlation . The larger it is the more severe the conversion is . It should not be smaller or equal to zero . Maximum delay : The maximum delay is the temporal flexibility given to the warping . Local distance : The local distance is the measure of distance between samples . It can be absolute or squared .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Correlations API, Chapter = Similarity Measures, Paragraph = DTW) DTW is the Dynamic Time Warping algorithm . Its specificity is that it compares pairs of samples , while allowing a fluctuation in a given time range ( defined by the parameter called Maximum delay ). Description Dynamic time warping is a distance measure between time signals that comes from the speech processing community but has been widely adopted by big data analysts . It is an adaptation of a classical distance measure that is tolerant to deviations in the time sampling . In speech processing its benefit is to adapt to sentences spoken at different speed . In general data analysis it is used because it tolerates divergences due to differences of data sources . Going into the details of this technique is not in the scope of this report . A good introduction can be found in this paper : Efficient Satellite Image Time Series Analysis Under Time Warping ( Petitjean , Inglada , & GanÃ§arski , 2012 ). A few adaptations have been made to the technique to make it more coherent with our use case : In particular a normalization pre-processing step has been added as an option to add the ability to detect pattern similarities even if series have different scales . Another normalization is done according to the length of the warping . This normalizationâ\\x80\\x99s legitimacy can be discussed but it is a necessary evil when exploring data with different sampling rates . The typical bands that are meant to limit computational complexity and memory consumption have been adapted to be based on the timestamps . The local distance measure has be defined to be ( Xt1-Yt2 )^2 or |Xt1-Yt2|. The absolute value is usually used but the squared distance in conjunction with normalization corresponds to a generalization of the linear correlation which is an interesting property . DTW generates a distance d which is converted to a similarity measure using the classical formula r=1/(( 1+d )^Î² ), where Î² can be set to have any desired sensitivity . Indeed if Î² is large only very small distances will be represented with a large similarity factor . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Delete Data Points) Delete will perform the query specified in the body and delete all data points returned by the query . Aggregators and groupers have no effect on which data points are deleted . Delete is designed such that you could perform a query , verify that the data points returned are correct , and issue the delete with that query . Note : Delete works for the Cassandra and H2 data store only .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Delete Data Points, Chapter = Response) Success The response will be 204 NO CONTENT with no body . Failure The response will be 400 Bad Request if the request is invalid . The response will be 500 Internal Server Error if an error occurs . { \"errors\":[\"Connect to 10 . 92 . 4 . 1 : 4242 timed out\"] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Delete Metric) Deletes a metric and all data points associated with the metric . Note : Delete works for the Cassandra and H2 data stores only .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Delete Metric, Chapter = Response) Success The response will be 204 NO CONTENT with no body . Failure The response will be 400 Bad Request if the request is invalid . The response will be 500 Internal Server Error if an error occurs . { \"errors\": [\"Connect to 10 . 92 . 4 . 1 : 4242 timed out\"] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Event sourcing, Chapter = Example) \"event_sourcing\": { \"rename_metrics\": true , \"apply_tag\": true , \"time_align\": \"align_end\", \"events\": [ { \"name\": \"event_1\", \"timerange\": { \"start_absolute\": 1565560800265 , \"end_relative\": { \"value\": 1 , \"unit\": \"hours\" } } } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Event sourcing, Chapter = Event sourcing Parameters) rename_metrics - ( boolean ) When set to true , the returned metrics will be suffixed by â\\x80\\x9c/â\\x80\\x9d followed by the event name . apply_tag - ( boolean ) When set to true , the metrics will be returned with the tag â\\x80\\x9c_eventâ\\x80\\x9d having as value the name of the event . time_align - ( no_align | align_start | align_end ) no_align : timestamp of data is not modified align_start The timestamp of each data point is shifted as if all events would have started at query start time align_end The timestamp of each data point is shifted as if all events would have finished at query end time events - ( Event array )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Event sourcing, Chapter = Event) name - ( String ) The name property defined the name of the event . time_range - ( TimeRange ) You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If either end time is not specified the current date and time is assumed . start_absolute The time in milliseconds . start_relative The relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . end_absolute The time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . end_relative The relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to the current date and time . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Features) The Features API returns metadata about various components of Skyminer . For example , this API will return metadata about aggregators and GroupBys . Returns metadata for all features .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Features, Chapter = Response) Success Returns 200 when successful . [{ \"name\": \"groupby\", \"label\": \"Group By\", \"properties\": [{ \"name\": \"tag\", \"label\": \"Tag\", \"description\": \"Groups data points by tag names .\", \"properties\": [{ \"name\": \"tags\", \"label\": \"Tags\", \"description\": \"A list of tags to group by .\", \"optional\": false , \"type\": \"array\", \"options\": [], \"defaultValue\": \"[]\", \"autocomplete\": \"tags\", \"multiline\": false , \"validations\": [{ \"expression\": \"value . length \\\\u003e 0\", \"type\": \"js\", \"message\": \"Tags can\\\\u0027t be empty .\" }] }] }, ... }, { \"name\": \"aggregators\", \"label\": \"Aggregator\", \"properties\": [{ \"name\": \"avg\", \"label\": \"AVG\", \"description\": \"Averages the data points together .\", \"properties\": [{ \"name\": \"align_sampling\", \"label\": \"Align sampling\", \"description\": \"When set to true the time for the aggregated data point for each range will fall on the start of the range instead of being the value for the first data point within that range . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur .\", \"optional\": false , \"type\": \"boolean\", \"options\": [], \"defaultValue\": \"true\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [] }] }, ... }] }] Returns metadata for a particular feature .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Features, Chapter = Response) Success Returns 200 when successful . { \"name\": \"aggregators\", \"label\": \"Aggregator\", \"properties\": [{ \"name\": \"avg\", \"label\": \"AVG\", \"description\": \"Averages the data points together .\", \"properties\": [{ \"name\": \"align_sampling\", \"label\": \"Align sampling\", \"description\": \"When set to true the time for the aggregated data point for each range will fall on the start of the range instead of being the value for the first data point within that range . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur .\", \"optional\": false , \"type\": \"boolean\", \"options\": [], \"defaultValue\": \"true\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [] }, { \"name\": \"align_start_time\", \"label\": \"Align start time\", \"description\": \"Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The effect of setting this to true is that your data will take the same shape when graphed as you refresh the data . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur .\", \"optional\": false , \"type\": \"boolean\", \"options\": [], \"defaultValue\": \"false\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [] }, { \"name\": \"align_end_time\", \"label\": \"Align end time\", \"description\": \"Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The difference between align_start_time and align_end_time is that align_end_time sets the timestamp for the datapoint to the beginning of the following period versus the beginning of the current period . As with align_start_time , setting this to true will cause your data to take the same shape when graphed as you refresh the data . Note that align_start_time and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur .\", \"optional\": false , \"type\": \"boolean\", \"options\": [], \"defaultValue\": \"false\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [] }, { \"name\": \"sampling\", \"label\": \"Sampling\", \"optional\": false , \"type\": \"Object\", \"multiline\": false , \"properties\": [{ \"name\": \"value\", \"label\": \"Value\", \"description\": \"The number of units for the aggregation buckets\", \"optional\": false , \"type\": \"long\", \"options\": [], \"defaultValue\": \"1\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [{ \"expression\": \"value \\\\u003e 0\", \"type\": \"js\", \"message\": \"Value must be greater than 0 .\" }] }, { \"name\": \"unit\", \"label\": \"Unit\", \"description\": \"The time unit for the sampling rate\", \"optional\": false , \"type\": \"enum\", \"options\": [\"MILLISECONDS\", \"SECONDS\", \"MINUTES\", \"HOURS\", \"DAYS\", \"WEEKS\", \"MONTHS\", \"YEARS\"], \"defaultValue\": \"MILLISECONDS\", \"autocomplete\": \"\", \"multiline\": false , \"validations\": [] }] }] } ... }] 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By bin) Data point values are grouped into bins or buckets , based on a list of bin values . For example , if the list of bins is 10 , 20 , 30 , then values lower than 10 are placed into the first group , the ones between 10 and 20 into the second group , and so forth .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By bin, Paragraph = Syntax) The name for this grouper is â\\x80\\x9cbinâ\\x80\\x9d. \"name\": \"bin\" The grouper requires a bins property . This is a list of bin values . \"bins\": [ bin1 , bin2 , bin3 , ...]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By bin, Paragraph = Example) This example groups values into groups of 2 . \"group_by\": [ { \"name\": \"bin\", \"bins\": [\"2\", \"4\", \"6\", \"8\"] } ] Each object of the response JSON contains the group_by information you specified in the query as well as a group object . The group object contains the group number starting with a group number of 0 . For example , the first group ( bin number 0 ) contains data points whose values are between 0 and 2 . The second group ( bin number 1 ) contains data points whose values are between 2 and 4 , etc . { \"queries\": [ { \"results\": [ { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"bin\", \"bins\": [\"2\", \"4\", \" 6\", \" 8\"], \"group\": { \"bin_number\": 0 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [ server1\"] }, \"values\": [ [ 1353222000000 , 1 ], [ 1353567600000 , 1 ] ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"bin\", \"bins\": [\"2\", \"4\", \" 6\", \" 8\"], \"group\": { \"bin_number\": 1 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server2\"] }, \"values\": [ [ 1353567600000 , 2 ], [ 1353913200000 , 2 ], [ 1353999600000 , 3 ] ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"bin\", \"bins\": [\"2\", \"4\", \" 6\", \" 8\"], \"group\": { \"bin_number\": 2 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server2\"] }, \"values\": [ [ 1353567600000 , 4 ], ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"bin\", \"bins\": [\"2\", \"4\", \" 6\", \" 8\"], \"group\": { \"bin_number\": 3 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server2\"] }, \"values\": [ [ 1353567600000 , 6 ], ] } ] } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By tag) You can group results by specifying one or more tag names . For example , if you have a customer tag , grouping by customer would create a resulting object for each customer . Multiple tag names can be used to further group the data points .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By tag, Paragraph = Syntax) The name for this grouper is â\\x80\\x9ctagâ\\x80\\x9d. \"name\": \"tag\" The grouper takes an array of tag names . \"tags\": [\"tagName1\", \"tagName2\"]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By tag, Paragraph = Example) The example below groups by both data_center and host . If , for example , there were two data centers dc1 and dc2 and two hosts server1 and server2 , you could group by the combination of host and data_center by listing both tag names in the tags property . The response JSON would contain four objects , one for each combination of data center and host ( assuming there were data points for all four combinations ). \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"data_center\", \"host\"] } ] Each object of the response JSON contains the group_by information you specified in the query as well as a group object . The group object contains the tags names and their corresponding values for the particular grouping . The first group in the results below include data points for the dc1 data center and server1 host . { \"queries\": [ { \"results\": [ { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"data_center\", \"host\"], \"group\": { \"data_center\": \"dc1\", \"host\": \"server1\" } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353222000000 , 31 ], [ 1364796000000 , 723 ] ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"data_center\", \"host\"], \"group\": { \"data_center\": \"dc2\", \"host\": \"server1\" } } ], \"tags\": { \"data_center\": [\"dc2\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353222000000 , 108 ], [ 1364796000000 , 1318 ] ] }, ... ] } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By time) The time grouper groups results by time ranges . For example , you could group data by day of week . Note that the grouper calculates ranges based on the start time of the query . So if you wanted to group by day of week and wanted the first group to be Sunday , then you need to set the queryâ\\x80\\x99s start time to be on Sunday .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By time, Paragraph = Syntax) The name for this grouper is â\\x80\\x9ctimeâ\\x80\\x9d. \"name\": \"time\" The grouper takes a range size and a group count . The range is a value and a unit . For example , 1 day would group by day of the week ( Sunday - Saturday ). The group count is the number of groups . This would typically be 7 to group by day of week . But you could set this to 14 to group by fortnight . \"range_size\": { \"value\": \"the value\", \"unit\": \"the unit\" }, \"group_count\": \"count\"',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By time, Paragraph = Example) The example below groups data points by hours of the day for a week . This creates 168 groups ( number of hours in a week ). The first group is the first hour on Sunday , the second group is the second hour of Sunday , etc ., until the last group which is the last hour on Saturday . Again this assumes that the query start time is set to Sunday at the first hour of the day . \"group_by\": [ { \"name\": \"time\", \"group_count\": \"168\", \"range_size\": { \"value\": \"1\", \"unit\": \"hours\" } } ] Each object of the response JSON contains the _group_by_ information you specified in the query as well as a _group_ object . The _group_ object contains the group number . In this example , the group number will be a number between 0 and 167 because there are 168 groups . { \"queries\": [ { \"results\": [ { ... } , { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"time\", \"range_size\": { \"value\": 1 , \"unit\": \"HOURS\" }, \"group_count\": 168 , \"group\": { \"group_number\": 60 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353222000000 , 146 ], [ 1353826800000 , 241 ] ] }, { .... }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"time\", \"range_size\": { \"value\": 1 , \"unit\": \"HOURS\" }, \"group_count\": 168 , \"group\": { \"group_number\": 156 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353567600000 , 2188 ], [ 1354172400000 , 3398 ], ] } ] } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By value) The value grouper groups by data point values . Values are placed into groups based on a range size . For example , if the range size is 10 , then values between 0-9 are placed in the first group , values between 10-19 into the second group , and so forth .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By value, Paragraph = Syntax) The name for this grouper is â\\x80\\x9cvalueâ\\x80\\x9d. \"name\": \"value\" The grouper requires a range size . This is range of the values for each group . \"range_size\": size',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Grouping, Chapter = By value, Paragraph = Example) This example groups value by a range size of 1000 . \"group_by\": [ { \"name\": \"value\", \"range_size\": 1000 } ] Each object of the response JSON contains the group_by information you specified in the query as well as a group object . The group object contains the group number starting with a group number of 0 . For example , the first group ( group number 0 ) contains data points whose values are between 0 and 999 . The second group ( group number 1 ) contains data points whose values are between 1000 and 1999 , etc . { \"queries\": [ { \"results\": [ { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"value\", \"range_size\": 1000 , \"group\": { \"group_number\": 0 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [ server1\"] }, \"values\": [ [ 1353222000000 , 146 ], [ 1353567600000 , 697 ] ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"value\", \"range_size\": 1000 , \"group\": { \"group_number\": 1 } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server2\"] }, \"values\": [ [ 1353567600000 , 1491 ], [ 1353913200000 , 2978 ], [ 1353999600000 , 2592 ] ] } ] } ] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Health Checks) Skyminer provides REST APIs that show the health of the system . There are currently two health checks executed for each API . The JVM thread deadlock check verifies that no deadlocks exist in the Skyminer JVM . The Datastore query check performs a query on the data store to ensure that the data store is responding . Other checks are added depending on enabled modules ( e . g . Time Series Secondary Index )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Health Checks, Chapter = Check) Checks the status of each health check . If all are healthy it returns status 204 otherwise it returns 500 . This can be configured to return something other than 204 by changing the kairosdb . health . healthyResponseCode property .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Health Checks, Chapter = Check, Paragraph = Response) SuccessReturns 204 if all checks are healthy . FailureReturns 500 if any of the checks are unhealthy . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = List Metric Names) Returns a list of all metric names . If you specify the prefix parameter , only names that start with prefix are returned .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = List Metric Names, Chapter = Request) for filtered metrics : http ://[ host ]:[ port ]/api/v1/metricnames http ://[ host ]:[ port ]/api/v1/metricnames ? prefix=[ prefix ] for all metrics names http ://[ host ]:[ port ]/api/v1/extended_metric_names http ://[ host ]:[ port ]/api/v1/extended_metric_names ? prefix=[ prefix ]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = List Metric Names, Chapter = Response) SuccessReturns 200 for successful queries . { \"results\": [ \"archive_file_search\", \"archive_file_tracked\" ] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata) The Metadata Rest API is a way to write data to the datastore in name/value pairs . Data is written separate from the time series data . Metadata is partitioned by a service name . A service partition can have multiple service keys . Each service key holds name/value pairs . A value is a string . Example Assume you have a service that maintains metadata about each metric . Letâ\\x80\\x99s call it the Metric Service . Your service associates each metric with a description , owner , and the unit type . The service name is â\\x80\\x9cMetric Serviceâ\\x80\\x9d, the metric is the service key and the name/value pairs are the owner , unit , and description and their values . Metric Service Metric Owner Unit Description disk . available OPs team MB Available disk space foo . throughput Foo team Bytes Number of bytes',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = Add the Value, Paragraph = Description) Writes the value for the given service , service key , and key . service The name of the service . serviceKey The name of the service key . key The name of the key . value The value to store . The value must be a string .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = Add the Value, Paragraph = Response) Success Returns 204 when successful . Failure The response will be 500 Internal Server Error if an error occurs writing the value . { \"errors\": [ \"Failed to add value\" ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = Get the Value, Paragraph = Description) Returns the value for the given service , service key , and key if it exists or an empty response if it does not exist . service The name of the service . serviceKey The name of the service key . key The name of the key .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = Get the Value, Paragraph = Response) Success The response contains the value or an empty string if not found . Returns 200 when successful . Failure The response will be 500 Internal Server Error if an error occurs writing the value . { \"errors\": [ \"Failed to retrieve value\" ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = List Service Keys, Paragraph = Description) Returns all keys for the given service or an empty list if no service keys exist for the given service . service The name of the service .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = List Service Keys, Paragraph = Response) Success { \"results\":[\"service_key_1\", \"service_key_2\"] } The response contains a list of service keys for the given service or an empty string if not found . Returns 200 when successful . Failure The response will be 500 Internal Server Error if an error occurs writing the value . { \"errors\": [ \"Failed to get keys\" ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = List Keys, Paragraph = Description) Returns all keys for the given service key or an empty list if no keys exist . service The name of the service . serviceKey The name of the service key .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = List Keys, Paragraph = Response) Success { \"results\":[\"key_1\", \"key_2\"] } The response contains a list of keys for the given service key or an empty string if not found . Returns 200 when successful . Failure The response will be 500 Internal Server Error if an error occurs writing the value . { \"errors\": [ \"Failed to get keys\" ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Metadata, Chapter = Delete Key, Paragraph = Response) Success { \"results\":[\"key_1\", \"key_2\"] } The response contains a list of keys for the given service key or an empty string if not found . Returns 200 when successful . Failure The response will be 500 Internal Server Error if an error occurs writing the value . { \"errors\": [ \"Failed to delete key\" ] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query CSV) Returns the result of a query in CSV format instead of JSON . There are three possible formats of CSV response : One row per timestamp : Each row contains the values of all the series for a specific timestamp One row per value : Each row contains one value of one of the series One row per value detailed : Each row contains one value of one of the series , details are split in several columns ( including various group-by ) The usage of these requests is strictly identical to the regular data querying ( see Query Metrics ). Maximum number of records in generated CSV is limited , and can be overriden in the query by adding a parameter named max_csv_records : a zero or negative value disables the limit . see Skyminer for default value',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query CSV, Chapter = Response) Success The response is 200 for successful queries . Epoch Time , Excel Time , abc_123 , xyz_456 1569567060022 , 43735 . 285416921295 , 21 , 1569567060022 , 43735 . 285416921295 , 23 , 6 . 959614 1569567065000 , 43735 . 28547453704 ,, 6 . 238859 Failure The response is 400 â\\x80\\x98Bad Requestâ\\x80\\x99 when the request is invalid . The response is 500 â\\x80\\x98Internal Server Errorâ\\x80\\x99 when an error occurs while retrieving data . The response payload contains the error message .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query CSV, Chapter = Response) Success The response is 200 for successful queries . Epoch Time , Excel Time , Series Name , Value 1569567060022 , 43735 . 285416921295 , abc_123 , 21 1569567060022 , 43735 . 285416921295 , abc_123 , 23 1569567060022 , 43735 . 285416921295 , xyz_456 , 6 . 959614 1569567065000 , 43735 . 28547453704 , xyz_456 , 6 . 238859 Failure The response is 400 â\\x80\\x98Bad Requestâ\\x80\\x99 when the request is invalid . The response is 500 â\\x80\\x98Internal Server Errorâ\\x80\\x99 when an error occurs while retrieving data . The response payload contains the error message .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query CSV, Chapter = Response) Success The response is 200 for successful queries . Epoch Time , Excel Time , Metric Name , Data Type , Value , carrier , fiscal_period 1605712606774 , 44153 . 63665247685 , skyminer . demo . carrier_eirp , long , 576 , carrier_1000 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605712684844 , 44153 . 637556064816 , skyminer . demo . carrier_eirp , long , 12 , carrier_101 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605712738284 , 44153 . 638174583335 , skyminer . demo . carrier_eirp , long , 578 , carrier_1025 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605713721784 , 44153 . 64955768519 , skyminer . demo . carrier_eirp , long , 78 , carrier_1002 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605716633254 , 44153 . 683255254626 , skyminer . demo . carrier_eirp , long , 11 , carrier_1001 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605717286474 , 44153 . 690815671296 , skyminer . demo . carrier_eirp , long , 12 , carrier_1028 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" 1605717537874 , 44153 . 69372539352 , skyminer . demo . carrier_eirp , long , 12 , carrier_1019 ,\"fiscal_year=2020 , fiscal_period=11 , name=FY2020-P11 , quarter=Q4\" Failure The response is 400 â\\x80\\x98Bad Requestâ\\x80\\x99 when the request is invalid . The response is 500 â\\x80\\x98Internal Server Errorâ\\x80\\x99 when an error occurs while retrieving data . The response payload contains the error message . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metric Tags) Similar to a query but only returns the tags ( no data points returned ). This can potentially return more tags than a query because it is optimized for speed and does not query all rows to narrow down the time range . This queries only the Row Key Index and thus the time range is the starting time range . When using Cassandra datastore , since the Cassandra row is set to 3 weeks , this can return tags for up to a 3 week period .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metric Tags, Chapter = Body) { \"start_absolute\": 1357023600000 , \"end_relative\": { \"value\": \"5\", \"unit\": \"days\" }, \"metrics\": [ { \"tags\": { \"host\": [\"server1\"] }, \"name\": \"abc_123\" }, { \"tags\": { \"dc\": [\"awsuse\"] }, \"name\": \"xyz_123\" } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metric Tags, Chapter = Query Properties) You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If either end time is not specified the current date and time is assumed . start_absolute The time in milliseconds . start_relative The relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . end_absolute The time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . end_relative The relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to the current date and time .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metric Tags, Chapter = Metric Properties) name The name of the metric ( s ) to return data points for . The name is required . tags Tags narrow down the search . Only metrics that include the tag and matches one of the values are returned . Tags is optional .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metric Tags, Chapter = Response) SuccessThe response contains either the metric values or possible error values . Returns 200 for successful queries . { \"results\": [ { \"name\": \"abc_123\", \"tags\": { \"host\": [\"server1\"], \"dc\": [\"awsuse\", \"awsusw\"], \"type\": [\"bar\"] }, \"values\": [[ 1492602706055 , 0 ],[ 1492602711000 , 0 ],[ 1492602712000 , 0 ],[ 1492602716055 , 0 ]] }, { \"name\": \"xyz_123\", \"tags\": { \"host\": [\"server1\",\"server2\"], \"dc\": [\"awsuse\"], \"type\": [\"bar\"] }, \"values\": [[ 1492602706055 , 0 ],[ 1492602711000 , 42 ],[ 1492602712000 , 0 ],[ 1492602716055 , 42 ]] } ] } Failure The response will be 400 Bad Request if the request is invalid . The response will be 500 Internal Server Error if an error occurs retrieving data . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics) Returns a list of metric values based on a set of criteria . Also returns a set of all tag names and values that are found across the data points . The time range can be specified with absolute or relative time values . Absolute time values are in milliseconds . Relative time values are specified as an integer duration and a unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , â\\x80\\x9c5 hoursâ\\x80\\x9d means that metric values submitted 5 hours ago will be returned . The end time is optional . If no end time is specified , the end time is assumed to be now ( the current date and time ). Grouping The results of the query can be grouped together . There are three ways to group the data ; by tags , by a time range , and by value . Grouping is done with the group_by property which is an array of one or more groupers . Note that grouping by a time range or value can slow down the query . Aggregators Optionally you can specify aggregators . Aggregators perform an operation on data points and down samples . For example , you could sum all data points that exist in 5 minute periods . Aggregators can be combined together . For example , you could sum all data points in 5 minute periods then average them for a week period . Aggregators are processed in the order they are specified in the JSON . The output of one is send to the input of the next . See the Aggregators for a complete list of aggregators . Filtering It is possible to filter the data returned by specifying a tag . The data returned will only contain data points associated with the specified tag . Filtering is done using the â\\x80\\x9ctagsâ\\x80\\x9d property .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Request Methods) Queries can be done using either a GET or POST method . The GET version requires that the JSON is encoded and passed to the â\\x80\\x9cqueryâ\\x80\\x9d parameter .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Body) { \"start_absolute\": 1357023600000 , \"end_relative\": { \"value\": \"5\", \"unit\": \"days\" }, \"time_zone\": \"Asia/Kabul\", \"event_sourcing\": { \"rename_metrics\": true , \"apply_tag\": true , \"time_align\": \"align_end\", \"events\": [{ \"name\": \"event_1\", \"timerange\": { \"start_absolute\": 1565560800265 , \"end_relative\": { \"value\": 1 , \"unit\": \"hours\" } } }] }, \"safeguard\": { \"group_limit\": 100 , \"limit_before_aggregation\": 10000000 , \"limit_after_aggregation\": 10000 }, \"metrics\": [{ \"tags\": { \"host\": [\"foo\", \"foo2\"], \"customer\": [\"bar\"] }, \"name\": \"abc . 123\", \"limit\": 10000 , \"aggregators\": [{ \"name\": \"sum\", \"sampling\": { \"value\": 10 , \"unit\": \"minutes\" } }] }, { \"tags\": { \"host\": [\"foo\", \"foo2\"], \"customer\": [\"bar\"] }, \"name\": \"xyz . 123\", \"aggregators\": [{ \"name\": \"avg\", \"sampling\": { \"value\": 10 , \"unit\": \"minutes\" } }] } ] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Query Properties) You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If either end time is not specified the current date and time is assumed . start_absoluteThe time in milliseconds . start_relativeThe relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . end_absoluteThe time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . end_relativeThe relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to the current date and time . time_zoneThe time zone for the time range of the query . If not specified , UTC is used . cache_timeThe amount of time in seconds to re use the cache from a previous query . When a query is made Skyminer looks for the cache file for the query . If a cache file is found and the timestamp of the cache file is within cache_time seconds from the current query , the cache is used . Cache files are identified by hashing the metric name , the start and end time of the query and any tags specified . For example if you query a metric using relative start of 4 hours ago and then 30 min later you run the same query with a cache_time set to 2000 ( just over 30 min ) you will get the cached data back . Sending a query with a cache_time set to 0 will always refresh the cache with new data from Cassandra . Changing aggregators on a query does not effect the use of cache . SafeguardsWhen querying Skyminer , the data returned can be very large . To prevent the response time from being excessively long , a safeguard mechanism has been developed with 3 limits : group_limit : maximum number of groups returned by a group_by ( vertical limit ) limit_before_aggregation : maximum number of datapoints for each series , before aggregation ( horizontal limit ) limit_after_aggregation : maximum number of datapoints for each series , after aggregation ( horizontal limit )',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Metric Properties) nameThe name of the metric ( s ) to return data points for . The name is required . aggregatorsThis is an ordered array of aggregators . They are processed in the order specified . The output of an aggregator is passed to the input of the next until all have been processed . If no aggregator is specified , then all data points are returned . Most aggregators support downsampling . Downsampling allows you to reduce the sampling rate of the data points and aggregate these values over a longer period of time . For example , you could average all daily values over the last week . Rather than getting 7 values you would get one value which is the average for the week . Sampling is specified with a â\\x80\\x9cvalueâ\\x80\\x9d and a â\\x80\\x9cunitâ\\x80\\x9d. value - An integer value . unit - The time range . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. align_sampling - An optional property . Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The effect of setting this to true is that your data will take the same shape when graphed as you refresh the data . This is false by default . Note that align_sampling and align_start_time are mutually exclusive . If more than one are set , unexpected results will occur . align_start_time - An optional property . When set to true the time for the aggregated data point for each range will fall on the start of the range instead of being the value for the first data point within that range . This is false by default . Note that align_sampling , align_start_time , and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur . align_end_time - An optional property . Setting this to true will cause the aggregation range to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The difference between align_start_time and align_end_time is that align_end_time sets the timestamp for the datapoint to the beginning of the following period versus the beginning of the current period . As with align_start_time , setting this to true will cause your data to take the same shape when graphed as you refresh the data . Note that align_start_time and align_end_time are mutually exclusive . If more than one are set , unexpected results will occur . start_time - An optional property . Used along with align_start_time . This is the alignment start time . This defaults to 0 . tagsTags narrow down the search . Only metrics that include the tag and matches one of the values are returned . Tags is optional . group_byThe resulting data points can be grouped by one or more tags , a time range , or by value , or by a combination of the three . The â\\x80\\x9cgroup_byâ\\x80\\x9d property in the query is an array of one or more groupers . Each grouper has a name and then additional properties specific to that grouper . See Grouping for information . Note that grouping by a time range , by value , or by bins can slow down the query . exclude_tags By default , the result of the query includes tags and tag values associated with the data points . If exclude_tags is set to true , the tags will be excluded from the response . limit Limits the number of data points returned from the data store . The limit is applied before any aggregator is executed . order Orders the returned data points . Values for order are â\\x80\\x9cascâ\\x80\\x9d for ascending or â\\x80\\x9cdescâ\\x80\\x9d for descending . Defaults to ascending . Thissorting is done before any aggregators are executed .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Query Metrics, Chapter = Response) Success The response contains either the metric values or possible error values . Returns 200 for successful queries . Results a group_by named â\\x80\\x9ctypeâ\\x80\\x9d. The type is the data type . If the data returned is numeric type then â\\x80\\x9cnumberâ\\x80\\x9d is returned . { \"queries\": [ { \"sample_size\": 14368 , \"results\": [ { \"name\": \"abc_123\", \"group_by\": [ { \"name\": \"type\", \"type\": \"number\" }, { \"name\": \"tag\", \"tags\": [ \"host\" ], \"group\": { \"host\": \"server1\" } } ], \"tags\": { \"host\": [ \"server1\" ], \"customer\": [ \"bar\" ] }, \"values\": [ [ 1364968800000 , 11019 ], [ 1366351200000 , 2843 ] ] } ] } ] } Failure The response will be 400 Bad Request if the request is invalid . The response will be 500 Internal Server Error if an error occurs retrieving data . { \"errors\": [ \"metrics [ 0 ]. aggregate must be one of MIN , SUM , MAX , AVG , DEV\", \"metrics [ 0 ]. sampling . unit must be one of SECONDS , MINUTES , HOURS , DAYS , WEEKS , YEARS\" ] } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Get Status, Paragraph = Response) Success Return 200 for successful queries . The response provides the following fields : cache_size : size of row key cache currently in use on the server indexing_job_status : status of a full index rebuild job : â\\x80\\x9cIDLEâ\\x80\\x9d, â\\x80\\x9dACTIVEâ\\x80\\x9d or â\\x80\\x9cERRORâ\\x80\\x9d message : message regarding the action requested or the status of the indexer enabled : true if indexer is used on queries , false otherwise module_status : error status of the Opensearch communication , â\\x80\\x9cOKâ\\x80\\x9d or â\\x80\\x9cERRORâ\\x80\\x9d If an indexing job has been initiated ( since startup or since last clear request ) the additional fields are provided : indexing_job_processed_metrics : number of metrics currently processed indexing_job_total_metrics : total number of metrics to process indexing_job_start_timestamp : start timestamp ( epoch milliseconds ) of the rebuild index job indexing_job_end_timestamp : end timestamp ( epoch milliseconds ) of the rebuild index job , 0 if not finished indexing_job_error_message : only if an error occurred during indexing - the last error message is provided Example : { \"cache_size\": 74156 , \"indexing_job_status\": \"IDLE\", \"message\": \"No rebuild in progress\", \"enabled\": true , \"module_status\": \"OK\" }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Rebuild index) Start a job to rebuild the index entirely for all metrics . If a job already exists the message will tell it and no action will be taken . This job will totally delete the previous index and replace it with a new one .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Disable on queries) Disable the module for querying data . This doesnâ\\x80\\x99t stop the online indexing - the new time series ingested will continue to be indexed .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Get Data types, Paragraph = Response) Success Return 200 for successful queries . The response provides an array of different known data types : Example : [\"constellation_diagram\",\"number\",\"spectrum_trace\",\"text\"]',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Get metrics names, Paragraph = Response) Success Return 200 for successful queries with results . Return 200 for successful queries without results . The response provides a map of different known metric names associated to data type : Example : { \"spectrum_trace\":[\"skyminer . demo . spectrum . data\",\"skyminer . demo . spectrum . data_2\",\"skyminer . demo . spectrum . trace\"], \"constellation_diagram\":[\"skyminer . demo . constellation_diagram\"] }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Get metrics time ranges, Paragraph = Body) json object with fields : - metric_name : name of the metric to search ( required ) - tag_filters : filters on tags ( map of key : tag name , values : list of possible values ) example : { \"metric_name\" : \"skyminer . demo\", \"tag_filters\" : { \"site\": [\"Site1\"], \"antenna\": [\"Antenna2\", \"Antenna3\"] } }',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module API, Chapter = Get metrics time ranges, Paragraph = Response) Success Return 200 for successful queries with results . Return 204 for successful queries without results . The response provides an array of time ranges ( timestamp format ) : Example : [ [ 1661990400000 , 1665619300000 ], [ 1666619300000 , 1666719300000 ] ] 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Version, Chapter = Request) http ://[ host ]:[ port ]/api/skyminer/version It is also possible to use the following requests to get further details on the build and commit respectively : http ://[ host ]:[ port ]/api/skyminer/version/build http ://[ host ]:[ port ]/api/skyminer/version/commit',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Version, Chapter = Response) Success Returns 200 for successful queries . When unable to fetch the build version , the value â\\x80\\x98unknownâ\\x80\\x99 will be returned instead . { \"version\": \"2019-S8\" } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Version, Chapter = Response) Success Returns 200 for successful queries . { \"version\": \"KairosDB 0 . 9 . 4\" } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Overview) This API provides an operation to store metrics and query for the current version of Kairosdb . Data points have a metric name , a value , a timestamp , and a list of one or more tags . Tags are named properties that identify the data , such as its type and where it comes from . Metric names , tag names and values are case sensitive and can contain any character except spaces and in the case of tags anything except â\\x80\\x98=â\\x80\\x99. If a data point names a metric that does not exist , the metric is created . The timestamp is the number of milliseconds since January 1st , 1970 UTC . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Put) You can submit data either with the telnet protocol on port 4242 . The port can be changed in the kairosdb . properties file . The format of the data is put <metric name> <time stamp> <value> <tag> <tag>... \\\\n Metric name must be one word and is limited to to utf8 characters . Time stamp can either be in milliseconds or in seconds since Jan 1 , 1970 ( unix epoch ). If the value is less than 3 , 000 , 000 , 000 it is considered seconds . If you want to send milliseconds you may want to consider using putm . Value can either be a long or double value . Tag is in the form of key=value . Be aware that the data sent must be followed by a line feed character . Here is a simple shell script that inserts data using netcat . #!/bin/bash # Current time in milliseconds now=$(($( date +%s%N )/1000000 )) metric=load_value_test value=42 host=10 . 92 . 4 . 4 echo \"put $metric $now $value host=A\" | nc -w 30 $host 4242 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Putm) This is identical to the put command except it always expects millisecond timestamps . You can submit data either with the telnet protocol on port 4242 . The port can be changed in the kairosdb . properties file . The format of the data is putm <metric name> <time stamp> <value> <tag> <tag>... \\\\n Metric name must be one word and is limited to utf8 characters . Time stamp milliseconds since Jan 1 , 1970 ( unix epoch ) Value can either be a long or double value . Tag is in the form of key=value . Be aware that the data sent must be followed by a line feed character . Here is a simple shell script that inserts data using netcat . #!/bin/bash # Current time in milliseconds now=$(($( date +%s%N )/1000 )) metric=load_value_test value=42 host=10 . 92 . 4 . 4 echo \"putm $metric $now $value host=A\" | nc -w 30 $host 4242 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Special Tags) Because of the simplistic approach to the telnet protocol we have used special tags to add additional functionality . These tags when used will not show up as tags on your data but , will trigger the functionality they define . kairos_opt . ttl This tag allows you to specify the ttl ( time to live ) for the data point . The value must be a number greater or equal to zero . Zero or no ttl is the default . 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Version) The version command returns the product name ( KairosDB ) and its version . version \\\\n The output looks like this KairosDB 1 . 0 . 0 Here is an simple example using netcat . echo \"version\" | nc -w 30 localhost 4242 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Python Connector) This section documents how to use the Python connector to send data to Skyminer . Please refer to the user documentation and python library documentation for more details .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Python Connector, Chapter = Send data points to Skyminer server) Use DataPointBuilder and add_data_points to send data points to the Skyminer server . from SkyminerTS import SkyminerTSPythonConnector from SkyminerTS import DataPoint from datetime import datetime timestamp_epoch = int ( datetime . now (). timestamp ()*1000 ) point_value = 12 api = SkyminerTSPythonConnector . API . init (\"http ://url-to-skyminer/api/v1/\") dp = DataPoint . DataPointBuilder ()\\\\ . with_point ( timestamp_epoch , point_value )\\\\ . with_name (\"test_metric_name\")\\\\ . with_tag (\"sensor\", \"bmp280\")\\\\ . with_tag (\"location\", \"skyminer dev room\") api . add_data_points ( DataPoint . DataPointsPayloadBuilder (). add_builder ( dp ). build ())',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Python Connector, Chapter = Send a Dataframe (single metric) to the Skyminer Server) from SkyminerTS import STSAPI , DataFrameToDataPointBuilder # Dataframe DF = your dataframe # Init the API API = STSAPI . init (\"http ://url-to-skyminer/api/v1/\") # DataPointBuilder DPB = DataFrameToDataPointBuilder ( DF , \\'EIRP\\', tags={\"from\" : \"SAT1\"}) # Send Dataframe API . add_data_points ( DPB . build ()) 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module) The time series indexer module uses Opensearch to store information on time series , in particular Cassandra row keys . It provides faster queries on data and a specific REST API to manage the module on a per-node basis . It is enabled by default on Skyminer platform complete deployments after version 2 . 7 . This module is not necessary to operate Skyminer .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Time series indexer configuration) The configuration is managed in a properties file . The settings are the following : skyminer . ts-index . open_search . urls : List of URLs to Opensearch servers skyminer . ts-index . open_search . index : Name of Opensearch index , optional by default skyminer-cassandra-ts-index is used skyminer . ts-index . enabled_by_default : Specifies if the module is enabled for queries at startup , optional , default is true . Accepted values : true or false . skyminer . ts-index . open_search . mapping_auto_update : Specifies if the index module is auto updated at startup , optional , default is false . Accepted values : true or false . skyminer . ts-index . open_search . search . metrics . size : Specifies maximal number of metrics searched , default is 10 000 . skyminer . ts-index . open_search . search . metrics . nb_values : Specifies maximal numbers of metrics to return when searching via the rest api , default is 1000 . skyminer . ts-index . open_search . search . metrics . tags . nb_values : Specifies maximal number of matching tags by metric to return when searching via the rest api , default is 5 . skyminer . ts-index . open_search . search . metrics . tags . values . nb_values : Specifies maximal number of matching tag values for each tag to return when searching via the rest api , default is 3 . skyminer . ts-index . open_search . timeout . connect : Specifies Opensearch connect timeout in ms , default is 10 000 . skyminer . ts-index . open_search . timeout . socket : Specifies Opensearch socket timeout in ms , default is 30 000 . The module is automatically enabled if the Indexed Cassandra datastore datastore module is used . Otherwise the module can be activated independently using the setting kairosdb . service . tsindex with value com . skyminer . timeseriesindex . RowKeyIndexingModule',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Time series indexer mechanisms) Every time data is indexed into Cassandra an index in Opensearch is maintained . On a cluster it is important that every node that writes data uses the module to avoid inconsistencies . A specific REST API allows to get status of the plugin rebuild the Opensearch index using the API and to get the status , this takes a long time depending on the number of metrics and time series . disable/enable the plugin on queries using the REST API . clear ( ignore ) error search specific metrics by keyword Please check ( Skyminer Time Series Indexer Module API ) for more information . In case there are communications errors with Opensearch , the status will be reported as ERROR . The error status is not persistent , and will disappear if the service is restarted . Hence it is recommended to rebuild the index periodically . The activation status ( enabled/disabled ) on queries is not persistenmt either , the module is activated by default on all queries after a restart .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Time series indexer metrics) The module when activated on queries generates the following metrics : skyminer . datastore . time-series-index . raw_row_key_count : Row key count for a query skyminer . datastore . time-series-index . key_query_time : Time in milliseconds to fetch row keys from the module',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Enabling the time series indexer after an upgrade) The indexer is not automatically enabled after a system upgrade . It has to be done on every node running Skyminer service . The indexer can be enabled by activating the module in the system configuration file : For using with Cassandra database to improve query performances , please see how to activate the relevant datastore : Indexed Cassandra datastore For using without query improvement , add the following line in the configuration : kairosdb . service . tsindex=com . skyminer . timeseriesindex . RowKeyIndexingModule A file named ts-index . properties shall be placed in Skyminer conf folder with relevant configuration parameters ( Time series indexer configuration ) Restart the Skyminer service ( Starting & Stopping Skyminer alone ) to activate the module Once activated rebuild the metrics index using legacy data using the rebuild cabability of the REST API : Rebuild index For example using curl command : curl <Skyminer_server_URL>/api/skyminer/rebuild/',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Disabling the time series indexer) The indexer can be disabled by applying the following steps on every node running Skyminer service : Remove ( delete ) the configuration file called ts-index . properties Deactivate usage of indexer in Cassandra datastore as described in : Indexed Cassandra datastore Restart the Skyminer service ( Starting & Stopping Skyminer alone ) to apply the new configuration',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer Time Series Indexer Module, Chapter = Skyminer time series indexer module API) GET /skyminer/ts-index/enableÂ Enable the module for querying data . Enable the module for querying data . Example request : GET /skyminer/ts-index/enable HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"enabled\": true , \"message\": \"Plugin has been enabled on queries\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/disableÂ Disable the module for querying data Disable the module for querying data . This doesnâ\\x80\\x99t stop the online indexing - the new time series ingested will continue to be indexed . Example request : GET /skyminer/ts-index/disable HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"enabled\": false , \"message\": \"Plugin has been enabled on queries\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/statusÂ Get indexer status Returns the indexer status Example request : GET /skyminer/ts-index/status HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"indexing_job_processed_metrics\": 15 , \"indexing_job_total_metrics\": 15 , \"indexing_job_start_timestamp\": 1665069072000 , \"indexing_job_end_timestamp\": 1665069073000 , \"enabled\": true , \"message\": \"No rebuild in progress\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/status/clearÂ Reset ( Clear ) status Reset the error flag on the status , and clear indexer job information . Example request : GET /skyminer/ts-index/status/clear HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"enabled\": true , \"message\": \"Status has been reset\", \"cache_size\": 100 , \"module_status\": \"OK\" } 403 Forbidden â\\x80\\x93 FORBIDDEN if rebuild is in progress GET /skyminer/ts-index/traceÂ Activate trace mode Activate trace mode for 5 minutes : log operation done by the module Example request : GET /skyminer/ts-index/trace HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"enabled\": true , \"message\": \"Tracing row keys - it will be stopped after 5 minutes\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/rebuildÂ Rebuild index for all metrics Start a job to rebuild the index entirely for all metrics . If a job already exists the message will tell it and no action will be taken . This job will totally delete the previous index and replace it with a new one . Example request : GET /skyminer/ts-index/rebuild HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"ACTIVE\", \"indexing_job_processed_metrics\": 15 , \"indexing_job_total_metrics\": 85 , \"indexing_job_start_timestamp\": 1665069072000 , \"enabled\": true , \"message\": \"Initiating index rebuild for all 100 metrics\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/rebuild/{metric}Â Rebuild index for specific metrics Refresh the index for a single metric . Parameters : metric ( string ) â\\x80\\x93 name of the metric to rebuild Example request : GET /skyminer/ts-index/rebuild/{metric} HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"indexing_job_status\": \"IDLE\", \"indexing_job_processed_metrics\": 50 , \"indexing_job_total_metrics\": 50 , \"indexing_job_start_timestamp\": 1665069072000 , \"indexing_job_end_timestamp\": 1665069073000 , \"enabled\": true , \"message\": \"Index Rebuilt for metric skyminer_spectrum_trace\", \"cache_size\": 100 , \"module_status\": \"OK\" } GET /skyminer/ts-index/data-typesÂ Get all data types Return all the data types known in time series Example request : GET /skyminer/ts-index/data-types HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json [ \"constellation_diagram\", \"number\", \"spectrum_trace\" ] 500 Internal Server Error â\\x80\\x93 Internal server error GET /skyminer/ts-index/metrics/namesÂ Get metrics names Return all the metrics names by data type Query Parameters : data_type ( string ) â\\x80\\x93 ask for metrics associated to a specific data type ( Required ) Example request : GET /skyminer/ts-index/metrics/names ? data_type=string HTTP/1 . 1 Host : example . com Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"constellation_diagram\": [ \"skyminer . demo . constellation_diagram\" ], \"spectrum_trace\": [ \"skyminer . demo . spectrum . data\", \"skyminer . demo . spectrum . data2\" ] } 204 No Content â\\x80\\x93 OK no content 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/ts-index/metrics/time-rangesÂ Get metrics presence over time Return all the time ranges where metric have data Example request : POST /skyminer/ts-index/metrics/time-ranges HTTP/1 . 1 Host : example . com Content-Type : application/json { \"metric_name\": \"string\", \"tag_filters\": {} } Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json [ [ 1661990400000 , 1665619300000 ], [ 1666619300000 , 1666719300000 ] ] 204 No Content â\\x80\\x93 OK no content 500 Internal Server Error â\\x80\\x93 Internal server error POST /skyminer/ts-index/searchÂ Search for metrics Search all metrics that contains the words in searchTerm . Search in metric names and tags Example request : POST /skyminer/ts-index/search HTTP/1 . 1 Host : example . com Content-Type : application/json { \"searchTerm\": \"string\" } Status Codes : 200 OK â\\x80\\x93 OK Example response : HTTP/1 . 1 200 OK Content-Type : application/json { \"matches\": [ { \"metric\": \"string\", \"tags\": {} } ] } 500 Internal Server Error â\\x80\\x93 Internal server error 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer document Module) The Skyminer document module uses Opensearch to store documents . It is enabled by default on Skyminer platform complete deployments after version 2 . 7 . This module is not necessary to operate Skyminer .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer document Module, Chapter = Skyminer document module configuration) The configuration is managed in a properties file : skyminerdocumentmodules . properties The module is automatically enabled if the first line of configuration file is not commented : kairosdb . service . skyminerdocument=com . skyminer . documentmodule . SkyminerDocumentModule The settings are the following : skyminerdocument . opensearch . urls : List of URLs to Opensearch servers skyminerdocument . opensearch . user : User for connecting to opensearch skyminerdocument . opensearch . password : Password for connecting to opensearch skyminerdocument . opensearch . index . number_of_shards : Number of shards required when creating a new index skyminerdocument . opensearch . index . number_of_replicas : Number of replicas required when creating a new index skyminerdocument . kairosqueuemanager . runners : Number of threads on server skyminerdocument . manager . elasticbatch : Maximum size of batches submitted to OpenSearch in number of documents skyminerdocument . max_batch_size_bytes : Maximum batch size accepted by server in bytes skyminerdocument . max_document_size_bytes : Maximum document size accepted by server in bytes skyminerdocument . opensearch . timeout . connect : Specifies Opensearch connect timeout in ms , default is 10 000 . skyminerdocument . opensearch . timeout . socket : Specifies Opensearch socket timeout in ms , default is 30 000 .',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Skyminer document Module, Chapter = Skyminer document module API) GET /docÂ describes the document backend in use ( e . g . OpenSearch ) Example request : GET /doc HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 OK . GET /doc/configÂ List indices Example request : GET /doc/config HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 Descriptions of a list of indices Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"indices\": [ \"string\" ] } POST /doc/config/{indexName}Â Create an index Parameters : indexName ( string ) â\\x80\\x93 name of the index Example request : POST /doc/config/{indexName} HTTP/1 . 1 Host : example . com Content-Type : application/json { \"settings\": { \"number_of_shards\": 1 , \"number_of_replicas\": 1 }, \"mappings\": { \"type1\": { \"field1\": { \"type\": \"string\" } } }, \"timeseries\": { \"metrics\": [ { \"name\": \"string\", \"valueField\": \"string\", \"timestampField\": \"2023-10-24T13 : 01 : 37 . 560189\", \"valueType\": \"string\", \"staticTags\": [ { \"name\": \"string\", \"valueField\": \"string\" } ] } ] } } Status Codes : 202 Accepted â\\x80\\x93 Description of a generic creation of index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"index\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 403 Forbidden â\\x80\\x93 Index is already used . Example response : HTTP/1 . 1 403 Forbidden Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } PUT /doc/config/{indexName}Â Update an index with additional mapping information Parameters : indexName ( string ) â\\x80\\x93 name of the index Status Codes : 202 Accepted â\\x80\\x93 Ok . 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 403 Forbidden â\\x80\\x93 trying to change an existing mapping - can only add new mappings . 404 Not Found â\\x80\\x93 Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } DELETE /doc/config/{indexName}Â Delete an index Parameters : indexName ( string ) â\\x80\\x93 name of the index Status Codes : 204 No Content â\\x80\\x93 Ok . No data return . 404 Not Found â\\x80\\x93 Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } GET /doc/config/{indexName}Â Get information about an index ( incl . mapping ) Parameters : indexName ( string ) â\\x80\\x93 name of the index Example request : GET /doc/config/{indexName} HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 Descriptions of an index Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"health\": \"string\", \"status\": \"string\", \"name\": \"string\", \"count\": 1 , \"size\": 1 . 0 } 404 Not Found â\\x80\\x93 Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } GET /doc/data/{indexName}/{id}Â Access to a document by id Parameters : indexName ( string ) â\\x80\\x93 name of the index id ( integer ) â\\x80\\x93 id of the document Example request : GET /doc/data/{indexName}/{id} HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 Description of a generic index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"type1\": { \"field1\": { \"type\": \"string\" } } } 404 Not Found â\\x80\\x93 ID or Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 500 Internal Server Error â\\x80\\x93 Unexpected error Example response : HTTP/1 . 1 500 Internal Server Error Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } DELETE /doc/data/{indexName}/{id}Â Delete a document by id Parameters : indexName ( string ) â\\x80\\x93 name of the index id ( integer ) â\\x80\\x93 id of the document Status Codes : 204 No Content â\\x80\\x93 Ok . No data return . 404 Not Found â\\x80\\x93 ID or Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 500 Internal Server Error â\\x80\\x93 Unexpected error Example response : HTTP/1 . 1 500 Internal Server Error Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } PUT /doc/data/{indexName}/{id}Â Update a document by id Parameters : indexName ( string ) â\\x80\\x93 name of the index id ( integer ) â\\x80\\x93 id of the document Status Codes : 202 Accepted â\\x80\\x93 Description of a generic index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"type1\": { \"field1\": { \"type\": \"string\" } } } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 ID or Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 500 Internal Server Error â\\x80\\x93 Unexpected error Example response : HTTP/1 . 1 500 Internal Server Error Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } POST /doc/data/{indexName}/{id}Â Create a new document with specified id Parameters : indexName ( string ) â\\x80\\x93 name of the index id ( integer ) â\\x80\\x93 id of the document Example request : POST /doc/data/{indexName}/{id} HTTP/1 . 1 Host : example . com Content-Type : application/json { \"type1\": { \"field1\": { \"type\": \"string\" } } } Status Codes : 202 Accepted â\\x80\\x93 Description of a generic index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"type1\": { \"field1\": { \"type\": \"string\" } } } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 403 Forbidden â\\x80\\x93 ID or Index is already used . Example response : HTTP/1 . 1 403 Forbidden Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 ID or Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 500 Internal Server Error â\\x80\\x93 Unexpected error Example response : HTTP/1 . 1 500 Internal Server Error Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } POST /doc/data/{indexName}Â Add a document Parameters : indexName ( string ) â\\x80\\x93 name of the index Example request : POST /doc/data/{indexName} HTTP/1 . 1 Host : example . com Content-Type : application/json { \"type1\": { \"field1\": { \"type\": \"string\" } } } Status Codes : 202 Accepted â\\x80\\x93 Description of a generic creation of index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"index\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } GET /doc/data/{indexName}Â Search in index Parameters : indexName ( string ) â\\x80\\x93 name of the index Query Parameters : range ( string ) â\\x80\\x93 The range parameter is used for pagination . You can define it with two integers offset and limit . ? range=<offset>-<limit> The offset integer defines where you want to start to fetch results and the limit integer defines where you want to stop . search ( string ) â\\x80\\x93 The search parameter is used for searching . It takes a comma-separated list of property : value . ? search=animal : cat ; color : black ; age : 2 Special arguments can be passed for research on properties via â\\x80\\x9c|â\\x80\\x9d. List of the arguments : - r : Search with regex on a property - gte : Greater than or equal to - gt : Greater than - lte : Lesser than or equal to - lt : Lesser than ? search=animal : cat ; color : bl*|r ; age : 2|gte sort ( string ) â\\x80\\x93 The sort parameter allows to sort on specific fields . It takes a comma-separated list of property : sort . Property can be sort only by ascending ( asc ) or descending ( desc ) order . ? sort=animal : desc ; age : asc Example request : GET /doc/data/{indexName} HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 OK . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json [ { \"type1\": { \"field1\": { \"type\": \"string\" } } } ] 206 Partial Content â\\x80\\x93 The argument range is used and the server is successfully fulling a range request . 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } Response Headers : Content-Range â\\x80\\x93 Indicates where in a full body message a partial message belongs . Content-Range : <start>-<end>/<size> Accept-Range â\\x80\\x93 Indicates the unit that can be used to define a range . Accept-Range : <index> 20 Link â\\x80\\x93 Links to the other ranges . POST /docs/data/{indexName}Â Create a new bulk of documents Parameters : indexName ( string ) â\\x80\\x93 name of the index Example request : POST /docs/data/{indexName} HTTP/1 . 1 Host : example . com Content-Type : application/json [ { \"type1\": { \"field1\": { \"type\": \"string\" } } } ] Status Codes : 202 Accepted â\\x80\\x93 Description of a generic creation of index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"index\": \"string\" } 206 Partial Content â\\x80\\x93 Documents partially added due to some unblocking errors . Example response : HTTP/1 . 1 206 Partial Content Content-Type : application/json { \"id\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } PUT /docs/data/{indexName}Â Update bulk of documents Parameters : indexName ( string ) â\\x80\\x93 name of the index Example request : PUT /docs/data/{indexName} HTTP/1 . 1 Host : example . com Content-Type : application/json [ { \"type1\": { \"field1\": { \"type\": \"string\" } } } ] Status Codes : 202 Accepted â\\x80\\x93 Description of a generic creation of index response . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"index\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 ID or Index not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } POST /notificationÂ Create a notification Example request : POST /notification HTTP/1 . 1 Host : example . com Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } Status Codes : 202 Accepted â\\x80\\x93 Ok . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } GET /notificationÂ Query notifications Query Parameters : range ( string ) â\\x80\\x93 The range parameter is used for pagination . You can define it with two integers offset and limit . ? range=<offset>-<limit> The offset integer defines where you want to start to fetch results and the limit integer defines where you want to stop . q ( string ) â\\x80\\x93 The q parameter is used for searching . It takes a comma-separated list of property : value . sort ( string ) â\\x80\\x93 The sort parameter allows to sort on specific fields . It takes a comma-separated list of property : sort . Property can be sort only by ascending ( asc ) or descending ( desc ) order . Example request : GET /notification HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 OK . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json [ { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } ] 206 Partial Content â\\x80\\x93 The argument range is used and the server is successfully fulling a range request . 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 422 Unprocessable Entity â\\x80\\x93 Semantically erroneous arguments . Response Headers : Content-Range â\\x80\\x93 Indicates where in a full body message a partial message belongs . Content-Range : <start>-<end>/<size> Accept-Range â\\x80\\x93 Indicates the unit that can be used to define a range . Accept-Range : <index> 20 Link â\\x80\\x93 Links to the other ranges . GET /notification/{id}Â Access to a notification by id Parameters : id ( integer ) â\\x80\\x93 id of the notification Example request : GET /notification/{id} HTTP/1 . 1 Host : example . com Status Codes : 202 Accepted â\\x80\\x93 Ok . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } 404 Not Found â\\x80\\x93 ID not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } PUT /notification/{id}Â Update a notification by id Parameters : id ( integer ) â\\x80\\x93 id of the notification Example request : PUT /notification/{id} HTTP/1 . 1 Host : example . com Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } Status Codes : 202 Accepted â\\x80\\x93 Ok . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 ID not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } POST /notification/{id}Â Create a notification with specific id Parameters : id ( integer ) â\\x80\\x93 id of the notification Example request : POST /notification/{id} HTTP/1 . 1 Host : example . com Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } Status Codes : 202 Accepted â\\x80\\x93 Ok . Example response : HTTP/1 . 1 202 Accepted Content-Type : application/json { \"acknowledged\": true , \"criticality\": 1 , \"description\": \"string\", \"notificationId\": \"string\", \"infos\": {}, \"priority\": 1 , \"site\": \"string\", \"source\": [ 1 . 0 ], \"timestamp\": 1 , \"notificationType\": \"string\", \"title\": \"string\" } 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 403 Forbidden â\\x80\\x93 ID is already used . Example response : HTTP/1 . 1 403 Forbidden Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } DELETE /notification/{id}Â Delete a notification by id Parameters : id ( integer ) â\\x80\\x93 id of the notification Status Codes : 204 No Content â\\x80\\x93 Ok . No data return . 404 Not Found â\\x80\\x93 ID not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } PUT /notification/{id}/ackÂ Acknowledge a notification by id Parameters : id ( integer ) â\\x80\\x93 id of the notification Status Codes : 202 Accepted â\\x80\\x93 Ok . 400 Bad Request â\\x80\\x93 Input data is malformated . Example response : HTTP/1 . 1 400 Bad Request Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 404 Not Found â\\x80\\x93 ID not found . Example response : HTTP/1 . 1 404 Not Found Content-Type : application/json { \"code\": 1 , \"message\": \"string\" } 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = Administrastion Manual, Title = Other Modules, Chapter = Additional modules) These modules can be provided as an option to Skyminer and delivered separately . Epoch NextGen archive datastore 2014-2023 , Kratos Communications Skyminer Administration Manual - KC-153-MA-0019 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer User Manual) This documentation contains the Skyminer userâ\\x80\\x99s manual . Notices regarding this document Introduction Key factors Start page License Query User interface Web interface Legacy Web interface Query components Aggregators Event manager Filtering Grouping Limits Data Types Metrics Outlier Detection Predictors Retrieval of last known value of data before query start Time Tag filtering Time override Vertical aggregators Query view General API Examples Correlations User Interface Overview Building a correlation query Analyzing results Correlation API Correlation Search API Correlation Matrix API Details on similarity measures Discarding linear correlations DTW References Dashboards User Interface Home page Grafana Menu Dashboards panel Dashboard page Specific usages Creating a new graph Select visualization types Skyminer Specific visualization panels Creating a singlestat Fetch previous sample Predictions Skyminer annotations query Templated queries Analytics User Interface Creating a notebook Creating a query Editing a query Skyminer extensions Skyminer query forwarding Skyminer WEBUI integration HideCode Jupyter Extension Skyminer Time Series Python Connector Install Quickstart Modules Examples Reporting Overview Presentation Basic architecture How to use skyminer BIRT Reports Walkthrough Create a simple data set Query builder Parameterize your time range Text data type Example report Appendix FAQ What is Skyminer ? Why are time series not aligned ? Is the data processed between the source and the display on Skyminer ? How much data can be displayed on Skyminer ? How can I export data from Skyminer ? How do I align time series to time boundaries ( e . g . every start of hour )? How to Rename a Time Series to the Time Unit after group by Calendar ( e . g . December for month_of_year )? 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Notices regarding this document) The information contained in this document is considered reliable and subject to change without notice . Illustrations and screenshots may not represent the latest software version but are included to best explain related text . The information in this document does not represent any commitment or warranty on the part of KratosÂ®. No part of this document or any part of the software described herein may be reproduced by any means without the express written consent of Kratos . For the purposes of these disclaimers , â\\x80\\x9cKratosâ\\x80\\x9d refers to Kratos Communications , Inc . and its wholly owned subsidiaries . Software License Agreement : Software is defined as the computer programs with which the Software License Agreement is included and any maintenance or update releases thereto . This Agreement sets forth the terms and conditions for licensing of the software , and installing the software indicates that the Agreement has been read and understood , and users accept its terms and conditions . MonicsÂ®, EpochÂ®, CompassÂ® and satIDÂ® are registered trademarks of Kratos Communications , Inc . All other trademarks are property of their respective owners . Copyright Â© Kratos Communications , Inc . All Rights Reserved . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Introduction, Chapter = Key factors) Scalability : Skyminer can be scaled out with no limit , either on-premises or in a public or private cloud platform . Start with a small system and increase its capacity as needed , thus delaying costs and making it possible to get cheaper and better resources later when an upscaling is actually needed . Flexibility : The Skyminer solution is highly modular , and can evolve dramatically without affecting existing operations . Features and capabilities can be provided as plugins , and you can define the data management policies that you need . Skyminer natively operates with other Kratos software , but is also able to support integrations with third-party or bespoke systems . Performances : Skyminer is able to deal with a large amount of data , while providing outstanding read and write performances . User-Friendly : The features available in Skyminer are powerful but intuitive , self-explanatory , and easy to use . There is no need for any programming skills : all users can benefit from the simplicity of the approach and the powerful features . Data scientists and analysts benefit from the unique integrations with Python , Jupyter or R that provide fast analysis feedback loops . Openness : Skyminer provides web services APIs and can be seamlessly integrated with third party systems and solutions . It relies on first class , field-proven , open-source technologies with friendly licenses so you can keep your data safe without being tied to a particular vendor or provider . Skyminer added value resides in its query engine , its integration capabilities , and the unique third-party integrations delivered out of the box .',\n",
       " 'Context : (Documentation = User Manual, Title = Introduction, Chapter = Start page) The start page of Skyminer is composed of a header and five tiles linking to different features : Time Series Query : Generate , execute , and display the result of Skyminer queries Time Series Dashboards : Create and monitor real-time dashboards in Grafana Analytics : Execute documents in Python to run advanced analytics and generate reports Documents Dashboard : Search backend , log analytics , real-time application monitoring Time Series Correlations : Generate , execute , and display the result of correlation queries',\n",
       " 'Context : (Documentation = User Manual, Title = Introduction, Chapter = License) Skyminer needs a license file to enable all its functionalities . The validity of the license can be checked in the About section of the Query page : Note An invalid , missing or expired license induces a limitation of the system . If you have any problem , please contact your system administrator . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Query) The query is the operation that fetches data from the database , executes some processing ( aggregation , predictionâ\\x80¦), and returns the result in the form of time series .',\n",
       " 'Context : (Documentation = User Manual, Title = Query, Chapter = Query components) Aggregators Event manager Filtering Grouping Limits Data Types Metrics Outlier Detection Predictors Retrieval of last known value of data before query start Time Tag filtering Time override Vertical aggregators Query view',\n",
       " 'Context : (Documentation = User Manual, Title = Correlations) User Interface Overview Building a correlation query Analyzing results Correlation API Correlation Search API Correlation Matrix API Details on similarity measures Discarding linear correlations DTW References 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Dashboards) Skyminer provides a dashboard tool called Grafana . Grafana allows you to create dashboard and interactive reports for metrics from monitoring systems and time series data . A special plugin has been developed to monitoring data directly from skyminer . The OpenSearch built-in datasource is enabled and configured to be able to query the index named skyminer-cassandra-ts-index by default .',\n",
       " 'Context : (Documentation = User Manual, Title = Dashboards, Chapter = Specific usages) Creating a new graph Select visualization types Skyminer Specific visualization panels Creating a singlestat Fetch previous sample Predictions Skyminer annotations query Templated queries 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Analytics) Analytics refers to advanced data processing and report generation via Jupyter . Jupyter is an open-source application used to create and share documents ( notebooks ) containing Python code and visualization . The Skyminer extensions for Jupyter provide multiple functionalities : Fetching data from Skyminer in Jupyter notebooks Forwarding data from the Skyminer query UI to predefined data processes Hiding elements of notebooks for report generation Analytic features details : User Interface Creating a notebook Creating a query Editing a query Skyminer extensions Skyminer query forwarding Skyminer WEBUI integration HideCode Jupyter Extension Skyminer Time Series Python Connector Install Quickstart Modules Examples 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Overview, Paragraph = Presentation) BIRT is a widely used report designer . It can be used to create a large variety of reports and connect them to multiple data sources . The Skyminer BIRT plugin makes it possible to import data directly from Skyminer into BIRT .',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Overview, Paragraph = Basic architecture) The Skyminer BIRT plugin is a Java client that uses the Skyminer query web services . To help the user build queries for the reports , it uses the Skyminer query Web UI in a Java WebView .',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Create a simple data set) It works quite the same as with other BIRT data sources . Create a new report with File>New>Other>Business Intelligence and Reporting Tool>Report In the Data Explorer tab right click on Data Sources to create a Skyminer Data Source When your data source has been created , right click on Data Sets to create a new data set with a query to Skyminer Once you have chosen the name of your data set and clicked on Next , the Skyminer Web UI should appear . You can build your query and generate it by clicking on Graph or Generate query . You can either validate your dataset or use the other tab to parameterize your query . You can now use this data in all your BIRT tables and charts . A comprehensive BIRT tutorial can be found on the official webpage : http ://eclipse . org/birt/documentation/tutorial/index . php',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Query builder) You can edit your query to Skyminer at any time . In the data set editor , in the Query tab , you can either use the Query Builder if you want to generate an entirely new query , or modify the Json text in the Raw Query pane if you simply want to edit your query . Note : it is important to remember that start and end time attributes will be ignored if Set time as a parameter is checked in the Time range parameters tab .',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Parameterize your time range) Note on BIRT parameters : Parameters are values that are asked from the user at runtime to customize their reports . There are two kinds of parameters in BIRT , data set parameters and report parameters . Report parameters are those that are prompted from the user . Data set parameters are defined in the data set and must be linked to a report parameter to receive the userâ\\x80\\x99s input at runtime . You might want your start and stop time to appear as two BIRT parameters . To do so , you simply have to go to the Time range parameter pane when you are building your query and check the Set time as a parameter option . This creates two dataset parameters that you can bind to your report parameters in the Parameters tab . Go to the Parameters tab in the data set editor window , select one of the two parameters you have and select Edit . You can configure the link to a report parameter with the Linked to Report Parameter field . You should then do the same with the other parameter , except if you are satisfied with just a default value . This allows you to be able to define the start and stop time of this dataset at runtime , so you can define a new time range each time you run your report without modifying your dataset configurations . These parameters use a natural language date parser , which allows you to enter your dates in a very flexible manner . You can use standard formats such as Thu , 04 Dec 2014 13 : 36 : 45 GMT , but also natural language formulas such as Yesterday at 8 am . Only the European format that writes dates like 4th of January as 04/01 is not supported . Here are a few examples of dates that would work : Last Monday at 8 Thu , 04 Dec 2014 13 : 36 : 45 GMT 10 hours ago December 5th at 8 : 05 : 14 EDT For more examples on accepted dates , please refer to the list in the Appendix .',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Text data type) If your series is not just numbers , you might want to look into the Value Type pane of the Query tab of the data set editor window . By default if your time series has text values they will be set to null .',\n",
       " 'Context : (Documentation = User Manual, Title = Reporting, Chapter = Walkthrough, Paragraph = Appendix) Accepted dates : 1978-01-28 1984/04/02 1/02/1980 2/28/79 The 31st of April in the year 2008 Fri , 21 Nov 1997 Jan 21 , â\\x80\\x9897 Sun , Nov 21 jan 1st february twenty-eighth next thursday last wednesday today tomorrow yesterday next week next month next year 3 days from now three weeks ago Accepted times : 0600h 06 : 00 hours 6pm 5 : 30 a . m . 5 12 : 59 23 : 59 8p noon afternoon midnight 10 seconds ago in 5 minutes 4 minutes from now Accepted time zones +0500 -08 : 00 UTC EST EDT ET CST PST PDT PT MST AKST HAST 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = What is Skyminer?) Skyminer system is a Big Data storage and analytics engine integrated with Kratos products , systems and solutions . It allows its users to store billions of samples with different data types over time , while maintaining efficient storage and outstanding write and read performances . Skyminer provides features to analyze data over time , organisational , or geospatial dimensions within and/or between data series .',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = Why are time series not aligned?) Different time series donâ\\x80\\x99t necessarily start and end at the same time . You can use the Time Align aggregator to align the start time of the data . You can also use statistical aggregation , interpolation , or the resample aggregator to get time series with samples aligned over time .',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = Is the data processed between the source and the display on Skyminer?) By default there is no process or filter applied to the data before displaying it on Skyminer . However you can choose to process the data in the query page .',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = How much data can be displayed on Skyminer?) By default there are 3 safeguards for the size of data displayed on Skyminer : A limit of 100 series ( group by ) A limit of 10 000 000 points before aggregation A limit of 10 000 points after aggregation These can be overridden by clicking on the padlock icon on the right of the query interface , but be aware that plotting a large number of points is rarely informative and causes performance issues .',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = How can I export data from Skyminer?) You can export the data to a JSON or CSV file by clicking on the respective button on the right of the query page . It is also possible to send the data to Jupyter in order to process it with Python scripts and to generate a PDF report by using Jupyter and the Skyminer extensions . Other export capabilities exist , contact Kratos for more information .',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = How do I align time series to time boundaries (e.g. every start of hour)?) Using aggregators : this can be achieved by selecting the Align Sampling option , in combination with either Align Start Time or Align End Time . The Resample aggregator also provides this capability without the need to compute statistics ( using last known value , or nearest timestamp value ).',\n",
       " 'Context : (Documentation = User Manual, Title = FAQ, Chapter = How to Rename a Time Series to the Time Unit after group by Calendar (e.g. December for month_of_year)?) Using both the Legend aggregator and Js Alias aggregator : Apply the Legend aggregator to your time series and select the Remove group-by info and Rename series options . Apply the Js Alias aggregator to your time series and use the following script : value . match (/name=( w+)/)[ 1 ]. 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = The query builder) Header , contains the logo and the version number of the deployed release . The logo is clickable and redirects the user to the index of Skyminer Web interface . The tabs to switch between Time Range and Event Manager Time range of the query , it can be either a time relative to the current date ( for instance 1 hour ago ) or an absolute date by picking in the calendar . The event builder , it is a tool that allows you to duplicate metrics to fetch the data at different times . Previous samples , this option is designed to retrieve the last known value of the query . Metrics section , a single query can have different metrics or query the same metric with different processing . This section can be toggled . The + button allows you to add a new empty metric to the list . The query view , thanks to a double data binding the json query is automatically built during the edition . The right buttons allow you to share , save or load a query . Link to the Skyminer User Manual Link to the Skyminer legacy Web interface This section provides multiple ways to run a query : Execute the query and display the result data directly in the Skyminer web UI ( refer to Select the view used to display the query results ) Save the data in Json format Save the data in CSV format Transfer the data to a Jupyter notebook for advanced processing . The notebook must be located located in the â\\x80\\x98Processorsâ\\x80\\x99 folder of Jupyter for it to be listed in the Skyminer UI . For more information , refer to Analytics . These buttons can only be clicked when all the data from the query is valid . Finally , the padlock icon can be clicked to bypass the safeguards and submit exceptionally large queries . The history icon can be clicked to visualize the executed queries history .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Executed queries history) The history allows you to : Visualize the up-to-500 last executed different requests Search for keywords in the query ( filters the table ) Import the selected query in the UI ( similar to the json query editor ) Remove a query from the history Clear the entire history Tag a query as favorite Favorite queries are not limited in time and number .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Metric advanced search) The metric advanced search allows you to : Search for keywords and visualize the matching metric list and their matching tags Set the metric name by selecting a metric in the table',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Feature search) The feature search allows you to : Visualize the feature list and their description Search for keywords ( filters the table ) Import the selected feature in the UI',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Metric processing chain replication) The metric processing chain replication allows you to replicate the processing chain from a metric to the selected ones . It includes : Time override Tag filters Group-by Aggregators Vertical aggregators Predictors Outlier detectors Please keep in mind that replicating a processing chain will replace the selected metricsâ\\x80\\x99 own processing chains ( the current processing chain will be lost ).',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Select the view used to display the query results) The view type selector defines which view is used to display the gathered data . There are 3 types of view : Graph type : a graph will be generated displaying the different obtained series ( as curves , refer to Generated graph ). Table type : a data table will be generated displaying the samples for the different obtained series ( refer to Generated table ). Constellation type : a constellation chart will be generated ( refer to Generated constellation diagram graph ). Spectrum legacy type : a spectrum chart will be generated displaying the different traces ( legacy one , refer to Generated spectrum graph - legacy ). Spectrum type : a spectrum chart will be generated displaying the different traces ( refer to Generated spectrum graph ). Spectrum & waterfall type : a spectrum chart with waterfall visualisation will be generated displaying the different traces ( refer to Generated spectrum with waterfall graph ).',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Display the query results sample histogram) The sample histogram shows the sample distribution over time . Several options are available : Bin time value and unit : you can configure the â\\x80\\x98binsâ\\x80\\x99 timespan . It is firstly automatically determined using the number of groups and the query time range Bin group predicate : there are 3 types of histogram group predicate : Keep groups : a graph will be generated showing the sample distribution by different obtained series . Group by metrics : a graph will be generated showing the sample distribution for each metric name ( or defined alias ). No group : a graph will be generated showing the overall sample distribution . Clicking on â\\x80\\x98binsâ\\x80\\x99 will focus the query results view on the clicked â\\x80\\x98binâ\\x80\\x99 timespan : Graph : a zoom will be applied on the time axis respecting the â\\x80\\x98binâ\\x80\\x99 bounds Table : data is filtered by time using the â\\x80\\x98binâ\\x80\\x99 bounds You can reset the focus timespan to the query time range by clicking on the â\\x80\\x98Resetâ\\x80\\x99 button .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated graph) If the Graph view type is selected , the resulting series are plotted . The indicator called Sample Size represents the number of data points that were processed by the query , before aggregation . Data Points are the objects actually returned in the response and plotted after aggregation . Plot mode bar allows you to customize the displayed plot . There are ten buttons : Trace type : you can choose to display the plot in SVG or WebGL . WebGL is much more efficient than SVG but may be not supported by all browsers . By default the plot will be displayed in WebGL if the number of data points exceeds 20 000 . Lines mode : you can choose to display lines on the plot , this option is activated by default . Markers mode : you can choose to display markers ( points ) on the plot , this option is also activated by default until 5 000 data points count . After this limit displaying markers can considerably affect plot rendering . Set query timerange : you can define the query timerange from the currently displayed timespan . That eases the focus on a specific area of the plot . Download plot as png . Autoscale : you can reset axis and scale with this button . Zoom In . Zoom Out . Show closest data on hover : this option displays additional information about the data point closest to your cursor . Compare data on hover : this option is activated by default , it allows you to see tooltips for each of your series at a given time in order to compare their value more easily . The legend of the plot is interactive , you can choose to hide or display series by clicking on their name . You can get a link to the graph . It is a way to save the query and re-execute it at any time by pasting the URL in your web browser . A link to the table is also available , it allows you to see last values for your metric in a table view . The last option is to run correlation analysis with the query you created . This button allows you to back to the query editor . Important remark : when building a query , you should always be careful to aggregate your series in a way not to return too many data points . Average and sum are often the most common aggregators that allow you to reduce the number of data points displayed for a given metric over a given time frame , thus circumventing the max point limitation . Plotting a large number of points is rarely informative . A limit has been set to 10 000 data points after aggregation in the query safeguard .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated table) If the Table view type is selected , the resulting series are displayed in a data table . The Sample Size indicator represents the number of data points that were processed by the query , before aggregation . Data Points are the objects actually returned in the response and plotted after aggregation . Search and filters allow you to search for data or filter the query results : Search : display data with a least one field matching the search term . Displayed metric ( s ): you can choose to display all metrics or select the one you want to display . Displayed group ( s ): you can choose to display all groups for the selected metric ( s ) or select the one you want to display . The table header is interactive , you can sort the series by column values . To do so , click on the column header cell . You can get a link to the graph . It is a way to save the query and re-execute it at any time by pasting the URL in your web browser . A link to the table is also available , it allows you to see the last values of the metric in a table view . The last option is to run correlation analysis with the query you created . This button allows you to go back to the query editor .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated constellation diagram graph) If the Constellation view type is selected , the resulting series are displayed in a constellation graph . Select metric / group / number of symbols for eye diagram Constellation diagram Eye diagrams Selection of charts to display Chart showing in phase vs quadrature through time Chart showing phase through time Chart showing amplitudes through time Viewport for selecting visible time in the charts within the current sample ( showing miniature of current charts of selected capture ) Sample player that allows to navigate through captures made over time and/or manage playback',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated spectrum graph - legacy) If the Spectrum legacy view type is selected , the resulting series are displayed in a spectrum graph . Input filters : you can select the traces to display on the spectrum graph . Options of the spectrum graph : Controls : play , pause , stop , next , â\\x80¦ Speed ( x1 , x2 , x5 , â\\x80¦) Unit ( Hz , kHz , MHz , GHz ) Frequency filter ( min/max ) Power filter ( min/max ) Timeline : allows navigation through timestamps to display the desired trace .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated spectrum graph) If the Spectrum view type is selected , the resulting series are displayed in a spectrum graph . Input filters : you can select the traces to display on the spectrum graph . Options of the spectrum graph : Unit ( Hz , kHz , MHz , GHz ) Frequency filter ( min/max ) Power filter ( min/max ) Display options : Controls : play , pause , stop , next , â\\x80¦ Speed ( x1 , x2 , x5 , â\\x80¦) Timeline : allows navigation through timestamps to display the desired trace .',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Generated spectrum with waterfall graph) If the **Spectrum and Waterfall* view type is selected , the resulting series are displayed in a spectrum graph . Spectrum graph at selected time Display graph of power vs time at selected frequency Waterfall graph ( allows interactive selection of a specific frequency and instant ) Input filters : you can select the traces to display on the spectrum graph . Visualization settings : Power levels Style of waterfall chart Time resolution of waterfall chart ( value and unit ) Style of power vs time chart Select the main trace ( Curve data on spectrum analyser + waterfall + powerchart data ): Selector main trace explorer Selector main curve color Select the secondary traces ( secondary curves data on spectrum analyser only ): Secondary traces selector explorer Secondary traces chip Secondary traces list Secondary traces warning message Secondary traces main curve switching ( by clicking on secondary chip ) Timeline : Controls : play , pause , stop , next , â\\x80¦ Speed with two levels of controls : unit scale ( x1 , x2 , â\\x80¦) and time resolution scale ( sec , min , â\\x80¦)',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Traces selector explorer) Allow to select the main & secondary traces to display on the advanced spectrum and change the curves colors . Informations on current query Metric trace dropdown selector Chip list of current selected series ( Metric + Group , that represented by a data curve ) Auto scroll on more than 3 lines Delete button Color of the curve Table of group related to previous metric Select all column Color curve selector Select group ( click or check ) Group attributes informations ( column ) Actions control Reset all series ( Metric + Group ) selected Confirm selection of secondary traces Close without action',\n",
       " 'Context : (Documentation = User Manual, Title = Web interface, Chapter = Editor Mode) The query builder can be used in editor mode , notably in BIRT Electron app and in Correlation UI iframe . This mode disables some features : Export to CSV Jupyter Run correlation analysis Open graph in new tab Open table view 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Legacy Web interface, Chapter = The query builder) Time range of the query . It can be expressed either as a time relative to the current date ( for instance 1 hour ago ) or as an absolute date by picking in the calendar . The event builder . It is a tool that allows the user to duplicate metrics to fetch the data at different times . Previous samples , this option is designed to retrieve the last known value of the query . Add or copy a metric . A single query can have different metrics or query the same metric with different processing . When you choose to copy a metric , the selected one is copied . This field is the name of the metric to query . A metric is a group of data points , typically referring to a specific input ( example : temperature , cpu usage â\\x80¦). Inside a given metric , different series can be differentiated by tags . Auto-completion in the text field helps you to find stored metrics . It is the only mandatory field of the query . Tag filtering allows you to select pairs of tag name and value that you want to filter against . All the data points that do not have these tags will be discarded . Time Override Group By allows you to split your metric into different series . Tag grouping separates the data points into different series according to the value of a given tag . Time grouping separates the data points into series according to time intervals . The count attribute specifies the number of time groups . For instance a Target Size of 1 day with a group count of 7 will put all the data points of every day of the week into a different group . Value grouping separates the data points into groups according to their value . The Target Size attribute must be a positive integer . It specifies the size of the interval of each value group , the first group starting at 0 . Horizontal aggregation will apply mathematical operations on each group independently . Some aggregators ( like mean , sum , averageâ\\x80¦) diminish the number of samples by creating summary statistics on time intervals . They are crucial when querying frequently sampled time series on long time intervals , otherwise the number of points returned by the server becomes huge and cannot be handled by the server . Vertical aggregation applies operations between time series groups . They are important when you have a large number of groups after the grouping operation . Predictors are forecasting tools . They return series that have timestamps after the timestamp of the original series . You can choose to either graph the data directly in the web UI or to save the text result . You can save it either in Json format , which is the original format of the response returned from the server , or in CSV , for further processing . The lock button defines limits on the query . The user can change these limits to be able to submit exceptionally large queries .',\n",
       " 'Context : (Documentation = User Manual, Title = Legacy Web interface, Chapter = The resulting query) Once the query is executed ( either graphed or saved ), the query in Json format ( by default , you can also choose to display it in js Object ) appears in the text box . You can decide to create a graph directly from a JSON query without going through the query builder .',\n",
       " 'Context : (Documentation = User Manual, Title = Legacy Web interface, Chapter = Generated graph) If the graph button has been clicked , the resulting series are plotted . You can get a link to the graph . It is a way to save the query and re-execute it at any time by pasting the URL in your web browser . A link to table is also available , it allows you to see the last values of your metric in a table view . The indicator called Sample Size represents the number of data points that were processed by the query , before aggregation . Data Points are the objects actually returned in the response and plotted after aggregation . Important remark : when building a query , you should always be careful to aggregate your series in a way that there are not to many data points that are returned . Average and sum aggregators are often the most appropriate to concentrate the database into shorter time series . Plotting a large number of points is rarely informative . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Overview) Aggregators perform operations on data points , like downsampling . For example , you could sum all data points that exist in 5 minute periods . Aggregators can be combined together . For example , you could sum all data points in 5 minute periods and then average the results over a period of one week . Aggregators are processed in the order specified in the query . The output of one is sent to the input of the next . If no aggregator is specified , then all datapoints that match the tag filtering are returned .',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Description) There are many different aggregators available in Skyminer . Note : The default aggregator option selected in the UI is Average',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Description, Paragraph = The downsampling aggregators) Downsampling allows you to reduce the sampling rate of the data points and aggregate these values over a longer period of time . For example , you could average all daily values over the last week . Instead of getting 7 values , you would only get the average over the whole week . Avg - returns the average value Std - returns the standard deviation Percentile - Calculates a probability distribution and returns the specified percentile for the distribution . The â\\x80\\x9cpercentileâ\\x80\\x9d value is defined as 0 < percentile <= 1 where . 5 is 50% and 1 is 100% Least squares - returns two points for the range which represent the best fit line through the set of points . Max - returns the largest value Min - returns the smallest value Minmax - returns both min and max Sum - returns the sum of all values First - returns the first sample in each range Last - returns the last sample in each range One very important notion with downsampling aggregators is alignment . There are two boolean parameters to downsampling aggregators that help set the alignment : Align sampling : forces the sampling ranges to be aligned based on the sampling size . For example if your sample size is either milliseconds , seconds , minutes or hours then the start of the range will always be at the top of the hour . The effect of setting this to true is that your data will take the same shape when graphed as you refresh the data . Align start time : the aggregated point for each sampling range is aligned to the start of the range .',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Description, Paragraph = Other aggregators) Some aggregators offer other kinds of processing without reducing the number of datapoints . Rate : ratio between the difference in value and timestamp of two successive datapoints . Takes a unit parameter that tells how to calculate the ratio ( ie rate in seconds , milliseconds , minutes , etcâ\\x80¦). Div : Divides all data points by a chosen factor . Scale : same as div but with a multiplying factor . Sampler : computes the sampling rate of change for the data points . Diff : computes the difference between successive data points . Gaps : marks gaps in data according to sampling rate with a null data point . Interpolation : generates a regular sampling at the specified rate using linear interpolation . Alias : changes the metric name . Any name is possible . Tag : adds a tag to all the datapoints returnes by the query . If the tag already exists , it adds the new value to the list of values for that tag . Untag : if neither tag name nor value is specified , removes all tags . If tag name is specified but not the value , removes that tag . If both the name and value are specified , removes the specified value only . Time align : shifts timestamps so the start of the time override matches the start of the reference time , that is to say the start time for the whole query . If align_to_end is true the end time of the time override will match the end of the reference time instead . Reference start or stop time can be overridden if one does not want the time of the query to be used . Time shift : shifts the timestamp of the specified amout of millisecond . Use a negative integer to shift backward . Limit : throws an exception if the specified number of datapoints is overstepped . This is very useful to avoid building excessively long queries . Normalize : Returns the normalized data using the Z-normalization algorithm . The resulting data will be scaled and centered around the series standard deviation in order to have a mean value close to zero . â\\x80\\x9d + A Minimum of two points are necessary to compute the normalization , and this aggregator will throw an exception otherwise',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Description, Paragraph = Javascript aggregators) JavaScript aggregators allow the user to create custom aggregators without modifying the client . You can either write an entire script that terminates with a return statement , or simply provide a single lined script . There are four sorts of JS aggregators : JS Filter : allows to filter values according to a scripted statement . For instance the simple script â\\x80\\x9cvalue !=0â\\x80\\x9d will filter out every values that are different to 0 . Current value is accessed through the value variable and current timestamp through timestamp . JS Function : transforms every value according to a given function . For instance â\\x80\\x9cvalue*2â\\x80\\x9d will multiply every value by 2 . Current value is accessed through the value variable and current timestamp through timestamp . JS Range : allows to create downsampling aggregators . For instance to recreate the sum aggregator one would write : var sum = 0 ; while ( values . hasNext ()) { var dp= values . next (); sum+=dp . getDoubleValue (); } return sum ; The variable values allows you to access to an iterator over the datapoints within the sub range . Use hasNext () and next () to iterate over the datapoints . Use getTimestamp () to access the timestamp of the datapoint and getDoubleValue () to access its value . JS Regression : is a particular type of range aggregation . It allows to access to the SimpleRegression library from the Apache Commons Math 3 library . You access the regression object with the variable named regression . You can then call functions such as regression . getSlope () or regression . getRSquare (). The following script returns the ratio between the regression sum squares and the error sum squares under some thresholding condition : if ( regression . getN () <100 || regression . getSumSquaredErrors () == 0 ) return 0 ; return regression . getRegressionSumSquares () / regression . getSumSquaredErrors ()',\n",
       " 'Context : (Documentation = User Manual, Title = Aggregators, Chapter = Access to Aggregator documentation in Skyminer UI) To access the aggregators documentation , click on the information button in the Skyminer Query interface . Click on the link to â\\x80\\x9cFeatures documentationâ\\x80\\x9d The following window will open , select the tab Aggregator to see details about each aggregator , their use and parameters Error messages Type error : the type returned by the script is not a number { \"errors\": [ \"Script result should be number , got class java . lang . String\" ] } Syntax error : the JavaScript is not valid { \"errors\": [ \"query . metric [ 0 ]. aggregators [ 1 ]. m_script has an invalid syntax :[ line & column number ] [ Syntax error message ]\" ] } Allocation limit : the number of points in the range is larger than the maximum size that can be allocated , defined by the skyminer . script_agregator . max_batch property in the configuration file ( only occurs if allocate_array is set to true ) { \"errors\": [ \"Number of points in aggregation range exceeds configured limit of 1000\" ] } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Event manager, Chapter = Overview) The event manager helps the user in this task by automatically duplicating specified metrics and adding the relevant time overrides , aliases and time aligns .',\n",
       " 'Context : (Documentation = User Manual, Title = Event manager, Chapter = Description) This feature is optional , it can be enabled or disabled by clicking on the appropriate toggle button . The toggle button that allows the user to enable or disable the feature Event manager can be configured with the UI ( by default ) or from a CSV file Rename metrics with aliases corresponding to the event name ( by default set to true ) Tag metrics ( by default set to true ). When set to true , the metrics will be returned with the tag â\\x80\\x9c_eventâ\\x80\\x9d which has the name of the event as its value Apply a time align to data from events ( by default set to Align start ) No align : The timestamp of data is not modified Align start : The timestamp of each data point is shifted as if all events had begun at query start time Align end : The timestamp of each data point is shifted as if all events had ended at query end time Add an event , the user can add as many events as they need UI for building an event . An event has a name and a time range that works as all the other time ranges of the UI Load event from CSV . Note that changing form CSV ` to `Build with UI automatically translates all events from CSV in the UI',\n",
       " 'Context : (Documentation = User Manual, Title = Event manager, Chapter = Usage in API) The event manager is specified in an object at the root of the query with the key event_sourcing . It has four parameters . rename_metrics : true or false apply_tag : true of false time_align : there is three possible values , no_align , align_start or align_end events : an array of event An event has two parameters . name : the name of the event time range start_relative : The relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . start_absolute : The time in milliseconds . end_relative : The relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to be the current date and time . end_absolute : The time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . Note : You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If neither end times are specified the current date and time are used . { \"start_relative\": { \"value\": \"12\", \"unit\": \"hours\" }, \"metrics\": [ { \"name\": \"metric 1\" } ], \"event_sourcing\": { \"rename_metrics\": false , \"apply_tag\": true , \"time_align\": \"align_start\", \"events\": [ { \"name\": \"event 1\", \"timerange\": { \"end_absolute\": null , \"start_relative\": { \"value\": \"3\", \"unit\": \"hours\" } } } ] } } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Filtering, Chapter = Overview) It is possible to filter the data datapoints by specifying one or more tags . In that case , the data returned will only contain data points associated with the specified tags .',\n",
       " 'Context : (Documentation = User Manual, Title = Filtering, Chapter = Description) You can specify multiple values for a given tag name to accept datapoints having any of these values . If multiple tag names are specified , only the datapoints that match the values for each of these tag names are returned . For example the filter above will accept datapoints for customer 1 or 2 that are on antenna 1 . A tag explorer tool helps the building of tag filtering .',\n",
       " 'Context : (Documentation = User Manual, Title = Filtering, Chapter = Usage in API) Filtering is done using the tags property in the metric . It is an object where property names are tag names and with arrays of values . ... \"tags\": { \"customer\": [\"customer1\", \" customer2\"], \"antenna\": [\"antenna1\"] } ... 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Grouping, Chapter = Overview) The resulting data points can be grouped by one or more tags , a time range , or by value , or by a combination of the three .',\n",
       " 'Context : (Documentation = User Manual, Title = Grouping, Chapter = Description) You can group results by specifying one or more tag names . For example , if you have a customer tag , grouping by customer would create a resulting object for each customer . Multiple tag names can be used to further group the data points . You can set a limit to the number of groups to be returned . The time grouper groups results by time ranges . For example , you could group data by day of week . Note that the grouper calculates ranges based on the start time of the query . So if you wanted to group by day of week and wanted the first group to be Sunday , then you need to set the queryâ\\x80\\x99s start time to be on Sunday . The value grouper groups by data point values . Values are placed into groups based on a range size . For example , if the range size is 10 , then values between 0-9 are placed in the first group , values between 10-19 into the second group , and so forth . Note : The default group by option selected in the UI is Tags',\n",
       " 'Context : (Documentation = User Manual, Title = Grouping, Chapter = Usage in API) The group_by property in the metric part of the query is an array of one or more groupers . Each grouper has a name ( tag , time or value ) and then additional properties specific to that grouper . Tag group by : tags attribute with a list of tag names Time group by : group_count with an integer and range_size with a range object ( time value + time unit ) Value group by : value_size with an integer ... \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"customer\", \"antenna\"] } ] ... Note that grouping by a time range or value can slow down the query . Note also that the group limit is a metric attribute and not a group by attribute . Response Each object of the response JSON contains the group_by information you specified in the query as well as a group object . The group object contains the tags names and their corresponding values for the particular grouping . The first group in the results below include data points for the dc1 data center and server1 host . { \"queries\": [ { \"results\": [ { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"data_center\", \"host\"], \"group\": { \"data_center\": \"dc1\", \"host\": \"server1\" } } ], \"tags\": { \"data_center\": [\"dc1\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353222000000 , 31 ], [ 1364796000000 , 723 ] ] }, { \"name\": \"metric1\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"data_center\", \"host\"], \"group\": { \"data_center\": \"dc2\", \"host\": \"server1\" } } ], \"tags\": { \"data_center\": [\"dc2\"], \"host\": [\"server1\"] }, \"values\": [ [ 1353222000000 , 108 ], [ 1364796000000 , 1318 ] ] } ] } ] } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Overview) When querying Skyminer , the data returned can be very large . To prevent the response time from being excessively long , it is usually a good idea to limit the number of points returned by Skyminer .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Description) Queries can become overwhelming either horizontally , when there are too many datapoints for one of the metrics returned , or vertically , when a group by returns too many groups .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Description, Paragraph = Horizontal limits) There are two mechanism to limit the number of datapoints in each series returned . You can use the limit parameter of the metric . This will stop returning points after the limit is reached , but will not throw an exception . You can use the limit aggregator . This will stop returning points and return an error message if the limit is reached .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Description, Paragraph = Safeguard) Safeguard is a mechanism used to facilitate the usage of horizontal and vertical limits . In the query UI , the padlock button can be used to define 3 parameters : Number of series : maximum number of groups returned by a group_by ( vertical limit ) Limit before aggregation : maximum number of datapoints for each series , before aggregation ( horizontal limit ) Limit after aggregation : maximum number of datapoints for each series , after aggregation ( horizontal limit ) These limits are set by default to avoid accidental disproportionally large queries .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Usage in API) The different kinds of limits can be set in different ways . This section does not present new API parameters as limit mechanisms have already been introduced in the general presentation of the metric object and in the section on aggregators .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Usage in API, Paragraph = Horizontal limits) The limit aggregator can be used like any other aggregator . The aggregator is named limit and only requires one parameter : limit : the maximum number of datapoints for each series returned . This aggregator throws an error when the limit is reached , and returns a response with status code 500 : { \"errors\": [ \"Limit Aggregator : query exceeded limit of 10000 data points , please increase limits or add filtering/downsampling\" ] } An alternative is the limit attribute , a root attribute of the metric object . It does not throw an error when the limit is reached but stops returning datapoints .',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Usage in API, Paragraph = Vertical limits) The vertical limits are done through the group_limit attribute of the metric . It sets the maximum number of groups returned by the group by . ... { \"tags\": {... tags for filtering ...}, \"name\": \"abc . 123\", \"limit\": 1000 , \"group_limit\": 100 , \"aggregators\": [... a list of aggregators ...] } ... An error message is returned if the limit is overstepped with status code 500 : { \"errors\": [ \"Groups Limit : exceeded limit of 100 groups in a single metric query , please increase limits , change grouping or add filters on tags\" ] }',\n",
       " 'Context : (Documentation = User Manual, Title = Limits, Chapter = Usage in API, Paragraph = Safeguard) Alternatively you can use the safeguard object to define those limits : { \"safeguard\": { \"group_limit\": 100 , \"limit_before_aggregation\": 10000000 , \"limit_after_aggregation\": 10000 }, \"metrics\": [ ... ] } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Data Types, Chapter = Overview) On the select metrics section , you can filter metric names by type . This feature is helpful when the data types associated to metrics is not obvious . For example you can choose only metrics which are spectrum_trace or constellation_diagram type .',\n",
       " 'Context : (Documentation = User Manual, Title = Data Types, Chapter = Description) Click on select data-type dropdown . Select a data type among those availables , by default the selected data-type is â\\x80\\x9cAll data typeâ\\x80\\x9d and displays all metric names . If data type selector is not available ( e . g . Skyminer Time Series Indexer Module is not activated in administration manual ), the feature is not proposed and all metrics are always available . After applying the filter , only metric containing datapoints of selected type can be selected . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Metrics, Chapter = Description) The + button allows you to add a new empty metric to the list . Metric , this section can also be toggled to display metric information or hide it . There are three buttons on the right , the first one allows you to replicate the processing chain to other metrics , the second one to duplicate this metric and the third one to delete this metric . This field is the name of the metric to query . A metric is a group of data points , typically referring to a specific input ( example : temperature , cpu usage â\\x80¦). Inside a given metric , different series can be identified by tags . Auto-completion in the text field helps you to find stored metrics . A metric advanced search is available by clicking the magnifier button . It is the only mandatory field of the query . This line shows to the user whether data are available for the picked metric . If there is data the line shows the time and the value of the first and last sample . Times are clickable and can be used to set the start and the end of the absolute time range . Graphical representation of data presence over the period . Red lines show first and last values , and timeranges with data are displayed . Only shown if time-series-index plugin is enabled . This field can be used to set a limit for metric data . Only the N first data points will be returned by the API . Time Override Tag filtering allows you to select pairs of tag names and values that you want to filter against . All data points that do not match these tags will be discarded . You can add a filter via the tag explorer or with two select inputs . Group By allows you to split your metric into different series . Tag : grouping separates the data points into different series according to the value of a given tag . Bin : grouping data by bins or buckets . Calendar : grouping data by calendar period ( e . g . day of the week , day of the month , year â\\x80¦). Fiscal period : grouping data by fiscal period . Time : grouping separates the data points into series according to time intervals . The count attribute specifies the number of time groups . For instance a Target Size of 1 day with a group count of 7 will put all the data points of each day of the week into a different group . Value : grouping separates the data points into groups according to their value . The Target Size attribute must be a positive integer . It specifies the size of the interval of each value group , with the first group starting at 0 . When a feature is minimized you can still have information on the content . Horizontal aggregation will apply mathematical operations on each group independently . Some aggregators ( like mean , sum , averageâ\\x80¦) diminish the number of samples by creating summary statistics on time intervals . They are crucial when querying frequently sampled time series on long time intervals , otherwise the number of points returned by the server becomes huge and cannot be handled . Vertical aggregation applies operations between time series groups . They are important when you have a large number of groups after the grouping operation . Predictors are forecasting tools . They return series that have timestamps after the final timestamps of the original series . Anomaly detection contains tools that return a time series which represents the anomalies of the series it is applied on . You can display information on the feature you add by clicking on the question mark . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection) Outlier detectors work just like predictors except that they return a time series that represents the anomalies of the series it is applied on . Two outlier detectors are currently implemented : Outliers DBScan Grubbsâ\\x80\\x99s Test',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = Grubbsâ\\x80\\x99s Test) The Grubbsâ\\x80\\x99s test detector is an outlier detection algorithm . It detects maximums and minimums outliers on a time series . The Grubbsâ\\x80\\x99s test algorithm is computationally slow for series greater than 70 000 points . The cost of this algorithm is linear relative to the time series size . It detects anomalies in two steps . First it removes the trend and the seasonality of the time series , then it computes Grubbsâ\\x80\\x99s Test algorithm . There are three parameters for the Grubbâ\\x80\\x99s tester algorithm : The algorithm used to detect the period of the time series ( if there is one ). You should use AUTO_PERIOD_DETECTOR by default which works for any type of time series with more accuracy . However , it has a higher computational cost than the other detectors . The confidence level of the outlier ( between 0 and 1 ). A boolean to return the time series with the outlier detection or not . This algorithm detects outlier based on the data of the time series . If your time series doesnâ\\x80\\x99t contain a lot of data point , anomalies can be missed . Grubbsâ\\x80\\x99s test defines a threshold which can be less meaningful on series with few data points . For more information about period detection algorithms , see : On Periodicity Detection and Structural Periodic Similarity IBM T . J . Watson Research Center ( AUTO_PERIOD ), A FULLY AUTOMATED PERIODICITY DETECTION IN TIME SERIE ( ACF )',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = DBSCAN) Density-based spatial clustering of applications with noise ( DBSCAN ) is a data clustering algorithm . It can be used to find anomalies in a single or multiple groups of data . steps : Find the points in the Îµ ( epsilon ) neighborhood of every point , and identify the core points with more than minPts neighbors . Find the connected components of core points on the neighbor graph , ignoring all non-core points . Assign each non-core point to a nearby cluster if the cluster is an Îµ ( eps ) neighbor , otherwise consider it as outlier Therefore , it detects anomalies at points which do not belong to any cluster .',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = DBSCAN, Paragraph = Advantages) DBSCAN works using multiple data groups , and is able to indicate the groups having outliers DBSCAN does not require one to specify the number of clusters in the data a priori , as opposed to most other clustering algorithms like K-Means . DBSCAN can find arbitrarily shaped clusters . DBSCAN has a notion of noise , and is robust to outliers . The parameters minPts and Îµ can be set by a domain expert , if the data is well understood .',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = DBSCAN, Paragraph = Disadvantages) The distance measure is euclidean distance . The time dimension can be scaled to overcome this limitation on the time axis . DBSCAN cannot cluster data sets well with large differences in densities , since the minPts-Îµ combination cannot then be chosen appropriately for all clusters .[ 8 ] If the data and scale are not well understood , choosing a meaningful distance threshold Îµ can be difficult . To help using this feature , Îµ can be estimated by a distribution algorithm .',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = DBSCAN, Paragraph = Time scale methods) Time scale methods define how DBSCAN algorithm shoul consider the distance between points in time dimension . TIME_SCALE_FULL_RANGE : Considers that the time dimension has the same variability as the entire data set . i . e ( maxTime-minTime )*time_scale_factor is equivalent to maxValue-minValue TIME_SCALE_AVERAGE_SAMPLING_RATE : Computes average time period between points . Then considers that the time dimension has the same variability of the entire data set . i . e average_sampling_rate*time_scale_factor is equivalent to maxValue-minValue TIME_SCALE_PROVIDED_FACTOR : Uses only provided factor : i . e . distance in time dimension is equal to time difference in milliseconds multiplied by this factor IGNORE_TIME_DIMENSION : time dimension is not used to compute distance between points',\n",
       " 'Context : (Documentation = User Manual, Title = Outlier Detection, Chapter = Standard Deviation Test) The standard deviation test detects outliers by analyzing the distance of each data point to the average value of the time series . It uses all data groups and returns outliers within their respective groups . There are four parameters for this algorithm : The statistical method used as a baseline to detect outliers . Possible values are AVERAGE , MOVING_AVERAGE and MOVING_MEDIAN The time period over which the statistical method is applied The standard deviation threshold , which sets the sensitivity of the outlier detection A boolean to return the time series with the outlier detection or not . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors) Predictors work just like aggregators except that they return a time series that represents the future of the series it is applied on . To get a more complete understanding of the different type of aggregators , please refer to the whitepaper RD5 . Three predictors are currently implemented : Least squares Holt DLM',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = DLM) DLM Stands for Dynamic Linear Models . It is the easiest and most reliable predictor out of the three , and should thus be preferred in most situations . However it can however be very intensive in terms of computing power : the complexity of this algorithn increases exponentially with the number of samples used in the prediction . The maximum number is limited by the skyminer . dlm . max_datapoints property of the configuration file . It has three parameters : The number of predictions to be performed . Interval between predictions depends on the regularity of your data and is automatically assessed . The name of the model that is used for the prediction . It must be one of AUTO , DETERMINISTIC_LINEAR , DETERMINISTIC_EXPONENTIAL , STOCHASTIC_LINEAR , STOCHASTIC_EXPONENTIAL , RANDOM_WALK , GENERAL A boolean to show the time series with the predictions or not .',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = Holt) The Hold predictor estimates the local trend of the time series , and can tolerate any amount of data with any regularity . The downside is that its smoothing parameters must be evaluated manually , making this algorithm hard to use for inexperienced users . Finally , it can only predict one sample , as predicting more than one wouldnâ\\x80\\x99t be relevant . It has three parameters : The level , which indicates how weight is balanced between the previous samples and those before . The slope . A boolean to show the time series with the predictions or not .',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = Least Squares) Least squares estimates the global trend of the timeseries , and can tolerate any amount of data with any regularity . It has two parameters : The number of predictions . A boolean to show the time series with the predictions or not .',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = Advanced, Paragraph = DLM) All the implemented models are designed to extract a periodic pattern and a trend . Thus the state space is always of the dimension of the period , with one element Tt and p-1 elements St . The dynamic equation modelling seasonality remains the same through all models : \\\\( S_t_+_S_{t-1}_+_ ... _+_S_{t-p+1}_=_w_{t2}\\\\) It is very important that used data is structurally regular ( consistent sampling rate ), though the algorithm allows for some missing values . If data sampling is irregular , it will be rejected by the processor . The aggregation tools offered by Skyminer allow you to easily prepare data in an appropriate manner . See the section on data requirements for more precise information . Parameter Estimation Defining a dynamic linear model is only the first step . The core of the pattern matching is actually the parameter estimation . For a given period p , parameters Î±, Ï\\x95, w_t1 , w_t2 et v_t are estimated with a powerful trust region algorithm called BOBYQA , created by Powel ( Powel , 2009 ). This algorithm tolerates simple constraints which is very interesting as we would like to avoid estimating negative variances with the algorithm . Even if it is an efficient optimisation algorithm , our problem is strongly non-convex and thus good initialisation of parameters is crucial . The periodic nature of the sequence must be assessed . As per the model , for a period of length p the state contains p-1 variables S_t in charge of estimating the values of the function during the period . If periods become large , this implies a very large state space , and thus very heavy computations . Moreover parameter estimation must be realised for each period length . To keep computational time reasonable , two techniques are employed : Sparse matrix computation , which allows to keep the complexity of a matrix/vector product linear and matrix/matrix product quadratic . Multiscale period evaluation . The idea is to evaluate the quality of the period length parameter for long periods first on a downsampled version of the data . Only the best period is selected and assessed on the finer data . This avoids optimising the parameters for every single period length between 2 and the maximum period length , and more importantly it limits the number of long period evaluations . The last step is to select the most appropriate model . Several models are availableâ\\x80¦ Trend estimation is what makes the difference between models . General Structural Name : GENERAL_STRUCTURAL \\\\( T_t_=_\\\\phi_T_{t-1}_+_\\\\alpha_+_w_{t1}\\\\) This is the most general model , using 3 parameters only for the trend . This model would have the best adaptability to many different data structures , but it also often provides false detections of trends and thus is very prone to overfitting . The use of this model should thus be avoided as much as possible . Also it is important to notice that the more parameters there are , the more difficult the optimisation and thus the more likely it is that the parameter estimation will not result in the best combination of parameters possible . Stochastic Linear Name : STOCHASTIC_LINEAR_PLUS_SEASONAL \\\\( T_t_=_T_{t-1}_+_\\\\alpha_+_w_{t1}\\\\) The stochastic linear model sets the value of the exponential trend to 1 , thus disabling it . It will detect a linear trend that tolerates some deviations thanks to the noise parameter w_t1 . Some examples can be easily found where this kind of model overestimates the presence of linear trends because the noise parameter gives it too much liberty . It must also be used with care . Deterministic Linear Name : DETERMINISTIC_LINEAR_PLUS_SEASONAL \\\\( T_t_=_T_{t-1}_+_\\\\alpha\\\\) The deterministic linear model tries to detect only a linear trend and does not allow the level to fluctuate around it . Everything that is not a periodic pattern or linear trend will thus be considered as measurement noise . This model lacks a little bit of flexibility because data that does not follow a constant or linear trend will be poorly estimated . Random Walk Name : RANDOM_WALK_PLUS_SEASONAL \\\\( T_t_=_T_{t-1}_+_w_{t1}\\\\) This is the classical random walk trend that allows the trend to freely fluctuate in any direction . The pattern estimation will thus only focus on periodic pattern extraction to give the better prediction possible . This is considered as a good combination with the deterministic linear model , in the case were the second one fails to detect a clean linear trend . Exponential models Name : STOCHASTIC_EXPONENTIAL_PLUS_SEASONAL and DETERMINISTIC_EXPONENTIAL_PLUS_SEASONAL \\\\( T_t_=_\\\\phi_T_{t-1}_+_w_{t1}\\\\) and \\\\( T_t_=_\\\\phi_T_{t-1}\\\\) These models should only be used if an exponential trend is known to exist . The testing of exponential models has shown that these were too unstable for forecasting . Even a very small error in the estimation of Ï\\x95 has important consequences even on medium horizon forecasting . For instance a value of 0 . 99 , which is close to 1 and can easily be generated by noise or outliers , makes the trend drop at 60% of its value in only 50 samples of forecasting . This is very unpleasant for the robustness of the algorithm . Automatic model Name : AUTO Automatic model tries to pick up the best fit between Random Walk and deterministic Linear . On purpose models that have a general worse fit ( like exponential ) are not evaluated . Data requirements The data provided to the predictor must comply with some basic requirements . The format of the data accepted forces it to be sampled at regular intervals . The number of missing values must remain reasonable , both in proportion and distribution throughout the data . Indeed the downsampling propagates missing values . Trying to infer them would introduce a strong bias into the downsampled sequence . Thus periodically distributed missing values might interfere with the periodic pattern estimation and the prediction will fail . A maximum number of about 200 samples should be considered for the input data . The periodic pattern estimation becomes less reliable and computationally more expensive when it becomes too long . It seems more reasonable to realize proper aggregation before trying to apply the analysis . Indeed the resolution of the forecast that one might expect logically depends on the time span of the data used and the length of the desired prediction . For instance , if one wants to predict the values that will occur in the next 5 hours , it seems very unlikely that the knowledge of data that dates back to a few months ago would be useful . Also it is statistically unrealistic to try to predict the behaviour of the data during 5 hours with a precision up to 1 millisecond . Also it is often unrealistic to expect a prediction that is longer than the data to be of any significance . Predicting on more than the half or the third of the length of the data should be avoided or used with great care . A minimum number of sample has also been set by default because the model is invalid with period length smaller than 2 . Thus if the minimum number of repetition for the pattern is 3 , the minimum number of samples will be 6 . Of course in most situations trying to extract a pattern on a set of 6 samples will not make sense . Note Example of procedure that maximises the prediction tool efficiency : Choose the date at which you would like to start your prediction . Choose the time span of the prediction you wish to make . Compute the length of the data to be considered for the prediction . It should be at least a few times longer than the prediction time span . Aggregate so the length of the considered data is represented by less than 200 samples . The resolution of the prediction being the same as that of your data , compute the number of sample to predict . Apply prediction ! References Chatfield , C . ( 2003 ). The Analysis of Time Series : An Introduction . Chapman & Hall/CRC . Hanzak , T . ( 2014 ). Methods for periodic and irregular . Prague : Charles University . Jalles , J . T . ( 2009 ). Structural Time Series Models and the Kalman Filter : a concise review . Cambridge . Powel , M . ( 2009 ). The BOBYQA algorithm for bound constrained optimization without derivatives . Shumway , & Stoffer . ( 2011 ). Time Series Analysis and Its Applications . New York Dordrecht Heidelberg London : Springer .',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = Advanced, Paragraph = Holt) Computes the holt exponential smoothing prediction . Holt is a variation that involves double exponential smoothing . Holt is considered as a good trend estimation tool that can adapt to any data irregularity , but it does not model periodic patterns . References Cipra , TomÃ¡Å¡ ( 2006 ) - Exponential smoothing for irregular data . ( English ). Applications of Mathematics , vol . 51 , issue 6 . https ://dml . cz/handle/10338 . dmlcz/134655 Chatfield , C . ( 2003 ). The Analysis of Time Series : An Introduction . Chapman & Hall/CRC .',\n",
       " 'Context : (Documentation = User Manual, Title = Predictors, Chapter = Advanced, Paragraph = Least Squares) Least squares prediction simply uses a least squares linear regression over the data time period ( data fitting using a straight line ). This gives a simple trend that is extrapolated by the algorithm . Indeed periodic patterns are not taken into account by this model . Provides a very fast but low accuracy prediction References https ://en . wikipedia . org/wiki/Least_squares 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Retrieval of last known value of data before query start Time, Chapter = Overview) It is possible to ask Skyminer to retrieve with the data set requested the last known value of metrics for the same query . This is useful for points that have a very low sampling rate , so data might not appear within the query , or data that is recorded in change-only mode , so the data might not appear , or just the changes in the results .',\n",
       " 'Context : (Documentation = User Manual, Title = Retrieval of last known value of data before query start Time, Chapter = Description) The fetch_previous_sample option is designed to retrieve the last known value of the query . It can be adapted to fit the purpose of the query : aligning sample time to query start time , merging with the data group or as a distinct data group , and the relative duration skyminer is allowed to go backward in time to find this sample before query start time . This feature is not usable in queries requested in descending order ( order : desc ). On the Web UI : check fetch previous samples option to enable the feature . The detailed setting appear . Those detailed setting are explained in the subsequent API section . The Web UI allows to define basic options only , advanced options ( fetch previous samples far in the past or ask for more than one sample ) are not desirable in most use cases so they are only enabled in using the API .',\n",
       " 'Context : (Documentation = User Manual, Title = Retrieval of last known value of data before query start Time, Chapter = Usage in API) The API is at query level . fetch_previous_sample : Object . Optional . If this field is set previous data will be returned merge_groups : Boolean . Specifies if groups need to be merged or kept distinct . If the groups are not merged , previous data is returned in a distinct group named query_extension , with the value previous_data . Default is true . time_align : Boolean . Specifies if the previous data sampling time has to be aligned to query start time . Default is false . for_empty_results_only : Boolean . Only applies for if merge_groups is requested . If this flag is set to false the results are merged always with the original query results . Otherwise they are only merged if there is no data in the original query results . Default is true . limit : Integer number . Number of samples to retrieve before query start time . Default is 1 . It is not recommended to change this setting . duration_relative : Object . Relative time to search for the last data sample in the past . It is the same format as relative query time . Default is 3 weeks . It is not recommended to change this setting . Example of query ( last 10 minutes , groups are merged , time not aligned , fetch last 5 samples ) : { \"metrics\": [... list of query metrics ...], \"cache_time\": 0 , \"start_relative\": { \"value\": \"1\", \"unit\": \"hours\" }, \"fetch_previous_sample\":{ \"duration_relative\":{ \"value\": 10 , \"unit\":\"minutes\" }, \"merge_groups\": true , \"limit\": 5 , \"time_align\": false } } Example of query ( default settings ) : { \"metrics\": [... list of query metrics ...], \"cache_time\": 0 , \"start_relative\": { \"value\": \"1\", \"unit\": \"hours\" }, \"fetch_previous_sample\":{} } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Tag filtering) Tag filtering allows you to select tag names and value pairs that you want to filter against . All data points that do not have these tags will be discarded . Selecting tag values dynamically reduces the number of available tag values . This behavior is visible with different elements .',\n",
       " 'Context : (Documentation = User Manual, Title = Tag filtering, Chapter = Tag component) This component allows you to add tag filters and values from two different drop down lists . These lists are automatically updated depending on available tags ( you can only choose available tags and values ). Blue buttons on the top of this component allow you to open the tag explorer ( see section below ). These buttons give you information about tag refinement . You can see the name of the tag , the number of selected tags , the number of available tags and the number of total tags before refinement ( see number 1 on the image above ). If you have chosen a value that is no longer available ( with no corresponding data ) after updating your filters , it will be greyed out ( see number 2 on the image above ).',\n",
       " 'Context : (Documentation = User Manual, Title = Tag filtering, Chapter = Tag explorer) The toggle button allows you to hide unavailable values . Information about tag , you can see the name of the tag , the number of selected tags , the number of available tags and the number of total tags before refinement . Searching , this field allows you to filter values depending on their name . An example of selected tag An example of unavailable selected tag An example of unavailable tag Note : Values in tag explorer are sorted . the first displayed tags are available tags followed by unavailable tags . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Time override, Chapter = Overview) Time override allows you to specify a different time range for each metric in the query . If it is not specified , the reference time of the query is used in each metric . It is specifically useful if one wants to compare values taken by the series on different time intervals . It will be typically used in conjunction with a time align aggregator .',\n",
       " 'Context : (Documentation = User Manual, Title = Time override, Chapter = Description) Time override can be specified exactly the same way as the query reference time . Only one time override can be specified for each metric . To have the values of the same series at different time intervals , the corresponding metric should be duplicated with different time overrides for each copy .',\n",
       " 'Context : (Documentation = User Manual, Title = Time override, Chapter = Usage in API) The time override is specified in an object in the time_override attribute of the metric . The global structure of the time override is the same as for the global time parameters of the query . As a reminder : You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If either end time is not specified the current date and time is assumed . start_absolute The time in milliseconds . start_relative The relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . end_absolute The time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . end_relative The relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to the current date and time . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Overview) Vertical aggregators complete the capabilities of regular ( horizontal ) aggregators . They can be used on any query that has a group by tag . They aggregate the datapoints of the different series together .',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Description) Available vertical aggregators : Average Min Max Difference Preference Sum Note : The default vertical aggregator option selected in the UI is Average',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Description, Paragraph = Classical vertical aggregators: average, min, max, difference and sum) Classical aggregators have four boolean options . Here is an example series grouped according to a given tag . No options : performs the vertical aggregation each time a sample is met in one series , and only with the subset of series that actually have a sample with the same timestamp . Start with last first : removes the points at the beginning of the time series until at least one measurement has been seen in every time series . This allows you to cut the series that start before the others . End with first last : same as the previous option but operates on the end of the sequence . Interpolate with last : when a value is missing in one group it is replaced by the last value seen . If no value has been seen yet , aggregation is done between the other series . To have actual interpolation one must use the â\\x80\\x9cinterpolationâ\\x80\\x9d horizontal aggregator first . Discard unsynchronized : this automatically ignores the three previous parameters . Each time a value misses in one of the series , all the other points having the same timestamp are discarded . Note : difference aggregator is particular . It can only handle to series , and returns the absolute value of the difference .',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Description, Paragraph = Preference vertical aggregator) The preference aggregator works on two series . It allows the user to prioritize one series over the other . If a sample is present in that series , it will be returned . Otherwise , the samples from the other series will be returned .',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Description, Paragraph = Interpolated vertical aggregator) To proceed to a vertical aggregation that uses interpolation to fill gap , use the interpolation horizontal aggregator which provides the linear interpolation of the series so it becomes regularly sampled . You can then chain it with any vertical aggregator presented in the previous sections .',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Usage in API) The vertical aggregator is specified in an array in the vertical_aggregators attribute of the metric . There can be only one vertical aggregator object , and a name attribute is always specifying the nature of the aggregation .',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Usage in API, Paragraph = Classical vertical aggregators) This is the list of classical vertical aggregators : vertical_avg vertical_sum vertical_min vertical_max vertical_dif These aggregators have four boolean parameters : start_with_last_first end_with_first_last fill_with_last discard_unsynchronized Note that if discard_unsynchronized is set to true , the other 3 parameters are ignored . All parameters are false as default . ... \"vertical_aggregators\": [ { \"name\": \"vertical_sum\", \"fill_with_last\": false } ] ... Error messages Multiple samples with the same timestamp : vertical aggregators do not support series with more than one sample having the same timestamp { \"errors\": [ \"The vertical aggregator cannot compute aggregation for data points with the same timestamp . You must precede it with another aggregator .\" ] } Difference aggregator only works with two series { \"errors\": [ \"Difference aggregator only works with two series\" ] }',\n",
       " 'Context : (Documentation = User Manual, Title = Vertical aggregators, Chapter = Usage in API, Paragraph = Preference vertical aggregator) Its name is vertical_pref . It has two parameter : discriminating_tag preferred_tag_value They represent the name and the value of the tag that should be preferred . Of course , it requires that the data to be grouped according to a tag name which actually takes that value . Error messages An error message will be returned with status code 500 if : the discriminating_tag parameter or the preferred_tag_value parameter is not specified the tag value is not found in any of the groups aggregated the discriminating tag has not been used as grouping tag there are more than two groups the preferred tag value is not found in any of the groups 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Query view, Chapter = Overview) The query view , thanks to a double data binding the json query is automatically built during the edition . From here , you can share a query using a link , save and load a query in JSON format .',\n",
       " 'Context : (Documentation = User Manual, Title = Query view, Chapter = Description) Those three buttons allow you to share , save or load a query The JSON query can be edited in this field . Three buttons are available for the edition , the first one allows you to cancel the changes you have made and close the editior . The second one allows you to reset the query before its changes without closing the editor . The last one allows you to save the changes you have made and update the query builder with these changes . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = Web services) The API endpoint for queries is /api/v1/datapoints/query . You should note that some properties that exist in the API are not accessible from the Web UI , mainly for usability reasons .',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = API online documentation) The API documentation is available in the query user interface : OpenAPi specifications Features ( aggregators , predictors , â\\x80¦) documentation Links can be found in the About section of the Query page :',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = Query properties) The global structure of a query is the following one : Start time ( mandatory ) End time ( optional ) List of metrics ( at least one element ) Cache time ( optional ) { \"start_absolute\": 1357023600000 , \"end_relative\": { \"value\": \"5\", \"unit\": \"days\" }, \"cache_time\": 0 , \"metrics\": [... the list of metrics ...] } Time parameters : You must specify either start_absolute or start_relative but not both . Similarly , you may specify either end_absolute or end_relative but not both . If either end time is not specified the current date and time is assumed . start_absolute The time in milliseconds . start_relative The relative start time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 5 minutes , the query will return all matching data points for the last 5 minutes . end_absolute The time in milliseconds . This must be later in time than the start time . If not specified , the end time is assumed to be the current date and time . end_relative The relative end time is the current date and time minus the specified value and unit . Possible unit values are â\\x80\\x9cmillisecondsâ\\x80\\x9d, â\\x80\\x9csecondsâ\\x80\\x9d, â\\x80\\x9cminutesâ\\x80\\x9d, â\\x80\\x9choursâ\\x80\\x9d, â\\x80\\x9cdaysâ\\x80\\x9d, â\\x80\\x9cweeksâ\\x80\\x9d, â\\x80\\x9cmonthsâ\\x80\\x9d, and â\\x80\\x9cyearsâ\\x80\\x9d. For example , if the start time is 30 minutes and the end time is 10 minutes , the query returns matching data points that occurred between the last 30 minutes up to and including the last 10 minutes . If not specified , the end time is assumed to the current date and time . Cache time : The amount of time in seconds to cache the output of the query . If the same query is executed before the cache time expired then cached data is returned .',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = Metric properties) The query object requires a list of one or more metrics . The metric is an object that always has a name property and optionally can have : aggregators vertical_aggregators predictors group_by tags time_override limit group_limit limit Limits the number of data points returned from the data store . The limit is applied before any aggregator is executed . group_limit Limits the number of groups returned by the query . The limit is applied before any aggregator is executed . order Orders the returned data points . Values for order are â\\x80\\x9cascâ\\x80\\x9d for ascending or â\\x80\\x9cdescâ\\x80\\x9d for descending . Defaults to ascending . This sorting is done before any aggregators are executed . The rest of the attributes2 will be described in the subsequent parts . { \"tags\": {... tags for filtering ...}, \"name\": \"abc . 123\", \"limit\": 1000 , \"group_limit\": 100 , \"aggregators\": [... a list of aggregators ...] }',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = Example of complete query) Here is an example of a complete query : { \"start_absolute\": 1357023600000 , \"end_relative\": { \"value\": \"5\", \"unit\": \"days\" }, \"metrics\": [ { \"tags\": { \"host\": [\"foo\", \"foo2\"], \"type\": [\"bar\"] }, \"name\": \"abc . 123\", \"aggregators\": [ { \"name\": \"sum\", \"sampling\": { \"value\": 10 , \"unit\": \"minutes\" } } ] }, { \"tags\": { \"host\": [\"foo\", \"foo2\"], \"type\": [\"bar\"] }, \"name\": \"xyz . 123\", \"aggregators\": [ { \"name\": \"avg\", \"sampling\": { \"value\": 10 , \"unit\": \"minutes\" } } ] } ] }',\n",
       " 'Context : (Documentation = User Manual, Title = API, Chapter = Query response) The response contains either the metric values or possible error values . Returns 200 for successful queries . It always includes a group_by named type . If the data returned is not a custom type then number is returned . { \"queries\": [ { \"sample_size\": 14368 , \"results\": [ { \"name\": \"abc_123\", \"group_by\": [ { \"name\": \"type\", \"type\": \"number\" }, { \"name\": \"tag\", \"tags\": [ \"host\" ], \"group\": { \"host\": \"server1\" } } ], \"tags\": { \"host\": [ \"server1\" ], \"customer\": [ \"bar\" ] }, \"values\": [ [ 1364968800000 , 11019 ], [ 1366351200000 , 2843 ] ] } ] } ] } Failure { \"errors\": [ \"metrics [ 0 ]. sampling . unit must be one of SECONDS , MINUTES , HOURS , DAYS , WEEKS , YEARS\" ] } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Examples, Chapter = Getting the trend on a long time interval) In the Time Range section , select 1 years ago as relative beginning time . Select the metric name you wish to plot . Create one aggregator , for instance SUM or AVG , and select a sampling of 1 Day . If you have a lot of different records for that metric , you might want to filter out a specific tag .',\n",
       " 'Context : (Documentation = User Manual, Title = Examples, Chapter = Predicting using a periodic pattern) Select a metric that has a predictable behavior on a given time period . Aggregate it with the sum or average aggregator so you have only approximately 100 samples and maximum 200 samples ( for instance if you are querying on 1 week of time range , use an hourly aggregation ). Test your query by graphing it . Create a new metric in the same query with exactly the same parameters . To add a new metric click on the + button on top of the metric description : Add an aggregator , select DYNAMIC LINEAR MODEL , leave the model as Auto and choose 10 or 20 predictions . You can now start the prediction . The first metric queried will display the series used for the prediction and the second one the predicted values :',\n",
       " 'Context : (Documentation = User Manual, Title = Examples, Chapter = Using group by tag to split series) Most metrics have tags to differentiate data points according to their source or other properties . To split those series , you should use the group by tag and select an appropriate tag name . This creates one series for every different value of the given tag . Using a vertical aggregator can then combine this series together . This will take the maximum between each series . The options of the vertical aggregator allow you to choose your policy toward missing samples . If a sample is missing in one of the series , you can discard the aggregate , you can use the latest value of the series where it is missing â\\x80¦',\n",
       " 'Context : (Documentation = User Manual, Title = Examples, Chapter = Comparing series at different points in time, Paragraph = Shift series one by one) If you want to compare the behaviour of a series at different times , use the time override and time shift features . Choose a reference time range . This will be used by Skyminer to re-synchronize your different samples . Add a time override in your first metric and choose your time range of interest . Select the ALIGN TIME aggregator . This will resynchronize your samples to the reference time range . The default option automatically synchronizes your samples to the start of the reference time . If you want to compare the same metric on multiple time ranges , you should consider using the ALIAS aggregator which allows you to rename your series in order to be able to distinguish different time ranges . Copy this metric using the add button above the metric tabs , and choose a new override range to include to you correlation , and if necessary attribute a new alias . You can combine group_by , query of different metrics , and different time overrides in one query . The data can be plotted using the â\\x80\\x9cGraphâ\\x80\\x9d button at any time . Note that it will be very time consuming if the query is large , and it will warn you not to plot the query if too many points need to be displayed .',\n",
       " 'Context : (Documentation = User Manual, Title = Examples, Chapter = Comparing series at different points in time, Paragraph = Use the event manager) To compare the values of the series at different points in time , you can also use the event manager . This will automatically duplicate the metric you specify underneath for each event . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview) The correlation UI is a web client that allows you to build correlation queries , execute them , and display their result .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview, Paragraph = Choose Correlations Mode (1)) Search Mode takes one reference series and computes its correlations against all the other series specified : one Vs many correlations . Matrix Mode takes one set of series and computes correlation between each pairs : many Vs many correlations',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview, Paragraph = Specify series used for correlations as skyminer queries (2)) It takes a Skyminer query in Json format . If correlation mode is Search , the first query will return a single series of data points . Clicking on Create Query button opens a query builder that helps in generating a valid query .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview, Paragraph = Select similarity measure (3)) Similarity measure is the correlation measure used to compute the similarity between pairs of series . It defines the mathematical method which compares the behaviors of the series . The value returned by a similarity measure is between 0 and 1 , 0 meaning that they have nothing in common and 1 that the measured behavior is exactly the same for the two series . Implemented measures : Discarding linear correlation : This correlation method is often called linear or Pearson correlation . It compares the relative variations of the time series , but is only using pairs of data points that have exactly equal timestamps . DTW : Dynamic Time Warping is a method that originates from Speech Processing and that compares pairs of samples while allowing a level of flexibility in sample comparisons over a given time range ( defined by the parameter called Maximum delay ). For a more precise description of similarity measures please refer to Details on similarity measures .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview, Paragraph = Select properties of result format (4)) Result format specifies some properties of how data must be returned by the server . Its main role is to limit the amount of data processed by the browser to avoid computationally expensive correlations queries . Shared parameters are : Threshold : filters results according to their value . Results that have correlation scores lower than this limit are discarded . Should be a number between 0 and 1 Return best : limits the maximum number of results according to this value . Should be a positive integer . In Matrix mode you have extra parameters : Return perfect scores : filters results that are exactly equal to 1 . These results typically need to be removed because they are generated by constant or equal series and thus do not represent valuable information . Sparse : specifies whether cross-correlations that were filtered by Threshold , Return best , or even Return perfect scores should be returned to fill the matrix on the lines and columns of correlations that have the kept correlation scores .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Overview, Paragraph = Select computational method (5)) Computation method specifies to the server how it should compute the correlations . This focuses mainly on the way to distribute computations on the server . Default threaded versions should be preferred for obvious computational speed reasons .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Building a correlation query, Paragraph = Correlation search) Correlation search allows you to compare one series of your choice to a set of other series . The goal is to have a reference oriented view into the data and find series that have a similar behavior to your series of interest . First activate Search mode and create a query with the series you want to use as reference by selecting Create Query under the Reference query field . This will open the Skyminer query building interface . The query you build to serve as reference query must return a single series . Do the same for Searched series but this time create a query that generates a set of series . You can do that by using the Group By feature and/or by querying multiple metrics . You should then chose your similarity measure . If you know that your data is regularly sampled and synchronized between the series , you should probably use Discarding linear correlation , which is often faster and less prone to false positives . Otherwise use DTW to compensate sampling irregularities and shifts . The Maximum delay option is the one that allows you to control what flexibility you want to give the algorithm on the time axis . For instance specifying it to 1 minute will allow the algorithm to proceed to time distortions and correlate sample that have timestamps that are up to 1 minute apart . Note : Normalize , Beta and Local distance are advanced parameters of the DTW algorithm . You should use the default settings or consult the detailed documentation on similarity measures and their parameters in Details on similarity measures . Both similarity measures have an option called Reliability threshold that can be interesting when you have irregularities in the sampling of your series . It specifies the minimum number of samples that need to be used in the correlation . If the requirement is not met the correlation score is set to 0 which allows you to easily discard this unreliable result . Result format allows you to limit the number of results returned by the server . You will typically want to focus on the best hundred correlation results . Finally you should specify the computation method . You should use the default threaded implementation which uses all the computational capacities of your server . Once the query is built , you have two options : You can launch the query and wait for the results to be displayed You can generate a link that will allow you to execute the query by submitting that URL in your web browser at any time When your results are displayed , you can save them and display them again at any time without having to re-execute the query . This is interesting for very deep queries that are long to execute .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Building a correlation query, Paragraph = Correlation Matrix) Creating a matrix query is very similar to a search query . Select the Matrix correlation mode . The Reference series area disappears because the Matrix mode only requires a set of series and computes all pairs of correlations in this set . Create a query in the Searched series area . Make sure it is a query that generates multiple series . You can do that by using the Group By feature and/or by querying multiple metrics . The similarity measures are identical to those used in Search mode . Result format has two extra options that allow you to better control the amount of results . When you choose to return the best n results in matrix mode , you get n correlation scores for n pairs of series ( this might generate a matrix of size n x n or less ). If you check the Sparse matrix option , only these results will be returned and the other entries of your matrix will be null . If you want to display the other correlation scores between the series involved in this best n correlations , you should uncheck this option Return perfect score can be used to filter out results that are not useful in that they are too perfect ( exactly equal to 1 ). For instance it happens that a set of measures is constantly equal to 0 because the measuring devices are not properly set . This would generate a set of highly correlated series that we might want to remove because they would hide other less obvious correlations . The rest of the query works exactly the same as in the search mode .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Analyzing results, Paragraph = Correlation search) When you execute your correlation search query , the results are returned in the form of a bar chart . The plot of the reference series is also displayed . The bar chart represents the correlation scores sorted by strength . The height of the bar is the result of the similarity measure . Its color represents the reliability of this measure . If it has been established on a large number of samples , and thus it is reliable , it will be green . If the number of samples is lower , it is represented in orange or red . To get more information on each correlations , you can pass your cursor on the bar chart to get a detailed tooltip . You can also click on a bar to query for the corresponding series . This allows you to visualize the raw data and better understand the correlation scores .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Analyzing results, Paragraph = Correlation Matrix) The matrix mode generates a dynamic correlation matrix . A correlation matrix of n series is an n x n matrix that represents the correlation scores of all pairs of series . Each line or column represents the correlation scores of one series against all the others . The matrix is symmetric because a correlation is commutative , and the scores on the diagonal are always equal to 1 as they represents the correlation of series with themselves . The more intense a color , the greater its correlation score . As for the bar chart in correlation search mode , green indicates that the correlation is reliable , and orange or red signify the correlation has been done on less samples . On the right of the matrix , the pane describe the cell your cursor is currently hovering . It provides you information on both series involved in the correlation and the score itself , along with its reliability . Under the matrix you have a slider that allows you to tune the coloring relative to the reliability . The reliability threshold at which the cell starts to turn into green is indicated on the right of the slider . The last two cells of the table are used to change the clustering of the matrix . this allows you to re-organize the cells by a given tag name or by metric name . You can chain multiple groupings by adding multiple elements consecutively Note : you should avoid displaying matrices that are larger than 200 x 200 because this can generate significant slowdowns of you browser . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Correlation API, Chapter = Correlation Search API, Paragraph = Query) Correlation search queries have five parameter categories : reference_series : it is the query for the series that should be searched . It should be a complete KairosDB query metric with only one metric and that generate only one series ( no grouping without vertical aggregation ). searched_series : it is the query for the set of series that should be compared to the reference series . It should be a complete KairosDB query metric , possibly with many metrics and tag group bys . similarity_measure : similarity measure is the method used to compare two series . Currently implemented are discarding_linear_correlation and dtw . A similarity measure goes with a reliability threshold ( reliability_threshold ) that indicates the minimum number of samples used in the correlation that are necessary . If it is under the threshold , correlation score is set to 0 . Data would typically be under this threshold if one of the series had only a few samples or if both series had unsynchronized sampling . You can find details on similarity measures and their parameter in Details on similarity measures . Discarding linear correlation is the classical Pearson correlation . DTW is the dynamic time warping distance measure . result_format : it is the only optional parameter of the correlation query . It has three parameters : threshold : only returns correlated series that have correlation scores above this threshold . Usually a double between 0 and 1 . best : integer n indicating that only the best n series should be returned . computation_method : implemented are sequential and threaded . Threaded uses every available core on the server . The following Json shows the structure of the query : { \"reference_series\": {â\\x80¦ KairosDB query â\\x80¦}, \"searched_series\": {â\\x80¦ KairosDB query â\\x80¦}, \"similarity_measure\": { â\\x80¦ Similarity measure parameters ( â\\x80\\x9cthresholdâ\\x80\\x9d, â\\x80¦) â\\x80¦}, \"result_format\": { â\\x80¦ Result format parameters ( â\\x80\\x9cbestâ\\x80\\x9d, â\\x80\\x9dthresholdâ\\x80\\x9d, â\\x80¦) â\\x80¦}, \"computation_method\": { â\\x80¦ Computation method parameters ( â\\x80\\x9cnameâ\\x80\\x9d, â\\x80¦) â\\x80¦ } }',\n",
       " 'Context : (Documentation = User Manual, Title = Correlation API, Chapter = Correlation Search API, Paragraph = Result) The result is separated in two parts : query : the query that actually generated this response results : The actual response to the query The results sub-object has four parameters : reference_series : the description of the reference series , in the same format as in usual KairosDB responses . correlated_series : a table with p elements describing the p series that have been correlated to the reference series . correlation_score : a table with p numbers ( usually between 0 and 1 ) representing the correlation scores of the p correlated series . correlation_reliability : a table with p numbers representing an indicator of the reliability of the correlation scores of the p correlated series . Here is an example response : { \"query\": { â\\x80¦ A correlation query as presented before â\\x80¦ }, \"results\": { \"reference_series\": { \"name\": \"kairosdb . protocol . http_request_count\", \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"metricnames\",\"query\",\"search\",\"tags\"] }, \"query_index\": 0 }, \"correlated_series\": [ { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"tagsâ\\x80\\x9d} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"tags\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"query\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"query\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"metricnames\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [ \"metricnames\" ] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"search\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"search\"] }, \"query_index\": 0 } ], \"correlation_score\": [ 0 . 95011 , 0 . 89177 , 0 . 77440 , 0 ], \"correlation_reliability\": [ 60 , 60 , 60 , 1 ] } } The largest part of the response is the description of the correlated series . To avoid generating overly long responses it is crucial to properly define the best and threshold parameters in the result format .',\n",
       " 'Context : (Documentation = User Manual, Title = Correlation API, Chapter = Correlation Matrix API, Paragraph = Query) Correlation search queries have four parameters , because they do not have a reference series . { \"searched_series\": {â\\x80¦ KairosDB query â\\x80¦}, \"similarity_measure\": {â\\x80¦ Similarity measure parameters ( â\\x80\\x9cthresholdâ\\x80\\x9d, â\\x80¦) â\\x80¦}, \"result_format\": {â\\x80¦ Result format parameters ( â\\x80\\x9cbestâ\\x80\\x9d, â\\x80\\x9dthresholdâ\\x80\\x9d, â\\x80\\x9dsparseâ\\x80\\x9d, â\\x80¦) â\\x80¦}, \"computation_method\": {â\\x80¦ Computation method parameters ( â\\x80\\x9cnameâ\\x80\\x9d, â\\x80¦) â\\x80¦} } There are 2 other differences between matrix query API and search query API : result_format has two extra boolean options called sparse and return_perfect_scores . When the best option and/or the threshold option are used , Skyminer only returns the correlations that match this criteria . The user may also be interested in cross correlations between the selected series . These are returned by setting sparse to false . computation_method : implemented methods are memory_cached and threaded_memory_cached . The latter is usually faster .',\n",
       " 'Context : (Documentation = User Manual, Title = Correlation API, Chapter = Correlation Matrix API, Paragraph = Result) The result is separated in the same two parts as for Search queries : quer and results . The results sub-object has two parameters : correlated_series : a table with p elements describing the p series that have been correlated to the reference series . correlation_score : a table with p numbers ( usually between 0 and 1 ) representing the correlation scores of the p correlated series . Here is an example response : { \"query\": { â\\x80¦ A correlation query as presented before â\\x80¦ }, \"results\": { \"correlated_series\": [ { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"tagsâ\\x80\\x9d} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"tags\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"query\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"query\"] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"metricnames\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [ \"metricnames\" ] }, \"query_index\": 0 }, { \"name\": \"kairosdb . protocol . http_request_count\", \"group_by\": [ { \"name\": \"tag\", \"tags\": [\"method\"], \"group\": {\"method\": \"search\"} } ], \"tags\": { \"host\": [\"skyminer01\"], \"method\": [\"search\"] }, \"query_index\": 0 } ], \"correlation_score\": [[ 0 , 1 , 0 . 95011 , 20 ], [ 0 , 2 , 0 . 86545 , 20 ], [ 1 , 2 , 0 . 47011 , 10 ]], } } 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = Discarding linear correlations, Paragraph = Summary) Discarding linear correlation is the classical correlation , also called Pearson correlation , where samples that does not have a match with the same timestamp in the other series are discarded .',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = Discarding linear correlations, Paragraph = Description) Linear correlation is the most classical and widely adopted time series similarity measure . It is classically defined for equally sampled time series , but can be adapted to irregular sampling by discarding values that are not sampled for both series . Adaptations have been made with kernel based methods ( Rehfeld , Marwan , Heitzig , & Kurths , 2011 ) to give a higher hit rate when samples are almost synchronized but not entirely , but it is actually equivalent to pre-aggregating the series with the proper smoother , and is thus useless in our case . The main advantage of linear correlations is that they do not store samples because only one pass is enough to compute first order (\\\\(\\\\hat{m}_x\\\\)) , second order ( \\\\(\\\\hat{m}_{x^2}\\\\) ), and cross series moment estimators ( \\\\(\\\\hat{m}_{xy}\\\\) ). \\\\(\\\\hat{m}_x = \\\\sum {x_t}\\\\) \\\\(\\\\hat{m}_{x^2}=\\\\sum x_t^2\\\\) \\\\(\\\\hat{m}_{xy}=\\\\sum x_t y_t\\\\) With these formulas , the computation of linear correlation is straightforward . \\\\( r=\\\\frac{\\\\hat{m}_{xy}\\\\_-\\\\_n .\\\\hat{m}_x_\\\\hat{m}_y}{\\\\sqrt{\\\\hat{m}_{x^2}\\\\_-\\\\_n .\\\\hat{m}_x^2}\\\\_\\\\sqrt{\\\\hat{m}_{y^2}\\\\_-\\\\_n .\\\\hat{m}_y^2}}\\\\) Where \\\\( n\\\\) is the number of synchronized samples .',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = DTW) API name : dtw Options : Reliability threshold ( reliability_threshold ) Normalize ( normalize ) Beta ( beta ) Maximum delay ( maximum_delay ) Local distance ( local_distance )',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = DTW, Paragraph = Summary) DTW is the Dynamic Time Warping algorithm . Its specificity is that it compares pairs of samples , while allowing a fluctuation in a given time range ( defined by the parameter called Maximum delay ).',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = DTW, Paragraph = Description) Dynamic time warping is a distance measure between time signals that comes from the speech processing community but has been widely adopted by big data analysts . It is an adaptation of a classical distance measure that is tolerant to deviations in the time sampling . In speech processing its benefit is to adapt to sentences spoken at different speed . In general data analysis it is used because it tolerates divergences due to differences of data sources . Going into the details of this technique is not in the scope of this report . A good introduction can be found in this paper : ( Petitjean , Inglada , & GanÃ§arski , 2012 ). A few adaptations have been made to the technique to make it more coherent with our use case : In particular a normalization pre-processing step has been added as an option to add the ability to detect pattern similarities even if series have different scales . Another normalization is done according to the length of the warping . This normalizationâ\\x80\\x99s legitimacy can be discussed but it is a necessary evil when exploring data with different sampling rates . The typical bands that are meant to limit computational complexity and memory consumption have been adapted to be based on the timestamps . The local distance measure has be defined to be \\\\(( x_{t_1}-y_{t_2})^2\\\\) or \\\\(|x_{t_1}-y_{t_2}|\\\\). The absolute value is usually used but the squared distance in conjunction with normalization corresponds to a generalization of the linear correlation which is an interesting property . DTW generates a distance \\\\( d\\\\) which is converted to a similarity measure using \\\\( r=\\\\frac{1}{( 1+d )^\\\\beta}\\\\), where \\\\(\\\\beta\\\\) can be set to have any desired sensitivity . Indeed if \\\\(\\\\beta\\\\) is large only very small distances will be represented with a large similarity factor .',\n",
       " 'Context : (Documentation = User Manual, Title = Details on similarity measures, Chapter = References) Martinez Heras , J ., Yeung , K ., Donati , A ., Sousa , B ., & Keil , N . ( 2009 ). DRMUST : Automating the Anomaly Investigation First-Cut . SpaceOps . Pasadena : American Institute of Aeronautics and Astronautics . Petitjean , F ., Inglada , J ., & GanÃ§arski , P . ( 2012 ). Satellite Image Time Series Analysis under Time Warping . IEEE Transactions on Geoscience and Remote Sensing . Rehfeld , K ., Marwan , N ., Heitzig , J ., & Kurths , J . ( 2011 ). Comparison of correlation analysis techniques for irregularly sampled time series . Nonlinear Processes . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Home page) When you install Grafana with Skyminer , the data source is already set by default . A default dashboard name â\\x80\\x98Homeâ\\x80\\x99 is created which allows you to quickly navigate between your saved dashboards . 1 - This section provides you a link to dashboards marked as favorite , recently viewed , and example dashboards . 2 - Settings and Cycle view mode 3 - Grafana Menu . 4 - Dashboards panel .',\n",
       " 'Context : (Documentation = User Manual, Title = Grafana Menu) This item is displayed on all pages of Grafana . You can hide or show it by clicking on the Grafana logo . Note : Skyminer does not support alerting , so the alert menu is unusable .',\n",
       " 'Context : (Documentation = User Manual, Title = Dashboards panel) This panel allows you to navigate through dashboards . You can filter them by name or by tags with the search field . Dashboards are organised by folders . When you click on a dashboard name you are redirected on the Dashboard page .',\n",
       " 'Context : (Documentation = User Manual, Title = Dashboard page) A dashboard , on Grafana , is made up of different rows and panels that will format the data . Each panel can be edited , deleted and re-arranged by clicking and dragging . Graphs can be interactively filtered by selecting/unselecting the individual series .',\n",
       " 'Context : (Documentation = User Manual, Title = Dashboard page, Chapter = Menu) Share dashboard , this button allows you to share your dashboard using a link or a snapshot , or by exporting it ( into a JSON file for example ) Add panel , this button allows you to add a new panel to your dashboard Save dashboard Settings , you can change the theme , time settings or even hide controls for when the dashboard is used in production Time range controls for your entire dashboard . This is where you set the start and stop time of your time series , but also the refresh rate of the dashboard by setting Refreshing every option Zoom , graphs are zoomable but note that it will modify the time range of the entire dashboard Refresh the dashboard ( on demand ) Cycle view mode 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Creating a new graph) To create a new graph , go to the row menu ( described in section above ) and click on the corresponding button ( 2 ). You will have to choose between â\\x80\\x9cAdd an empty panelâ\\x80\\x9d or â\\x80\\x9cAdd a new rowâ\\x80\\x9d. Select â\\x80\\x9cAdd an empty panelâ\\x80\\x9d. At this point you have an empty graph . You can edit its content by clicking on its title then on Edit . This will open the query editing view . This view allows you to build a simple request in 2 clicks , you just have to select a metric from an auto-complete dropdown list of metrics . You also have Panel Options on the right . These options allows you to change the panel name , select your visualization type , and many more : note : If you want to visualize data from this view , you should click on the auto refresh button ( framed in red on the screenshot below ). You can then customize your graph with Skyminer features ( add tags , group your data , add aggregators â\\x80¦). With the auto refresh , all changes will automatically be displayed . Once you are satisfied with your request , you can go back to the dashboard panel with the Back to dashboard button in the menu bar on the top . Graphs can be customized in various ways and made very functional and visually pleasant : colors , format ( dots , lines , bars , specific styles , stacked charts , step charts )',\n",
       " 'Context : (Documentation = User Manual, Title = Select visualization types) When creating a panel , you have choice between different kind of visualization . They can be selected on the Panel options panel : The default one is â\\x80\\x9cgraphâ\\x80\\x9d as seen previously .',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Specific visualization panels, Chapter = Advanced spectrum) Accessible with this option : Allows visualization of spectrum graph with waterfall : ( same as Skyminer Query Generated spectrum with waterfall graph )',\n",
       " 'Context : (Documentation = User Manual, Title = Creating a singlestat) A singlestat is a panel that displays the latest value of a series of interest , with a customizable color according to that value . It can be used to generate simple indicators for critical values . Creating a singlestat is exactly the same than Creating a new graph . To configure and customize your singlestat , go to the Options tab when you are in editing mode . There are multiple options and many possible combinations . You can configure the colors of your singlestat , define thresholds , stack a chart , and add a gauge :',\n",
       " 'Context : (Documentation = User Manual, Title = Fetch previous sample) Unlike all other Skyminer features , â\\x80\\x9cFetch previous sampleâ\\x80\\x9d is global to all metrics defined in the dashboard panel . When activated , it will fetch the last known sample value and place it at the beginning of the dashboard . Different options can be used : Merge groups : activated by default , all groups are merged into one . Time align : activated by default , the last known sample timestamp is aligned with the start of the time range . The original timestamp of the sample is preserved if this option deactivated . For empty results only : deactivated by default , it will fetch the last known sample only for the metrics that should return an empty result ( no sample for the given time range ).',\n",
       " 'Context : (Documentation = User Manual, Title = Predictions) Skyminer predictions can be accessed from the dashboard , provided that the end time for the dashboard is set in the future . In the time range selection menu , choose Custom and set the ending date to a time slightly after the time at which you expect the prediction to end .',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer annotations query) Skyminer is able to display annotations using Skyminer queries . To do so , create a new annotations query and select the Skyminer datasource instead of the Grafana one . The query editor is the same as the metric query one . Please refer to the Grafana official documentation .',\n",
       " 'Context : (Documentation = User Manual, Title = Templated queries) Grafana allows the user to use template variables for dashboards . You can access this option in the dashboard settings .',\n",
       " 'Context : (Documentation = User Manual, Title = Templated queries, Chapter = Configure a variable) Name of the variable The data source from which the data is pulled , in this case you should always choose Skyminer Query , defines what data you want from the data source ( please see Query syntax for more information ) Regex , you can apply a regex on the results from the query in order to filter values Sort , this feature allows you to sort values Multi-value , this option allows you to pick multiple value from the list of values Include all option , add a value in the list of values that allows you to select all values in one click Preview of values , show values received from the data source with the query you defined Add , donâ\\x80\\x99t forget to add the variable and save it',\n",
       " 'Context : (Documentation = User Manual, Title = Templated queries, Chapter = Query syntax) A syntax for the query part of this feature has been defined . You must use it to operate this feature . This syntax is really close to JSON . \"metric\":\"metric_name\", \"tagKey\": \"tag_name\", \"tagName\": [\"tagValues\"], \"tagName2\": [\"tagValues2\"] ... Note : This syntax is case sensitive , be careful with capitalization',\n",
       " 'Context : (Documentation = User Manual, Title = Templated queries, Chapter = Query syntax, Paragraph = Examples) Empty request : getting all metric names An empty query means you want to get all metric names from the data source . Getting all tags name for a metric If you want to get all tags name for a given metric you need to use this syntax : \"metric\":\"foo\" Getting tag values In order to get all values for a given metric and tag name you need to use this syntax : \"metric\":\"foo\", \"tagKey\": \"the_tag_you_want\" Getting tag values filter by another tag value If you want to get all values for a given metric and tag name with a filter ( associated tag ) you need to use this syntax : \"metric\":\"foo\", \"tagKey\": \"bar\", \"status\":[\"success\"] This query will return all values of bar for the metric foo that also match with the tag status with value success . Remember a metric has a lot of tag combinations . For example bar is an array of value [ â\\x80\\x98aâ\\x80\\x99, â\\x80\\x98bâ\\x80\\x99, â\\x80\\x98câ\\x80\\x99] that is respectively in combination with status : [ â\\x80\\x98failedâ\\x80\\x99, â\\x80\\x98successâ\\x80\\x99, â\\x80\\x98successâ\\x80\\x99] The query above will return â\\x80\\x98bâ\\x80\\x99 and â\\x80\\x98câ\\x80\\x99 as values , â\\x80\\x98aâ\\x80\\x99 does not match with the filer . Note that you can chain several filters .',\n",
       " 'Context : (Documentation = User Manual, Title = Templated queries, Chapter = Dashboard with template variables) New fields appear at the top the dashboard page when template variables are defined . You can apply these variables to different fields of the query ( all input text ), especially on metric names and tags . When you change the value of the template variable the dashboard automatically refresh itself . 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Creating a notebook) To create a new notebook and make it accessible from the Skyminer Query page : Make sure you are in the Processors folder Click on New Select Python 3 You can then rename and implement your notebook . Alternatively , you can select an existing notebook and duplicate it to avoid writing one from scratch .',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Creating a query) The Jupyter UI integration extension makes it possible to create and edit Skyminer queries directly in the Jupyter UI and then access the resulting data . Click on Show Skyminer A Skyminer Query interface will appear Create your query in the Skyminer Query interface If your Query is valid the OK button will turn blue . Click on it The extension will inject code inside the notebook',\n",
       " 'Context : (Documentation = User Manual, Title = User Interface, Chapter = Editing a query) To edit a query with the extension in a Jupyter notebook : Click on the code of the cell containing the request . The cell will be highlighted in green . The code of the cell must contain the skyminer_query variable for this to work Click on the button Show Skyminer . A Skyminer Query interface will appear and contain your request . Edit your query Click on the OK button 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer extensions, Chapter = Skyminer query forwarding) Query results can easily be forwarded from the Skyminer query page to Jupyter . After building your Skyminer query , click on the Jupyter button on the right of the page Choose which notebook you want to send the query result to The list contains all the notebooks located in the Processors folder of Jupyter . By default the folder contains some example notebooks , but you can easily add new ones . To do so , refer to Creating a notebook .',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer extensions, Chapter = Skyminer WEBUI integration) This extension makes it possible to use the GET parameters of the url of the notebook as python variables . It uses the Skyminer TS Python Connector library to retrieve data in Python ( see Skyminer Time Series Python Connector ).',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer extensions, Chapter = Skyminer WEBUI integration, Paragraph = Example with SkyminerTS) from SkyminerTS import MetricBuilder , QueryBuilder , TimeUnit , STSAPI , QueriesToDataframe , TimeRelative # Init the API API = STSAPI . init ( SERVER_URL + \"/api/v1\") # Get metric result = API . get_data_points ( WEB_ARGS [\"query\"]) # Convert the result as Dataframes DF = QueriesToDataframe ( result ) # Print Dataframes print ( DF )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer extensions, Chapter = HideCode Jupyter Extension) The HideCode extension hides the python code by default when exporting the notebook to PDF . To hide or show cells in PDF Export : Click on Hide code in the toolbar Click on the item Show status A checkbox will appear at the top of each cell . If a checkbox is checked , the code of the cell will be visible in the PDF 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector) Skyminer Time Series Python Connector is a Python3 library used to access the Skyminer API . You can retrieve data in your Python application . Please refer to the python library documentation for more details .',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Install) pip install httplib2 pandas matplotlib Copy the SkyminerTS and SkyminerTSPlot directories in your project . Import them in your Python script with from KairosAPI import <Modules>',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Install, Paragraph = Import) from SkyminerTS import MODULE1 , MODULE2 ... from SkyminerTSPlot import MODULE3 ... Example : from SkyminerTS import MetricBuilder , QueryBuilder , TimeUnit , STSAPI , QueriesToDataframe from SkyminerTSPlot import plot_skyminer_dataframe',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Quickstart, Paragraph = Get the datapoints of the last hour) from SkyminerTS import MetricBuilder , QueryBuilder , TimeUnit , STSAPI , QueriesToDataframe , TimeRelative # Init the API API = STSAPI . init (\"http ://url-to-skyminer/api/v1/\") # Build our metric MB = MetricBuilder (\"kairosdb . datastore . cassandra . client . requests_timer . min\") # Build our query QB = QueryBuilder () # Define the timerange QB . with_start_relative ( 1 , TimeUnit . HOURS ) # Put the metric in the query QB . with_metric ( MB ) # Retrieve the result result = API . get_data_points ( QB . build ()) # Convert the result as Dataframes DF = QueriesToDataframe ( result ) # Print Dataframes print ( DF )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Quickstart, Paragraph = Plot the result of a Skyminer query) from SkyminerTS import STSAPI from SkyminerTSPlot import line_plot_dataframe # Init the API skyminerAPI = STSAPI . init ( SERVER_URL + \"/api/v1\") # Get the result of the query queryResult = skyminerAPI . get_data_points ( WEB_ARGS [\"query\"]) # Convert the result of the query to a Panda Dataframe dataframe = QueriesToDataframe ( queryResult ) # Plot the dataframe plot_skyminer_dataframe ( dataframe=dataframe , one_plot_per_group=False , plot_width=14 , plot_height=9 )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = STSAPI) STSAPI is the interface with the Skyminer Server . You pass QueryBuilders to the STSAPI . STS = STSAPI . init (\"http ://ip_skyminer/api/v1\") MB = MetricBuilder (\"my_metric\") QB = QueryBuilder () QB . with_start_relative ( 1 , TimeUnit . HOURS ) QB . with_metric ( MB ) result = API . get_data_points ( QB . build ()) init ( url , charset=\\'utf-8\\', timeout\\\\_ms=100 , disable\\\\_ssl\\\\_certificate\\\\_validation=False , ca\\\\_certs=None ) Change the timeout of requests : STS = STSAPI . init (\"http ://ip_skyminer/api/v1\", timeout_ms=1000 ) You can disable ssl certificate validation : STS = STSAPI . init (\"http ://ip_skyminer/api/v1\", disable_ssl_certificate_validation=True ) You can use your own self-signed certificate : STS = STSAPI . init (\"http ://ip_skyminer/api/v1\", ca_certs=\"/certs/mycert . crt\") Use get_data_points to get data pointsfrom the Skyminer server . STS . get_data_points ( query )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = QueryBuilder) Use QueryBuilder to build a query and send it to the Skyminer server with STSAPI . You can pass MetricBuilders to QueryBuilder to create your query . MB = MetricBuilder (\"my_metric\") QB = QueryBuilder () QB . with_metric ( MB ) with_start_absolute ( start_absolute ) QB = QueryBuilder () QB . with_start_absolute ( 1563800009514 ) with_start_relative ( value , unit ) QB = QueryBuilder () QB . with_start_relative ( 1 , TimeUnit . HOURS ) with_end_absolute ( end_absolute ) QB = QueryBuilder () QB . with_end_absolute ( 1563810009514 ) with_end_relative ( value , unit ) QB = QueryBuilder () QB . with_end_relative ( 1 , TimeUnit . HOURS ) with_time_zone ( timezone ) The time zone for the time range of the query . If not specified , UTC is used . QB = QueryBuilder () QB . with_time_zone (\"Asia/Kabul\") with_metrics ( metrics ) QB = QueryBuilder () MB1 = MetricBuilder (\"my_metric1\") MB2 = MetricBuilder (\"my_metric2\") QB . with_metrics ([ MB1 , MB2 , ...]) with_metric ( metricBuilder ) QB = QueryBuilder () MB = MetricBuilder (\"my_metric\") QB . with_metric ( MB ) build () Build the metric in JSON format for STSAPI',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = MetricBuilder) Use MetricBuilder to build your metric . MB = MetricBuilder (\"my_metric\") with_aggregator ( aggregator ) AvgAgg = AvgAggregator () AvgAgg . with_sampling ( TimeRelative ( 1 , TimeUnit . MINUTES )) MB = MetricBuilder (\"my_metric\") MB . with_aggregator ( AvgAgg ) with_aggregators ( aggregators ) Aggregators are processed in the order specified . The output of an aggregator is passed to the input of the next until all have been processed . AvgAgg = AvgAggregator () AvgAgg . with_sampling ( TimeRelative ( 1 , TimeUnit . HOURS )) CountAgg = CountAggregator () CountAgg . with_sampling ( TimeRelative ( 5 , TimeUnit . MINUTES )) MB = MetricBuilder (\"my_metric\") MB . with_aggregators ([ AvgAgg , CountAgg ]) with_tag_filter ( key , values ) MB = MetricBuilder (\"my_metric\") MB . with_tag_filter (\"city\", [\"Toulouse\", \"Paris\"]) with_tags_filter ( tags ) MB = MetricBuilder (\"my_metric\") MB . with_tags_filter ({\"city\": [\"Toulouse\", \"Paris\"], \"type\" : \"road\"}) exclude_tags ( boolean=True ) By default , the result of the query includes tags and tag values associated with the data points . If excludetags_ is set to true , the tags will be excluded from the response . with_limit ( limit ) Limits the number of data points returned from the data source . The limit is applied before any aggregator is executed . MB = MetricBuilder (\"my_metric\") MB . with_limit ( 10000 ) with_order ( order ) Orders the returned data points . Values for order are QueryDataOrder . ASC for ascending or QueryDataOrder . DESC for descending . Defaults to ascending . This sorting is done before any aggregators are executed . MB = MetricBuilder (\"my_metric\") MB . with_order ( QueryDataOrder . ASC ) with_group_by ( group_by ) The resulting data points can be grouped by one or more tags , a time range , or by value , or by a combination of the three . GB = GroupByTag ([\"source\"]) MB = MetricBuilder (\"my_metric\") MB . with_group_by ( GB ) with_group_bys ( group_bys ) GB1 = GroupByTag ([\"source\"]) GB2 = GroupByValue ( 10 ) MB = MetricBuilder (\"my\\\\_metric\") MB . with\\\\_group\\\\_bys ([ GB1 , GB2 ])',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = QueriesToDataframe) Use QueriesToDataframe to convert the result of your queries in dataframes . QB = QueryBuilder () STS = STSAPI . init (\"http ://ip_skyminer/api/v1\") queries = STS . get_data_points ( QB . build ()) # Dataframes DF = QueriesToDataframe ( queries ) Example of result : [ timestamp value cluster host retry_type 0 1563801780000 0 [ skyminer_cluster ] [ de331f397f5d ] [ read_timeout , request_error , unavailable , write_timeout ] 1 1563801840000 0 [ skyminer_cluster ] [ de331f397f5d ] [ read_timeout , request_error , unavailable , write_timeout ] 2 1563801900000 0 [ skyminer_cluster ] [ de331f397f5d ] [ read_timeout , request_error , unavailable , write_timeout ] 3 1563801960000 0 [ skyminer_cluster ] [ de331f397f5d ] [ read_timeout , request_error , unavailable , write_timeout ] .... [ 1068 rows x 5 columns ]]',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = DataFrameToDataPointBuilder(dataframe, metric_name, value_field=â\\x80\\x99valueâ\\x80\\x99, tags={})) /!\\\\ Your dataframe should have a Timestamp ( ms ) index as the output of QueriesToDataframe /!\\\\ Use DataFrameToDataPointBuilder to convert a dataframe into DataPointBuilder . # Dataframes DF = my dataframe Builder = DataFrameToDataPointBuilder ( DF , \\'EIRP\\', tags={\"from\" : \"SAT1\"}) By default DataFrameToDataPointBuilder uses the value column as values but you can change it with the value_field property .',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = GroupByBin) The Bin grouper groups data point values into bins or buckets . Values are placed into groups based on a list of bin values . For example , if the list of bins is 10 , 20 , 30 , then values less than 10 are placed in the first group , values between 10-19 into the second group , and so forth . # Init with a bin GB = GroupByBin ([ 500000 ] # Add another bin GB . with_bin ( 600000 ) # Add multiple bins GB . with_bins ([ 800000 , 1000000 ]) MB = MetricBuilder (\"my_metric\") MB . with_group_by ( GB )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = GroupByTag) You can group results by specifying one or more tag names . For example , if you have a customer tag , grouping by customer would create a resulting object for each customer . Multiple tag names can be used to further group the data points . # Init with a tag GB = GroupByTag ([\"source\"]) # Add another tag GB . with_tag (\"host\") # Add multiple tags GB . with_tags ([\"flag\", \"packet_type\"]) MB = MetricBuilder (\"my_metric\") MB . with_group_by ( GB )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = GroupByTime) The time grouper groups results by time ranges . For example , you could group data by day of the week . Note that the grouper calculates ranges based on the start time of the query . So if you wanted to group by day of the week and wanted the first group to be Sunday , then you would need to set the queryâ\\x80\\x99s start time to be on Sunday . # GroupByTime ( count , value , TimeUnit ) # - count = The number of groups . # This would typically be 7 to group by day of week . # - value = The number of units for the aggregation buckets . # - TimeUnit = The time unit for the sampling rate . GB = GroupByTime ( 7 , 1 , TimeUnit . HOURS ) MB = MetricBuilder (\"my_metric\") MB . with_group_by ( GB )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Modules, Paragraph = GroupByValue) The value grouper groups by data point values . Values are placed into groups based on a range size . For example , if the range size is 10 , then values between 0-9 are placed in the first group , values between 10-19 into the second group , and so forth . # Init with a value GB = GroupByValue ( 10 ) MB = MetricBuilder (\"my_metric\") MB . with_group_by ( GB )',\n",
       " 'Context : (Documentation = User Manual, Title = Skyminer Time Series Python Connector, Chapter = Examples, Paragraph = Get a dataframe of a metric group by tags with an average aggregator and plot it) # Import from SkyminerTS import MetricBuilder , QueryBuilder , TimeUnit , STSAPI , QueriesToDataframe , TimeRelative , GroupByTag from SkyminerTS . Aggregators . AvgAggregator import AvgAggregator # Plot library import matplotlib . pyplot as plt plt . interactive ( False ) # ( Optional ) Better visualization of Dataframes import pandas as pd pd . options . display . max_columns = 1000 pd . options . display . max_rows = 1000 pd . options . display . max_colwidth = 199 pd . options . display . width = None # Init the API API = STSAPI . init (\"http ://url-to-skyminer/api/v1/\") # Build our metric MB = MetricBuilder (\"kairosdb . datastore . cassandra . client . requests_timer . min\") # Add the group by tags MB . with_group_by ( GroupByTag ([\"cluster\", \"host\"])) # Add the average aggregator AVG = AvgAggregator () AVG . with_sampling ( TimeRelative ( 1 , TimeUnit . HOURS )) MB . with_aggregator ( AVG ) # Build our query QB = QueryBuilder () # Define the timerange QB . with_start_relative ( 10 , TimeUnit . DAYS ) # Put the metric in the query QB . with_metric ( MB ) # Retrieve the result result = API . get_data_points ( QB . build ()) # Convert the result as Dataframes DF = QueriesToDataframe ( result ) # Print Dataframes print ( DF ) # Graph DF . plot () plt . show () Result example : [ timestamp value cluster host group_by 0 1563800400000 408573 . 162162 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 1 1563804000000 549544 . 783333 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 2 1563807600000 390316 . 450000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 3 1563811200000 361768 . 000000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 4 1563814800000 346524 . 700000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 5 1563818400000 332155 . 350000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 6 1563822000000 330671 . 000000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 7 1563825600000 342598 . 533333 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 8 1563829200000 319996 . 700000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 9 1563832800000 321140 . 000000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 10 1563836400000 336191 . 600000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 11 1563840000000 326488 . 200000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 12 1563843600000 324164 . 666667 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 13 1563847200000 331242 . 350000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 14 1563850800000 305792 . 500000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 15 1563854400000 313840 . 666667 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 16 1563858000000 336570 . 750000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 17 1563861600000 324286 . 833333 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 18 1563865200000 376431 . 183333 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 19 1563868800000 373827 . 616667 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}} 20 1563872400000 351905 . 000000 [ skyminer_cluster ] [ de331f397f5d ] {\\'tag\\': {\\'cluster\\': \\'skyminer_cluster\\', \\'host\\': \\'de331f397f5d\\'}}] 2014-2023 , Kratos Communications Skyminer User Manual - KC-153-UM-0020 v . 2023-dev-S17']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc412bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb47cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new collection (or clear the existing one)\n",
    "new_vectorstore = chroma_client.get_or_create_collection(name=\"Skyminer-T\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Add the new embedded documents to the new collection in Chroma\n",
    "new_vectorstore.add(\n",
    "    documents=new_documents_list,\n",
    "    embeddings=new_embeddings_list,\n",
    "    metadatas=new_metadatas_list,\n",
    "    ids=new_ids_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1348a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=EPOCH),\n",
       " Collection(name=QMS),\n",
       " Collection(name=EPOCH-T),\n",
       " Collection(name=Skyminer),\n",
       " Collection(name=QMS-T),\n",
       " Collection(name=Skyminer-T)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56381525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302dfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
