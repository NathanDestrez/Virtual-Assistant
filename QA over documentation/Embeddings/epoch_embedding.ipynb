{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0309d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:06.982584Z",
     "start_time": "2023-11-17T10:17:00.625082Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#  Scraping\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "\n",
    "# Langchain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Chroma \n",
    "import chromadb \n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#Model \n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71dc850",
   "metadata": {},
   "source": [
    "## Extract file's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3739c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:06.986983Z",
     "start_time": "2023-11-17T10:17:06.983585Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_doc_title(pdf_path):\n",
    "    title_parts = pdf_path.split(\"/\")\n",
    "    title = title_parts[-1].replace(\".pdf\", \"\")\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0f933",
   "metadata": {},
   "source": [
    "## Extract table of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b6d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.028317Z",
     "start_time": "2023-11-17T10:17:06.987986Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_toc_pages(pdf_path):\n",
    "    toc_start_page = None\n",
    "    toc_end_page = None\n",
    "\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Initialize variables to keep track of line number and TOC detection\n",
    "        line_number = 0\n",
    "        toc_started = False\n",
    "        \n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            toc_page = False\n",
    "            \n",
    "            # Split the page text into lines\n",
    "            lines = page_text.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                line_number += 1\n",
    "                \n",
    "                # Use a case-insensitive regular expression pattern to identify the TOC\n",
    "                toc_pattern = r'\\s*table\\s+of\\s+contents\\s*'\n",
    "                \n",
    "                if re.search(toc_pattern, line, re.IGNORECASE):\n",
    "                    if toc_start_page is None:\n",
    "                        toc_start_page = page_number + 1  # Add 1 because page numbers are 1-based\n",
    "                    toc_started = True\n",
    "\n",
    "                # If TOC has started and a chapter/section is detected, mark it as the end\n",
    "                if toc_started:\n",
    "                    #toc_end_pattern = r'^(\\d+(\\.\\d+)*)\\s+(.*?)\\s+(\\d+)\\s*$'\n",
    "                    toc_end_pattern = r'^(\\d+(\\.\\d+)*(\\.)*)\\s+(.*?)\\s+(\\d+)(\\-\\d+)*\\s*$'\n",
    "                    \n",
    "                    if re.search(toc_end_pattern, line, re.IGNORECASE):\n",
    "                        toc_page = True\n",
    "                        break\n",
    "                        \n",
    "            if toc_started and not toc_page:\n",
    "                toc_end_page = page_number  # Page where TOC ends\n",
    "                break\n",
    "\n",
    "            # Exit the loop if both start and end pages are found\n",
    "            if toc_start_page is not None and toc_end_page is not None:\n",
    "                break\n",
    "\n",
    "    return toc_start_page, toc_end_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bd1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.036333Z",
     "start_time": "2023-11-17T10:17:07.030321Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_table_of_contents(pdf_path):\n",
    "    toc_entries = []\n",
    "\n",
    "    toc_start_page, toc_end_page = find_toc_pages(pdf_path)\n",
    "    if(toc_start_page == None):\n",
    "        return None\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        for page_number in range(toc_start_page - 1, toc_end_page):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Split the page text into lines and process TOC entries\n",
    "            lines = page_text.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "\n",
    "                if line.strip():\n",
    "                    #toc_match = re.match(r'^(\\d+(\\.\\d+)*)\\s+(.*?)\\s+(\\d+)\\s*$', line)\n",
    "                    toc_match = re.match(r'^(\\d+(\\.\\d+)*(\\.)*)\\s+(.*?)\\s+(\\d+)(\\-\\d+)*\\s*$', line)\n",
    "                    \n",
    "                    if toc_match:\n",
    "                        page_number = toc_match.group(5)\n",
    "                        title_number = toc_match.group(1)\n",
    "                        title = toc_match.group(4).replace(\".\", \"\").rstrip()\n",
    "                        toc_entries.append((title_number, title, page_number))\n",
    "\n",
    "    return toc_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a09cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.046158Z",
     "start_time": "2023-11-17T10:17:07.037335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data, title=None, parent=None, page = None):\n",
    "        self.data = data\n",
    "        self.title = title\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.page = page\n",
    "    \n",
    "    def add_parent(parent):\n",
    "        self.parent = parent\n",
    "\n",
    "def find_node_by_number(root, target_data):\n",
    "    if root.data == target_data:\n",
    "        return root\n",
    "\n",
    "    for child in root.children:\n",
    "        node = find_node_by_number(child, target_data)\n",
    "        if node:\n",
    "            return node\n",
    "    return None\n",
    "\n",
    "def build_tree(toc_list):\n",
    "    root = TreeNode(\"Root\")\n",
    "    \n",
    "    for entry in toc_list:\n",
    "        level_str = str(entry[0])\n",
    "        title = entry[1]\n",
    "        page = entry[2]\n",
    "        \n",
    "        # Split the level string by '.' to determine the hierarchy\n",
    "        pos_separator = level_str.rfind('.')\n",
    "        if(pos_separator > -1):\n",
    "            parent_level = level_str[0: pos_separator]\n",
    "            parent_node = find_node_by_number(root, parent_level)\n",
    "            if(parent_node is None):\n",
    "                positions = [i for i, _ in enumerate(level_str) if level_str.startswith(\".\", i)]\n",
    "                for i in range(len(positions)):\n",
    "                    current_level = level_str[0:positions[i]]\n",
    "                    current_node = find_node_by_number(root, current_level)\n",
    "                    if(current_node is None):\n",
    "                        if(i == 0):\n",
    "                            node = TreeNode(current_level, parent = root, title = \"\", page = page)\n",
    "                            root.children.append(node)\n",
    "                        else:\n",
    "                            parent_level = level_str[0:positions[i-1]]\n",
    "                            parent_node = find_node_by_number(root, parent_level)\n",
    "                            node =  TreeNode(current_level, parent = parent_node, title = \"\", page = page)\n",
    "                            parent_node.children.append(node)\n",
    "                parent_level = level_str[0: pos_separator]\n",
    "                parent_node = find_node_by_number(root, parent_level)\n",
    "                node =  TreeNode(level_str, parent = parent_node, title = title, page = page)\n",
    "                parent_node.children.append(node)\n",
    "            else:\n",
    "                node =  TreeNode(level_str, parent = parent_node, title = title, page = page)\n",
    "                parent_node.children.append(node)\n",
    "        else:\n",
    "            parent_level = 0\n",
    "            parent = root\n",
    "            node = TreeNode(level_str, parent = root, title = title, page = page)\n",
    "            root.children.append(node)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369b997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.050915Z",
     "start_time": "2023-11-17T10:17:07.047155Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_all_titles(toc_tree, number_chapter):\n",
    "\n",
    "    node = find_node_by_number(toc_tree, str(number_chapter))\n",
    "    if(node == None):\n",
    "        title_list = [number_chapter]\n",
    "    else:\n",
    "        titles_list = [node.data + \" \" + node.title]\n",
    "    \n",
    "    while node.parent.data != \"Root\":\n",
    "        titles_list.append(node.parent.data + \" \" + node.parent.title)\n",
    "        node = node.parent\n",
    "    titles = \"\"\n",
    "    for i in range(len(titles_list) -1, -1, -1):\n",
    "        titles = titles + \"/\" + titles_list[i]\n",
    "    return titles\n",
    "\n",
    "#list_node = find_all_titles(toc_tree, 3.2)\n",
    "#list_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0605f",
   "metadata": {},
   "source": [
    "## Extract paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926c5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.062439Z",
     "start_time": "2023-11-17T10:17:07.051912Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_between_titles(pdf_path, toc_entries, header_len = 4):\n",
    "    text_list = []\n",
    "    pdf_text = []\n",
    "    \n",
    "    toc_start_page, toc_end_page = find_toc_pages(pdf_path)\n",
    "    start_page = toc_end_page + 1\n",
    "    doc_title = extract_doc_title(pdf_path)\n",
    "    \n",
    "    toc_tree = build_tree(toc_entries)\n",
    "    \n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        for page_number in range(start_page, len(pdf_reader.pages)):\n",
    "            # Get all pages\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "            pdf_text.append(page_text)\n",
    "\n",
    "    for i, toc_entry in enumerate(toc_entries):\n",
    "        title_number, title, page_number_toc = toc_entry\n",
    "        \n",
    "        #Get next title name and number\n",
    "        if i < len(toc_entries)-1:\n",
    "            title_number_next, title_next, page_number_toc_next = toc_entries[i+1]\n",
    "        else:\n",
    "            title_number_next = None\n",
    "            title_next = None\n",
    "\n",
    "        next_title_found = False\n",
    "        current_title_found = False\n",
    "        current_text = []\n",
    "\n",
    "        for page_text in pdf_text:\n",
    "            # Split the page text into lines\n",
    "            lines = page_text.split('\\n')\n",
    "            line_number = 0\n",
    "\n",
    "            for line in lines:\n",
    "                \n",
    "                line_number+=1\n",
    "                if line_number <= header_len:\n",
    "                    #ignore headers\n",
    "                    continue\n",
    "                if line_number == len(lines) and re.match(r'^\\s+(\\d+)\\s+$', line):\n",
    "                    #Ignore page_number\n",
    "                    continue\n",
    "                    \n",
    "                # Check if the line contains the current title\n",
    "                if (title.lower().replace(\" \", \"\") in line.lower().replace(\" \", \"\") and (title_number + \" \") in line):\n",
    "                    # Start a new section for the current title\n",
    "                    current_title_found = True\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    if(title_number_next is not None):\n",
    "                        # Check if the next title is found\n",
    "                        if (title_next.lower().replace(\" \", \"\") in line.lower().replace(\" \", \"\") and (title_number_next + \" \") in line):\n",
    "                            next_title_found = True\n",
    "\n",
    "                            # Append the section if a title was found\n",
    "                            if current_title_found:\n",
    "                                titles = find_all_titles(toc_tree, title_number)\n",
    "                                text_list.append({\n",
    "                                    'Title Number': title_number,\n",
    "                                    'Title': title,\n",
    "                                    'Page': page_number_toc,\n",
    "                                    'Text': '\\n'.join(current_text),\n",
    "                                    'Source': doc_title + titles,\n",
    "                                    'Documentation': doc_title\n",
    "                                })\n",
    "                            current_title_found = False\n",
    "                            # Breaks the loop as soon as a new title is found\n",
    "                            break\n",
    "                        elif current_title_found:\n",
    "                            # Append the line to the current text if it's not a title\n",
    "                            current_text.append(line)\n",
    "                    else:\n",
    "                        # Append the line to current text\n",
    "                        current_text.append(line)\n",
    "                        \n",
    "                if next_title_found:\n",
    "                     # Breaks the loop as soon as a new title is found\n",
    "                    break\n",
    "                    \n",
    "        if(title_number_next is None):\n",
    "            # Add the current text for the last title\n",
    "            titles = find_all_titles(toc_tree, title_number)\n",
    "            text_list.append({\n",
    "                'Title Number': title_number,\n",
    "                'Title': title,\n",
    "                'Page': page_number_toc,\n",
    "                'Text': '\\n'.join(current_text),\n",
    "                'Source': doc_title + titles,\n",
    "                'Documentation': doc_title\n",
    "                })\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5333d8",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f7d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:17:07.068748Z",
     "start_time": "2023-11-17T10:17:07.063442Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the provided text by:\n",
    "    - Removing HTML tags and content\n",
    "    - Removing Markdown-specific syntax\n",
    "    - Converting Unicode characters to their actual representation\n",
    "    - Removing URLs\n",
    "    - Removing extra white spaces\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input string to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The cleaned string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    no_html = soup.get_text(separator=' ')\n",
    "    \n",
    "    # 2. Remove Markdown Syntax\n",
    "    no_markdown = re.sub(r'\\!\\[.*?\\]\\(.*?\\)|\\[(.*?)\\]\\(.*?\\)|\\*\\*.*?\\*\\*|\\*.*?\\*|#[^\\n]*', '', no_html)\n",
    "    \n",
    "    # 3. Convert Unicode characters (for common entities; can be expanded further)\n",
    "    no_unicode = re.sub(r'&amp;', '&', no_markdown)\n",
    "    no_unicode = re.sub(r'&lt;', '<', no_unicode)\n",
    "    no_unicode = re.sub(r'&gt;', '>', no_unicode)\n",
    "    \n",
    "    # 4. Remove URLs\n",
    "    no_urls = re.sub(r'http[s]?://\\S+', '', no_unicode)\n",
    "    \n",
    "    # 5. Remove extra white spaces\n",
    "    clean_string = ' '.join(no_urls.split())\n",
    "    \n",
    "    return clean_string\n",
    "\n",
    "def clean_documents(documents):\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "    return documents\n",
    "\n",
    "def clean_text_entries(text_entries):\n",
    "    for text_entry in text_entries:\n",
    "        text_entry['Text'] = clean_text(text_entry['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da1355",
   "metadata": {},
   "source": [
    "## Add several PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59590336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:20:00.901894Z",
     "start_time": "2023-11-17T10:20:00.897683Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf_files(folder_path):\n",
    "    pdf_files = [file for file in os.listdir(folder_path) if file.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "folder_path = '../docs/EPOCH Docs/'\n",
    "\n",
    "pdf_list = get_pdf_files(folder_path)\n",
    "\n",
    "# Print the list of PDF files\n",
    "print(\"List of PDF files:\")\n",
    "for pdf_file in pdf_list:\n",
    "    print(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0d8b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:11.202246Z",
     "start_time": "2023-11-17T10:20:07.821115Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for pdf_path in pdf_list:\n",
    "    print(pdf_path)\n",
    "    toc_entries = extract_table_of_contents(folder_path + \"/\" + pdf_path)\n",
    "    if toc_entries == None:\n",
    "        print(\"This PDF doesn't have a TOC\")\n",
    "        continue\n",
    "    text_entries = extract_text_between_titles(folder_path + \"/\" + pdf_path, toc_entries)\n",
    "    clean_text_entries(text_entries)\n",
    "    if df_final.empty:\n",
    "        df_final = pd.DataFrame(text_entries)\n",
    "    else:\n",
    "        df = pd.DataFrame(text_entries)\n",
    "        df_final = pd.concat([df_final, df], axis=0)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79b694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:11.232143Z",
     "start_time": "2023-11-17T10:21:11.203248Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final['word_count'] = df_final['Text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4fbc7",
   "metadata": {},
   "source": [
    "## Store in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaa31c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:11.387992Z",
     "start_time": "2023-11-17T10:21:11.233138Z"
    }
   },
   "outputs": [],
   "source": [
    "chroma_client = client = chromadb.PersistentClient(path='C:/Users/Nathan/Kratos_data-Science/Chroma/v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc136ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:32.491159Z",
     "start_time": "2023-11-17T10:21:32.462504Z"
    }
   },
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b06251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:33.540787Z",
     "start_time": "2023-11-17T10:21:33.158219Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2',  device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587622a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:21:34.001542Z",
     "start_time": "2023-11-17T10:21:33.981220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Chroma\n",
    "vectorstore = chroma_client.get_or_create_collection(name=\"EPOCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1dda96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:22:55.357072Z",
     "start_time": "2023-11-17T10:21:34.670097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lists to store the extracted information from documents\n",
    "documents_list = []\n",
    "embeddings_list = []\n",
    "metadatas_list = []\n",
    "ids_list = []\n",
    "\n",
    "# Assuming 'content' in your dataframe is what you consider as the document/page_content\n",
    "for _, row in df_final.iterrows():\n",
    "    embedding = model.encode(row['Text'])\n",
    "    \n",
    "    # Constructing metadata\n",
    "    metadata = {\n",
    "        \"source\": f\"{row['Source']}\",\n",
    "        \"title_number\": f\"{row['Title Number']}\",\n",
    "        \"title\": f\"{row['Title']}\",\n",
    "        \"page_number\": f\"{row['Page']}\",\n",
    "        \"word_count\": row['word_count'],\n",
    "        \"documentation\": f\"{row['Documentation']}\",\n",
    "        \"file_path\": \"path\"\n",
    "    }\n",
    "\n",
    "    documents_list.append(row['Text'])\n",
    "    embeddings_list.append(embedding.tolist())\n",
    "    metadatas_list.append(metadata)\n",
    "\n",
    "# Generating IDs for the documents\n",
    "ids_list = [\"v\" + str(i + 1) for i in range(len(documents_list))]\n",
    "\n",
    "# Add the embedded documents to the collection in Chroma\n",
    "vectorstore.add(\n",
    "    documents=documents_list,\n",
    "    embeddings=embeddings_list,\n",
    "    metadatas=metadatas_list,\n",
    "    ids=ids_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350c640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:22:55.362690Z",
     "start_time": "2023-11-17T10:22:55.358073Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4193b43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:22:55.404180Z",
     "start_time": "2023-11-17T10:22:55.363692Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"There are\", vectorstore.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
